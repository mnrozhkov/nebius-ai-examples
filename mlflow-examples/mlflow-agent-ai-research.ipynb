{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: Simplifying Agent Development with MLflow & CrewAI\n",
    "\n",
    "**Goal:** This tutorial demonstrates how Managed MLflow transforms the development of multi-agent systems (using CrewAI as an example) from a potentially opaque and difficult-to-debug process into a structured, observable, and iterative engineering workflow.\n",
    "\n",
    "**Scenario:** We will use the \"AI Research\" agentic systen. This system researches new AI/ML tools. While useful, developing such agents presents common challenges:\n",
    "\n",
    "*   **Black Box Execution:** What *exactly* did the agent do step-by-step? Why did it choose *that* tool?\n",
    "*   **Comparing Changes:** How do we reliably compare results if we tweak prompts or agent configurations?\n",
    "*   **Debugging Failures:** Why did the agent fail? What was it trying to do right before the error?\n",
    "*   **Performance:** Is the agent getting slower? Where are the bottlenecks?\n",
    "*   **Quality Evaluation:** Is changing the LLM or prompts actually improving the *quality* of the output?\n",
    "*   **Cost/Speed Optimization:** How can we track and reduce token usage or latency?\n",
    "\n",
    "We will tackle some problems by incrementally integrating MLflow tracking features into our CrewAI development process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "\n",
    "1. Launch your instance of the Managed Service for MLflow with [the MLflow quickstart](https://docs.nebius.com/mlflow/quickstart).\n",
    "2. Set up your API key to connect to Nebius AI Studio with [the AI Studio quickstart](https://docs.nebius.com/studio/inference/quickstart).\n",
    "\n",
    "> **Note:** Launching an MLflow cluster for the first time may take 15-30 minutes to be fully provisioned and ready to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-17T05:30:00.905175Z",
     "iopub.status.busy": "2025-04-17T05:30:00.903990Z",
     "iopub.status.idle": "2025-04-17T05:30:02.695175Z",
     "shell.execute_reply": "2025-04-17T05:30:02.694679Z",
     "shell.execute_reply.started": "2025-04-17T05:30:00.905130Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mlflow==2.21.2 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (2.21.2)\n",
      "Requirement already satisfied: python-dotenv in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (1.0.1)\n",
      "Requirement already satisfied: openai in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (1.69.0)\n",
      "Requirement already satisfied: crewai==0.114.0 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (0.114.0)\n",
      "Requirement already satisfied: crewai-tools in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (0.38.1)\n",
      "Requirement already satisfied: duckduckgo-search in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (7.5.5)\n",
      "Requirement already satisfied: mlflow-skinny==2.21.2 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from mlflow==2.21.2) (2.21.2)\n",
      "Requirement already satisfied: Flask<4 in /Users/mikhailrozhkov/.local/lib/python3.11/site-packages (from mlflow==2.21.2) (2.2.5)\n",
      "Requirement already satisfied: Jinja2<4,>=2.11 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from mlflow==2.21.2) (3.1.5)\n",
      "Requirement already satisfied: alembic!=1.10.0,<2 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from mlflow==2.21.2) (1.13.2)\n",
      "Requirement already satisfied: docker<8,>=4.0.0 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from mlflow==2.21.2) (7.1.0)\n",
      "Requirement already satisfied: graphene<4 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from mlflow==2.21.2) (3.3)\n",
      "Requirement already satisfied: gunicorn<24 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from mlflow==2.21.2) (23.0.0)\n",
      "Requirement already satisfied: markdown<4,>=3.3 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from mlflow==2.21.2) (3.7)\n",
      "Requirement already satisfied: matplotlib<4 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from mlflow==2.21.2) (3.9.0)\n",
      "Requirement already satisfied: numpy<3 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from mlflow==2.21.2) (1.26.4)\n",
      "Requirement already satisfied: pandas<3 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from mlflow==2.21.2) (2.2.3)\n",
      "Requirement already satisfied: pyarrow<20,>=4.0.0 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from mlflow==2.21.2) (15.0.2)\n",
      "Requirement already satisfied: scikit-learn<2 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from mlflow==2.21.2) (1.5.2)\n",
      "Requirement already satisfied: scipy<2 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from mlflow==2.21.2) (1.11.4)\n",
      "Requirement already satisfied: sqlalchemy<3,>=1.4.0 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from mlflow==2.21.2) (2.0.36)\n",
      "Requirement already satisfied: appdirs>=1.4.4 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from crewai==0.114.0) (1.4.4)\n",
      "Requirement already satisfied: auth0-python>=4.7.1 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from crewai==0.114.0) (4.8.1)\n",
      "Requirement already satisfied: blinker>=1.9.0 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from crewai==0.114.0) (1.9.0)\n",
      "Requirement already satisfied: chromadb>=0.5.23 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from crewai==0.114.0) (0.5.23)\n",
      "Requirement already satisfied: click>=8.1.7 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from crewai==0.114.0) (8.1.8)\n",
      "Requirement already satisfied: instructor>=1.3.3 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from crewai==0.114.0) (1.3.3)\n",
      "Requirement already satisfied: json-repair>=0.25.2 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from crewai==0.114.0) (0.25.3)\n",
      "Requirement already satisfied: json5>=0.10.0 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from crewai==0.114.0) (0.10.0)\n",
      "Requirement already satisfied: jsonref>=1.1.0 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from crewai==0.114.0) (1.1.0)\n",
      "Requirement already satisfied: litellm==1.60.2 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from crewai==0.114.0) (1.60.2)\n",
      "Requirement already satisfied: openpyxl>=3.1.5 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from crewai==0.114.0) (3.1.5)\n",
      "Requirement already satisfied: opentelemetry-api>=1.30.0 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from crewai==0.114.0) (1.32.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-http>=1.30.0 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from crewai==0.114.0) (1.32.0)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.30.0 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from crewai==0.114.0) (1.32.0)\n",
      "Requirement already satisfied: pdfplumber>=0.11.4 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from crewai==0.114.0) (0.11.6)\n",
      "Requirement already satisfied: pydantic>=2.4.2 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from crewai==0.114.0) (2.8.2)\n",
      "Requirement already satisfied: pyvis>=0.3.2 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from crewai==0.114.0) (0.3.2)\n",
      "Requirement already satisfied: regex>=2024.9.11 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from crewai==0.114.0) (2024.11.6)\n",
      "Requirement already satisfied: tomli-w>=1.1.0 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from crewai==0.114.0) (1.2.0)\n",
      "Requirement already satisfied: tomli>=2.0.2 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from crewai==0.114.0) (2.2.1)\n",
      "Requirement already satisfied: uv>=0.4.25 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from crewai==0.114.0) (0.6.10)\n",
      "Requirement already satisfied: aiohttp in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from litellm==1.60.2->crewai==0.114.0) (3.11.14)\n",
      "Requirement already satisfied: httpx<0.28.0,>=0.23.0 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from litellm==1.60.2->crewai==0.114.0) (0.27.0)\n",
      "Requirement already satisfied: importlib-metadata>=6.8.0 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from litellm==1.60.2->crewai==0.114.0) (7.1.0)\n",
      "Requirement already satisfied: jsonschema<5.0.0,>=4.22.0 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from litellm==1.60.2->crewai==0.114.0) (4.23.0)\n",
      "Requirement already satisfied: tiktoken>=0.7.0 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from litellm==1.60.2->crewai==0.114.0) (0.7.0)\n",
      "Requirement already satisfied: tokenizers in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from litellm==1.60.2->crewai==0.114.0) (0.20.1)\n",
      "Requirement already satisfied: cachetools<6,>=5.0.0 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from mlflow-skinny==2.21.2->mlflow==2.21.2) (5.3.2)\n",
      "Requirement already satisfied: cloudpickle<4 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from mlflow-skinny==2.21.2->mlflow==2.21.2) (3.0.0)\n",
      "Requirement already satisfied: databricks-sdk<1,>=0.20.0 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from mlflow-skinny==2.21.2->mlflow==2.21.2) (0.35.0)\n",
      "Requirement already satisfied: fastapi<1 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from mlflow-skinny==2.21.2->mlflow==2.21.2) (0.110.3)\n",
      "Requirement already satisfied: gitpython<4,>=3.1.9 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from mlflow-skinny==2.21.2->mlflow==2.21.2) (3.1.40)\n",
      "Requirement already satisfied: packaging<25 in /Users/mikhailrozhkov/.local/lib/python3.11/site-packages (from mlflow-skinny==2.21.2->mlflow==2.21.2) (23.2)\n",
      "Requirement already satisfied: protobuf<6,>=3.12.0 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from mlflow-skinny==2.21.2->mlflow==2.21.2) (5.29.4)\n",
      "Requirement already satisfied: pyyaml<7,>=5.1 in /Users/mikhailrozhkov/.local/lib/python3.11/site-packages (from mlflow-skinny==2.21.2->mlflow==2.21.2) (6.0.1)\n",
      "Requirement already satisfied: requests<3,>=2.17.3 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from mlflow-skinny==2.21.2->mlflow==2.21.2) (2.32.3)\n",
      "Requirement already satisfied: sqlparse<1,>=0.4.0 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from mlflow-skinny==2.21.2->mlflow==2.21.2) (0.5.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.0.0 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from mlflow-skinny==2.21.2->mlflow==2.21.2) (4.12.2)\n",
      "Requirement already satisfied: uvicorn<1 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from mlflow-skinny==2.21.2->mlflow==2.21.2) (0.30.3)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from openai) (4.2.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from openai) (1.8.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from openai) (0.4.2)\n",
      "Requirement already satisfied: sniffio in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from openai) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from openai) (4.66.5)\n",
      "Requirement already satisfied: embedchain>=0.1.114 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from crewai-tools) (0.1.128)\n",
      "Requirement already satisfied: lancedb>=0.5.4 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from crewai-tools) (0.5.7)\n",
      "Requirement already satisfied: pyright>=1.1.350 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from crewai-tools) (1.1.373)\n",
      "Requirement already satisfied: pytube>=15.0.0 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from crewai-tools) (15.0.0)\n",
      "Requirement already satisfied: primp>=0.14.0 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from duckduckgo-search) (0.14.0)\n",
      "Requirement already satisfied: lxml>=5.3.0 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from duckduckgo-search) (5.3.1)\n",
      "Requirement already satisfied: Mako in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from alembic!=1.10.0,<2->mlflow==2.21.2) (1.3.5)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/mikhailrozhkov/.local/lib/python3.11/site-packages (from anyio<5,>=3.5.0->openai) (3.4)\n",
      "Requirement already satisfied: cryptography>=43.0.1 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from auth0-python>=4.7.1->crewai==0.114.0) (44.0.2)\n",
      "Requirement already satisfied: pyjwt>=2.8.0 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from auth0-python>=4.7.1->crewai==0.114.0) (2.8.0)\n",
      "Requirement already satisfied: urllib3>=2.2.3 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from auth0-python>=4.7.1->crewai==0.114.0) (2.3.0)\n",
      "Requirement already satisfied: build>=1.0.3 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from chromadb>=0.5.23->crewai==0.114.0) (1.2.1)\n",
      "Requirement already satisfied: chroma-hnswlib==0.7.6 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from chromadb>=0.5.23->crewai==0.114.0) (0.7.6)\n",
      "Requirement already satisfied: posthog>=2.4.0 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from chromadb>=0.5.23->crewai==0.114.0) (3.5.0)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from chromadb>=0.5.23->crewai==0.114.0) (1.18.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from chromadb>=0.5.23->crewai==0.114.0) (1.32.0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from chromadb>=0.5.23->crewai==0.114.0) (0.53b0)\n",
      "Requirement already satisfied: pypika>=0.48.9 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from chromadb>=0.5.23->crewai==0.114.0) (0.48.9)\n",
      "Requirement already satisfied: overrides>=7.3.1 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from chromadb>=0.5.23->crewai==0.114.0) (7.7.0)\n",
      "Requirement already satisfied: importlib-resources in /Users/mikhailrozhkov/.local/lib/python3.11/site-packages (from chromadb>=0.5.23->crewai==0.114.0) (6.1.1)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from chromadb>=0.5.23->crewai==0.114.0) (1.71.0)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from chromadb>=0.5.23->crewai==0.114.0) (4.0.1)\n",
      "Requirement already satisfied: typer>=0.9.0 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from chromadb>=0.5.23->crewai==0.114.0) (0.9.0)\n",
      "Requirement already satisfied: kubernetes>=28.1.0 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from chromadb>=0.5.23->crewai==0.114.0) (30.1.0)\n",
      "Requirement already satisfied: tenacity>=8.2.3 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from chromadb>=0.5.23->crewai==0.114.0) (8.3.0)\n",
      "Requirement already satisfied: mmh3>=4.0.1 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from chromadb>=0.5.23->crewai==0.114.0) (4.1.0)\n",
      "Requirement already satisfied: orjson>=3.9.12 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from chromadb>=0.5.23->crewai==0.114.0) (3.10.6)\n",
      "Requirement already satisfied: rich>=10.11.0 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from chromadb>=0.5.23->crewai==0.114.0) (13.7.0)\n",
      "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.2 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from embedchain>=0.1.114->crewai-tools) (4.12.3)\n",
      "Requirement already satisfied: gptcache<0.2.0,>=0.1.43 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from embedchain>=0.1.114->crewai-tools) (0.1.43)\n",
      "Requirement already satisfied: langchain<0.4.0,>=0.3.1 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from embedchain>=0.1.114->crewai-tools) (0.3.21)\n",
      "Requirement already satisfied: langchain-cohere<0.4.0,>=0.3.0 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from embedchain>=0.1.114->crewai-tools) (0.3.5)\n",
      "Requirement already satisfied: langchain-community<0.4.0,>=0.3.1 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from embedchain>=0.1.114->crewai-tools) (0.3.20)\n",
      "Requirement already satisfied: langchain-openai<0.3.0,>=0.2.1 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from embedchain>=0.1.114->crewai-tools) (0.2.14)\n",
      "Requirement already satisfied: langsmith<0.4.0,>=0.3.18 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from embedchain>=0.1.114->crewai-tools) (0.3.19)\n",
      "Requirement already satisfied: mem0ai<0.2.0,>=0.1.54 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from embedchain>=0.1.114->crewai-tools) (0.1.79)\n",
      "Requirement already satisfied: pypdf<6.0.0,>=5.0.0 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from embedchain>=0.1.114->crewai-tools) (5.4.0)\n",
      "Requirement already satisfied: pysbd<0.4.0,>=0.3.4 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from embedchain>=0.1.114->crewai-tools) (0.3.4)\n",
      "Requirement already satisfied: schema<0.8.0,>=0.7.5 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from embedchain>=0.1.114->crewai-tools) (0.7.5)\n",
      "Requirement already satisfied: Werkzeug>=2.2.2 in /Users/mikhailrozhkov/.local/lib/python3.11/site-packages (from Flask<4->mlflow==2.21.2) (2.3.8)\n",
      "Requirement already satisfied: itsdangerous>=2.0 in /Users/mikhailrozhkov/.local/lib/python3.11/site-packages (from Flask<4->mlflow==2.21.2) (2.1.2)\n",
      "Requirement already satisfied: graphql-core<3.3,>=3.1 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from graphene<4->mlflow==2.21.2) (3.2.3)\n",
      "Requirement already satisfied: graphql-relay<3.3,>=3.1 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from graphene<4->mlflow==2.21.2) (3.2.0)\n",
      "Requirement already satisfied: aniso8601<10,>=8 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from graphene<4->mlflow==2.21.2) (9.0.1)\n",
      "Requirement already satisfied: certifi in /Users/mikhailrozhkov/.local/lib/python3.11/site-packages (from httpx<0.28.0,>=0.23.0->litellm==1.60.2->crewai==0.114.0) (2023.7.22)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from httpx<0.28.0,>=0.23.0->litellm==1.60.2->crewai==0.114.0) (1.0.4)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from httpcore==1.*->httpx<0.28.0,>=0.23.0->litellm==1.60.2->crewai==0.114.0) (0.14.0)\n",
      "Requirement already satisfied: docstring-parser<0.17,>=0.16 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from instructor>=1.3.3->crewai==0.114.0) (0.16)\n",
      "Requirement already satisfied: pydantic-core<3.0.0,>=2.18.0 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from instructor>=1.3.3->crewai==0.114.0) (2.20.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/mikhailrozhkov/.local/lib/python3.11/site-packages (from Jinja2<4,>=2.11->mlflow==2.21.2) (2.1.3)\n",
      "Requirement already satisfied: deprecation in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from lancedb>=0.5.4->crewai-tools) (2.1.0)\n",
      "Requirement already satisfied: pylance==0.9.18 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from lancedb>=0.5.4->crewai-tools) (0.9.18)\n",
      "Requirement already satisfied: ratelimiter~=1.0 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from lancedb>=0.5.4->crewai-tools) (1.2.0.post0)\n",
      "Requirement already satisfied: retry>=0.9.2 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from lancedb>=0.5.4->crewai-tools) (0.9.2)\n",
      "Requirement already satisfied: attrs>=21.3.0 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from lancedb>=0.5.4->crewai-tools) (23.2.0)\n",
      "Requirement already satisfied: semver>=3.0 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from lancedb>=0.5.4->crewai-tools) (3.0.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from matplotlib<4->mlflow==2.21.2) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from matplotlib<4->mlflow==2.21.2) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from matplotlib<4->mlflow==2.21.2) (4.45.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from matplotlib<4->mlflow==2.21.2) (1.4.5)\n",
      "Requirement already satisfied: pillow>=8 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from matplotlib<4->mlflow==2.21.2) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from matplotlib<4->mlflow==2.21.2) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/mikhailrozhkov/.local/lib/python3.11/site-packages (from matplotlib<4->mlflow==2.21.2) (2.8.2)\n",
      "Requirement already satisfied: et-xmlfile in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from openpyxl>=3.1.5->crewai==0.114.0) (2.0.0)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from opentelemetry-api>=1.30.0->crewai==0.114.0) (1.2.14)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from opentelemetry-exporter-otlp-proto-http>=1.30.0->crewai==0.114.0) (1.70.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.32.0 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from opentelemetry-exporter-otlp-proto-http>=1.30.0->crewai==0.114.0) (1.32.0)\n",
      "Requirement already satisfied: opentelemetry-proto==1.32.0 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from opentelemetry-exporter-otlp-proto-http>=1.30.0->crewai==0.114.0) (1.32.0)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.53b0 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from opentelemetry-sdk>=1.30.0->crewai==0.114.0) (0.53b0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from pandas<3->mlflow==2.21.2) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from pandas<3->mlflow==2.21.2) (2023.3)\n",
      "Requirement already satisfied: pdfminer.six==20250327 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from pdfplumber>=0.11.4->crewai==0.114.0) (20250327)\n",
      "Requirement already satisfied: pypdfium2>=4.18.0 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from pdfplumber>=0.11.4->crewai==0.114.0) (4.30.0)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in /Users/mikhailrozhkov/.local/lib/python3.11/site-packages (from pdfminer.six==20250327->pdfplumber>=0.11.4->crewai==0.114.0) (3.3.2)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from pydantic>=2.4.2->crewai==0.114.0) (0.6.0)\n",
      "Requirement already satisfied: nodeenv>=1.6.0 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from pyright>=1.1.350->crewai-tools) (1.9.1)\n",
      "Requirement already satisfied: ipython>=5.3.0 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from pyvis>=0.3.2->crewai==0.114.0) (8.18.1)\n",
      "Requirement already satisfied: jsonpickle>=1.4.1 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from pyvis>=0.3.2->crewai==0.114.0) (4.0.4)\n",
      "Requirement already satisfied: networkx>=1.11 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from pyvis>=0.3.2->crewai==0.114.0) (3.2.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from scikit-learn<2->mlflow==2.21.2) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from scikit-learn<2->mlflow==2.21.2) (3.4.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from aiohttp->litellm==1.60.2->crewai==0.114.0) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from aiohttp->litellm==1.60.2->crewai==0.114.0) (1.3.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from aiohttp->litellm==1.60.2->crewai==0.114.0) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from aiohttp->litellm==1.60.2->crewai==0.114.0) (6.0.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from aiohttp->litellm==1.60.2->crewai==0.114.0) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from aiohttp->litellm==1.60.2->crewai==0.114.0) (1.18.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from beautifulsoup4<5.0.0,>=4.12.2->embedchain>=0.1.114->crewai-tools) (2.5)\n",
      "Requirement already satisfied: pyproject_hooks in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from build>=1.0.3->chromadb>=0.5.23->crewai==0.114.0) (1.1.0)\n",
      "Requirement already satisfied: cffi>=1.12 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from cryptography>=43.0.1->auth0-python>=4.7.1->crewai==0.114.0) (1.16.0)\n",
      "Requirement already satisfied: google-auth~=2.0 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from databricks-sdk<1,>=0.20.0->mlflow-skinny==2.21.2->mlflow==2.21.2) (2.25.2)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from deprecated>=1.2.6->opentelemetry-api>=1.30.0->crewai==0.114.0) (1.16.0)\n",
      "Requirement already satisfied: starlette<0.38.0,>=0.37.2 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from fastapi<1->mlflow-skinny==2.21.2->mlflow==2.21.2) (0.37.2)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from gitpython<4,>=3.1.9->mlflow-skinny==2.21.2->mlflow==2.21.2) (4.0.11)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from importlib-metadata>=6.8.0->litellm==1.60.2->crewai==0.114.0) (3.19.2)\n",
      "Requirement already satisfied: decorator in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from ipython>=5.3.0->pyvis>=0.3.2->crewai==0.114.0) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from ipython>=5.3.0->pyvis>=0.3.2->crewai==0.114.0) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from ipython>=5.3.0->pyvis>=0.3.2->crewai==0.114.0) (0.1.6)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from ipython>=5.3.0->pyvis>=0.3.2->crewai==0.114.0) (3.0.41)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from ipython>=5.3.0->pyvis>=0.3.2->crewai==0.114.0) (2.17.2)\n",
      "Requirement already satisfied: stack-data in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from ipython>=5.3.0->pyvis>=0.3.2->crewai==0.114.0) (0.6.3)\n",
      "Requirement already satisfied: traitlets>=5 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from ipython>=5.3.0->pyvis>=0.3.2->crewai==0.114.0) (5.14.0)\n",
      "Requirement already satisfied: pexpect>4.3 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from ipython>=5.3.0->pyvis>=0.3.2->crewai==0.114.0) (4.9.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /Users/mikhailrozhkov/.local/lib/python3.11/site-packages (from jsonschema<5.0.0,>=4.22.0->litellm==1.60.2->crewai==0.114.0) (2023.11.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /Users/mikhailrozhkov/.local/lib/python3.11/site-packages (from jsonschema<5.0.0,>=4.22.0->litellm==1.60.2->crewai==0.114.0) (0.31.0)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /Users/mikhailrozhkov/.local/lib/python3.11/site-packages (from jsonschema<5.0.0,>=4.22.0->litellm==1.60.2->crewai==0.114.0) (0.13.0)\n",
      "Requirement already satisfied: six>=1.9.0 in /Users/mikhailrozhkov/.local/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb>=0.5.23->crewai==0.114.0) (1.16.0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb>=0.5.23->crewai==0.114.0) (1.8.0)\n",
      "Requirement already satisfied: requests-oauthlib in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb>=0.5.23->crewai==0.114.0) (1.3.1)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb>=0.5.23->crewai==0.114.0) (3.2.2)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.45 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from langchain<0.4.0,>=0.3.1->embedchain>=0.1.114->crewai-tools) (0.3.49)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.7 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from langchain<0.4.0,>=0.3.1->embedchain>=0.1.114->crewai-tools) (0.3.7)\n",
      "Requirement already satisfied: cohere<6.0,>=5.5.6 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from langchain-cohere<0.4.0,>=0.3.0->embedchain>=0.1.114->crewai-tools) (5.6.2)\n",
      "Requirement already satisfied: langchain-experimental<0.4.0,>=0.3.0 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from langchain-cohere<0.4.0,>=0.3.0->embedchain>=0.1.114->crewai-tools) (0.3.4)\n",
      "Requirement already satisfied: tabulate<0.10.0,>=0.9.0 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from langchain-cohere<0.4.0,>=0.3.0->embedchain>=0.1.114->crewai-tools) (0.9.0)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from langchain-community<0.4.0,>=0.3.1->embedchain>=0.1.114->crewai-tools) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from langchain-community<0.4.0,>=0.3.1->embedchain>=0.1.114->crewai-tools) (2.8.1)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from langchain-community<0.4.0,>=0.3.1->embedchain>=0.1.114->crewai-tools) (0.4.0)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from langsmith<0.4.0,>=0.3.18->embedchain>=0.1.114->crewai-tools) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from langsmith<0.4.0,>=0.3.18->embedchain>=0.1.114->crewai-tools) (0.23.0)\n",
      "Requirement already satisfied: azure-search-documents<12.0.0,>=11.5.0 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from mem0ai<0.2.0,>=0.1.54->embedchain>=0.1.114->crewai-tools) (11.5.2)\n",
      "Requirement already satisfied: psycopg2-binary<3.0.0,>=2.9.10 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from mem0ai<0.2.0,>=0.1.54->embedchain>=0.1.114->crewai-tools) (2.9.10)\n",
      "Requirement already satisfied: qdrant-client<2.0.0,>=1.9.1 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from mem0ai<0.2.0,>=0.1.54->embedchain>=0.1.114->crewai-tools) (1.10.1)\n",
      "Requirement already satisfied: coloredlogs in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from onnxruntime>=1.14.1->chromadb>=0.5.23->crewai==0.114.0) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from onnxruntime>=1.14.1->chromadb>=0.5.23->crewai==0.114.0) (24.3.25)\n",
      "Requirement already satisfied: sympy in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from onnxruntime>=1.14.1->chromadb>=0.5.23->crewai==0.114.0) (1.12)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-asgi==0.53b0 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb>=0.5.23->crewai==0.114.0) (0.53b0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation==0.53b0 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb>=0.5.23->crewai==0.114.0) (0.53b0)\n",
      "Requirement already satisfied: opentelemetry-util-http==0.53b0 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb>=0.5.23->crewai==0.114.0) (0.53b0)\n",
      "Requirement already satisfied: asgiref~=3.0 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from opentelemetry-instrumentation-asgi==0.53b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb>=0.5.23->crewai==0.114.0) (3.8.1)\n",
      "Requirement already satisfied: monotonic>=1.5 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from posthog>=2.4.0->chromadb>=0.5.23->crewai==0.114.0) (1.6)\n",
      "Requirement already satisfied: backoff>=1.10.0 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from posthog>=2.4.0->chromadb>=0.5.23->crewai==0.114.0) (2.2.1)\n",
      "Requirement already satisfied: py<2.0.0,>=1.4.26 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from retry>=0.9.2->lancedb>=0.5.4->crewai-tools) (1.11.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from rich>=10.11.0->chromadb>=0.5.23->crewai==0.114.0) (3.0.0)\n",
      "Requirement already satisfied: contextlib2>=0.5.5 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from schema<0.8.0,>=0.7.5->embedchain>=0.1.114->crewai-tools) (21.6.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from tokenizers->litellm==1.60.2->crewai==0.114.0) (0.26.1)\n",
      "Requirement already satisfied: httptools>=0.5.0 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb>=0.5.23->crewai==0.114.0) (0.6.1)\n",
      "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb>=0.5.23->crewai==0.114.0) (0.19.0)\n",
      "Requirement already satisfied: watchfiles>=0.13 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb>=0.5.23->crewai==0.114.0) (0.22.0)\n",
      "Requirement already satisfied: websockets>=10.4 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb>=0.5.23->crewai==0.114.0) (12.0)\n",
      "Requirement already satisfied: azure-core>=1.28.0 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from azure-search-documents<12.0.0,>=11.5.0->mem0ai<0.2.0,>=0.1.54->embedchain>=0.1.114->crewai-tools) (1.29.6)\n",
      "Requirement already satisfied: azure-common>=1.1 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from azure-search-documents<12.0.0,>=11.5.0->mem0ai<0.2.0,>=0.1.54->embedchain>=0.1.114->crewai-tools) (1.1.28)\n",
      "Requirement already satisfied: isodate>=0.6.0 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from azure-search-documents<12.0.0,>=11.5.0->mem0ai<0.2.0,>=0.1.54->embedchain>=0.1.114->crewai-tools) (0.6.1)\n",
      "Requirement already satisfied: pycparser in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from cffi>=1.12->cryptography>=43.0.1->auth0-python>=4.7.1->crewai==0.114.0) (2.21)\n",
      "Requirement already satisfied: boto3<2.0.0,>=1.34.0 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from cohere<6.0,>=5.5.6->langchain-cohere<0.4.0,>=0.3.0->embedchain>=0.1.114->crewai-tools) (1.34.147)\n",
      "Requirement already satisfied: fastavro<2.0.0,>=1.9.4 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from cohere<6.0,>=5.5.6->langchain-cohere<0.4.0,>=0.3.0->embedchain>=0.1.114->crewai-tools) (1.9.5)\n",
      "Requirement already satisfied: parameterized<0.10.0,>=0.9.0 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from cohere<6.0,>=5.5.6->langchain-cohere<0.4.0,>=0.3.0->embedchain>=0.1.114->crewai-tools) (0.9.0)\n",
      "Requirement already satisfied: types-requests<3.0.0,>=2.0.0 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from cohere<6.0,>=5.5.6->langchain-cohere<0.4.0,>=0.3.0->embedchain>=0.1.114->crewai-tools) (2.32.0.20240712)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /Users/mikhailrozhkov/.local/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community<0.4.0,>=0.3.1->embedchain>=0.1.114->crewai-tools) (3.20.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community<0.4.0,>=0.3.1->embedchain>=0.1.114->crewai-tools) (0.9.0)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow-skinny==2.21.2->mlflow==2.21.2) (5.0.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.21.2->mlflow==2.21.2) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.21.2->mlflow==2.21.2) (4.9)\n",
      "Requirement already satisfied: filelock in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm==1.60.2->crewai==0.114.0) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm==1.60.2->crewai==0.114.0) (2024.2.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from jedi>=0.16->ipython>=5.3.0->pyvis>=0.3.2->crewai==0.114.0) (0.8.3)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from langchain-core<1.0.0,>=0.3.45->langchain<0.4.0,>=0.3.1->embedchain>=0.1.114->crewai-tools) (1.33)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb>=0.5.23->crewai==0.114.0) (0.1.2)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from pexpect>4.3->ipython>=5.3.0->pyvis>=0.3.2->crewai==0.114.0) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=5.3.0->pyvis>=0.3.2->crewai==0.114.0) (0.2.12)\n",
      "Requirement already satisfied: grpcio-tools>=1.41.0 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from qdrant-client<2.0.0,>=1.9.1->mem0ai<0.2.0,>=0.1.54->embedchain>=0.1.114->crewai-tools) (1.71.0)\n",
      "Requirement already satisfied: portalocker<3.0.0,>=2.7.0 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from qdrant-client<2.0.0,>=1.9.1->mem0ai<0.2.0,>=0.1.54->embedchain>=0.1.114->crewai-tools) (2.8.2)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb>=0.5.23->crewai==0.114.0) (10.0)\n",
      "Requirement already satisfied: executing>=1.2.0 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from stack-data->ipython>=5.3.0->pyvis>=0.3.2->crewai==0.114.0) (2.0.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from stack-data->ipython>=5.3.0->pyvis>=0.3.2->crewai==0.114.0) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from stack-data->ipython>=5.3.0->pyvis>=0.3.2->crewai==0.114.0) (0.2.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from sympy->onnxruntime>=1.14.1->chromadb>=0.5.23->crewai==0.114.0) (1.3.0)\n",
      "Requirement already satisfied: botocore<1.35.0,>=1.34.147 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from boto3<2.0.0,>=1.34.0->cohere<6.0,>=5.5.6->langchain-cohere<0.4.0,>=0.3.0->embedchain>=0.1.114->crewai-tools) (1.34.162)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from boto3<2.0.0,>=1.34.0->cohere<6.0,>=5.5.6->langchain-cohere<0.4.0,>=0.3.0->embedchain>=0.1.114->crewai-tools) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from boto3<2.0.0,>=1.34.0->cohere<6.0,>=5.5.6->langchain-cohere<0.4.0,>=0.3.0->embedchain>=0.1.114->crewai-tools) (0.10.2)\n",
      "Requirement already satisfied: setuptools in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from grpcio-tools>=1.41.0->qdrant-client<2.0.0,>=1.9.1->mem0ai<0.2.0,>=0.1.54->embedchain>=0.1.114->crewai-tools) (75.2.0)\n",
      "Requirement already satisfied: h2<5,>=3 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from httpx[http2]>=0.20.0->qdrant-client<2.0.0,>=1.9.1->mem0ai<0.2.0,>=0.1.54->embedchain>=0.1.114->crewai-tools) (4.1.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.45->langchain<0.4.0,>=0.3.1->embedchain>=0.1.114->crewai-tools) (2.4)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from pyasn1-modules>=0.2.1->google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.21.2->mlflow==2.21.2) (0.5.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community<0.4.0,>=0.3.1->embedchain>=0.1.114->crewai-tools) (1.0.0)\n",
      "Requirement already satisfied: hyperframe<7,>=6.0 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client<2.0.0,>=1.9.1->mem0ai<0.2.0,>=0.1.54->embedchain>=0.1.114->crewai-tools) (6.0.1)\n",
      "Requirement already satisfied: hpack<5,>=4.0 in /Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client<2.0.0,>=1.9.1->mem0ai<0.2.0,>=0.1.54->embedchain>=0.1.114->crewai-tools) (4.0.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install mlflow==2.21.2 python-dotenv openai crewai==0.114.0 crewai-tools duckduckgo-search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Secrets and Environment Variables\n",
    "\n",
    "Set the following environment variables where this notebook is running, so that the code in the following cells can connect to both Nebius Managed Service for MLflow and Nebius AI Studio. \n",
    "\n",
    "MLflow:<br>\n",
    "`MLFLOW_TRACKING_SERVER_CERT_PATH`<br>\n",
    "`MLFLOW_TRACKING_URI`<br>\n",
    "`MLFLOW_TRACKING_USERNAME`<br>\n",
    "`MLFLOW_TRACKING_PASSWORD`<br>\n",
    "\n",
    "AI Studio:<br>\n",
    "`NEBIUS_API_KEY`\n",
    "\n",
    "### Environment Setup\n",
    "\n",
    "To set the environment variables, run the following cell. You may choose to set them interactively or by loading from a `.env` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-17T05:30:06.400685Z",
     "iopub.status.busy": "2025-04-17T05:30:06.400254Z",
     "iopub.status.idle": "2025-04-17T05:30:06.423121Z",
     "shell.execute_reply": "2025-04-17T05:30:06.422843Z",
     "shell.execute_reply.started": "2025-04-17T05:30:06.400641Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load ENV variables from file: ../.env\n",
      "All required environment variables are set.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Add the parent directory to Python path\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "\n",
    "from env_setup import setup_env_from_file, setup_env_interactive, verify_env_setup\n",
    "\n",
    "# Option 1: Interactive setup\n",
    "# setup_env_interactive()\n",
    "\n",
    "# Option 2: Load from .env file\n",
    "setup_env_from_file('../.env')\n",
    "\n",
    "# Verify the setup\n",
    "verify_env_setup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check connection to MLflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-17T05:30:13.847165Z",
     "iopub.status.busy": "2025-04-17T05:30:13.846758Z",
     "iopub.status.idle": "2025-04-17T05:30:15.936613Z",
     "shell.execute_reply": "2025-04-17T05:30:15.936252Z",
     "shell.execute_reply.started": "2025-04-17T05:30:13.847130Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Experiment: artifact_location='mlflow-artifacts:/9', creation_time=1744833056702, experiment_id='9', last_update_time=1744833056702, lifecycle_stage='active', name='Step 4 - Evaluate Agent', tags={}>,\n",
       " <Experiment: artifact_location='mlflow-artifacts:/8', creation_time=1744828328349, experiment_id='8', last_update_time=1744828328349, lifecycle_stage='active', name='Step 4 - Update prompts', tags={}>,\n",
       " <Experiment: artifact_location='mlflow-artifacts:/7', creation_time=1744747169730, experiment_id='7', last_update_time=1744747169730, lifecycle_stage='active', name='Step 3 - Metrics', tags={}>,\n",
       " <Experiment: artifact_location='mlflow-artifacts:/6', creation_time=1744745999110, experiment_id='6', last_update_time=1744745999110, lifecycle_stage='active', name='Step 2 - Autolog', tags={}>,\n",
       " <Experiment: artifact_location='mlflow-artifacts:/1', creation_time=1744712739337, experiment_id='1', last_update_time=1744712739337, lifecycle_stage='active', name='CrewAI Agent Debugging Demo', tags={}>,\n",
       " <Experiment: artifact_location='mlflow-artifacts:/0', creation_time=1744710198517, experiment_id='0', last_update_time=1744710198517, lifecycle_stage='active', name='Default', tags={}>]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mlflow \n",
    "\n",
    "# List experiments in MLflow\n",
    "mlflow.search_experiments()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Quick implementation of an AI Agent with CrewAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-17T05:34:29.053211Z",
     "iopub.status.busy": "2025-04-17T05:34:29.052773Z",
     "iopub.status.idle": "2025-04-17T05:34:32.277445Z",
     "shell.execute_reply": "2025-04-17T05:34:32.277110Z",
     "shell.execute_reply.started": "2025-04-17T05:34:29.053180Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages/pydantic/_internal/_config.py:291: PydanticDeprecatedSince20: Support for class-based `config` is deprecated, use ConfigDict instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.8/migration/\n",
      "  warnings.warn(DEPRECATION_MESSAGE, DeprecationWarning)\n",
      "/Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages/paramiko/pkey.py:100: CryptographyDeprecationWarning: TripleDES has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.TripleDES and will be removed from cryptography.hazmat.primitives.ciphers.algorithms in 48.0.0.\n",
      "  \"cipher\": algorithms.TripleDES,\n",
      "/Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages/paramiko/transport.py:258: CryptographyDeprecationWarning: TripleDES has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.TripleDES and will be removed from cryptography.hazmat.primitives.ciphers.algorithms in 48.0.0.\n",
      "  \"class\": algorithms.TripleDES,\n",
      "/Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages/pydantic/_internal/_fields.py:201: UserWarning: Field name \"schema\" in \"DatabricksQueryToolSchema\" shadows an attribute in parent \"BaseModel\"\n",
      "  warnings.warn(\n",
      "/Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:404: UserWarning: <built-in function callable> is not a Python type (it may be an instance of an object), Pydantic will allow any object with no validation since we cannot even enforce that the input is an instance of the given type. To get rid of this error wrap the type with `pydantic.SkipValidation`.\n",
      "  warn(\n",
      "/Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages/crewai_tools/tools/scrapegraph_scrape_tool/scrapegraph_scrape_tool.py:34: PydanticDeprecatedSince20: Pydantic V1 style `@validator` validators are deprecated. You should migrate to Pydantic V2 style `@field_validator` validators, see the migration guide for more details. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.8/migration/\n",
      "  @validator(\"website_url\")\n",
      "/Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages/crewai_tools/tools/selenium_scraping_tool/selenium_scraping_tool.py:26: PydanticDeprecatedSince20: Pydantic V1 style `@validator` validators are deprecated. You should migrate to Pydantic V2 style `@field_validator` validators, see the migration guide for more details. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.8/migration/\n",
      "  @validator(\"website_url\")\n",
      "/Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages/crewai_tools/tools/vision_tool/vision_tool.py:15: PydanticDeprecatedSince20: Pydantic V1 style `@validator` validators are deprecated. You should migrate to Pydantic V2 style `@field_validator` validators, see the migration guide for more details. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.8/migration/\n",
      "  @validator(\"image_path_url\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLflow Version: 2.21.2\n",
      "CrewAI Version: 0.114.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import io\n",
    "import time\n",
    "import json\n",
    "import traceback\n",
    "import contextlib\n",
    "from textwrap import dedent\n",
    "from crewai import Agent, Crew, Task, Process\n",
    "from crewai import __version__ as crewai_version\n",
    "from crewai_tools import WebsiteSearchTool\n",
    "from langchain_openai import ChatOpenAI\n",
    "import mlflow\n",
    "\n",
    "print(f\"MLflow Version: {mlflow.__version__}\")\n",
    "print(f\"CrewAI Version: {crewai_version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-17T05:34:38.065626Z",
     "iopub.status.busy": "2025-04-17T05:34:38.064882Z",
     "iopub.status.idle": "2025-04-17T05:34:38.217782Z",
     "shell.execute_reply": "2025-04-17T05:34:38.217511Z",
     "shell.execute_reply.started": "2025-04-17T05:34:38.065593Z"
    }
   },
   "outputs": [],
   "source": [
    "# To search across any discovered websites\n",
    "\n",
    "search_tool = WebsiteSearchTool()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define an Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-17T05:43:02.712739Z",
     "iopub.status.busy": "2025-04-17T05:43:02.712192Z",
     "iopub.status.idle": "2025-04-17T05:43:02.741460Z",
     "shell.execute_reply": "2025-04-17T05:43:02.740969Z",
     "shell.execute_reply.started": "2025-04-17T05:43:02.712704Z"
    }
   },
   "outputs": [],
   "source": [
    "class ResearchAgents:\n",
    "    def researcher_agent(self):\n",
    "        \"\"\"Agent responsible for discovering relevant AI tools for specific tasks.\"\"\"\n",
    "        return Agent(\n",
    "            role=\"Senior Data Researcher\",\n",
    "            goal=\"Discover and evaluate relevant AI tools (libraries, frameworks) for specific tasks\",\n",
    "            backstory=\"An expert in AI development and MLOps\",\n",
    "            tools=[search_tool],\n",
    "            verbose=True,\n",
    "            max_iter=3\n",
    "        )\n",
    "\n",
    "    def analyst_agent(self):\n",
    "        \"\"\"Agent responsible for analyzing tools and creating detailed reports.\"\"\"\n",
    "        return Agent(\n",
    "            role=\"Reporting Analyst\",\n",
    "            goal=\"Create detailed reports based on data analysis and research findings\",\n",
    "            backstory=\"\"\"You are a technical expert specializing in AI technologies evaluation. \n",
    "    You have a deep understanding of AI libraries and frameworks, and can \n",
    "    quickly assess their technical merits\"\"\",\n",
    "            tools=[search_tool],\n",
    "            verbose=True,\n",
    "            max_iter=3 # Maximum iterations before the agent must provide its best answer. Default is 20.\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-17T05:43:05.133172Z",
     "iopub.status.busy": "2025-04-17T05:43:05.132809Z",
     "iopub.status.idle": "2025-04-17T05:43:05.158397Z",
     "shell.execute_reply": "2025-04-17T05:43:05.157967Z",
     "shell.execute_reply.started": "2025-04-17T05:43:05.133143Z"
    }
   },
   "outputs": [],
   "source": [
    "class AIOpsResearchTasks:\n",
    "    def search_tools_task(self, agent, task, ai_stack):\n",
    "        \"\"\"Task to discover relevant tools for a specific task considering the existing AI stack.\"\"\"\n",
    "        return Task(\n",
    "            description=dedent(f\"\"\"\n",
    "                You are a research agent tasked with finding AI tools for: {{task}}.\n",
    "                Consider compatibility with: {{ai_stack}}.\n",
    "                \n",
    "                For each tool, identify:\n",
    "                - Name and URL\n",
    "                - Primary use case\n",
    "                - Brief description (2-3 sentences)\n",
    "                \n",
    "                Format your response as a JSON list of objects.\n",
    "            \"\"\"),\n",
    "            agent=agent,\n",
    "            expected_output=\"A JSON list containing details of 3-5 relevant AI tools with their names, URLs, use cases, and descriptions.\",\n",
    "            output_file=\"output/tool_candidates.json\"\n",
    "        )\n",
    "\n",
    "    def analyze_tools_task(self, agent, task, ai_stack):\n",
    "        \"\"\"Task to perform in-depth analysis of discovered tools.\"\"\"\n",
    "        return Task(\n",
    "            description=dedent(f\"\"\"\n",
    "                Read the tool list from the previous task's result and perform a detailed analysis of each tool.\n",
    "                \n",
    "                For each tool:\n",
    "                1. Research its capabilities, limitations, community adoption, documentation quality\n",
    "                2. Evaluate how well it addresses the specified task: '{task}'\n",
    "                3. Consider its compatibility with '{ai_stack}'\n",
    "                4. Identify pros and cons\n",
    "                \n",
    "                Your output should be a JSON list of these detailed analysis objects.\n",
    "            \"\"\"),\n",
    "            agent=agent,\n",
    "            expected_output=\"A JSON list containing detailed analysis of each tool with comprehensive information about features, pros, cons, and recommendation scores.\",\n",
    "            output_file=\"output/tool_analysis.json\",\n",
    "        )\n",
    "\n",
    "    def create_report_task(self, agent, task, ai_stack):\n",
    "        \"\"\"Task to create a comprehensive report with recommendations.\"\"\"\n",
    "        return Task(\n",
    "            description=dedent(f\"\"\"\n",
    "                Read the analysis from the previous task's result and create a comprehensive Markdown report.\n",
    "                \n",
    "                The report should include:\n",
    "                \n",
    "                1. An introduction explaining the task ('{task}') and existing stack ('{ai_stack}')\n",
    "                2. For each tool, create a section with:\n",
    "                   - Tool name and URL as a heading\n",
    "                   - Description\n",
    "                   - Features (as bullet points)\n",
    "                   - Pros (as bullet points)\n",
    "                   - Cons (as bullet points)\n",
    "                   - Integration complexity\n",
    "                   - Recommendation score with justification\n",
    "                3. A summary/conclusion comparing the tools and providing final recommendations\n",
    "                \n",
    "                Use proper Markdown formatting with headings, bullet points, and emphasis where appropriate.\n",
    "                Sort tools by recommendation score (descending).\n",
    "                \n",
    "                Your output should be a complete, well-formatted Markdown document.\n",
    "            \"\"\"),\n",
    "            agent=agent,\n",
    "            expected_output=\"A comprehensive Markdown report analyzing each tool with recommendations, properly formatted with headings, bullet points, and clear sections.\",\n",
    "            output_file=\"output/tool_recommendation_report.md\",\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T20:48:17.410964Z",
     "iopub.status.busy": "2025-03-29T20:48:17.410280Z",
     "iopub.status.idle": "2025-03-29T20:48:17.452481Z",
     "shell.execute_reply": "2025-03-29T20:48:17.452093Z",
     "shell.execute_reply.started": "2025-03-29T20:48:17.410925Z"
    }
   },
   "source": [
    "### Design a Crew \n",
    "\n",
    "A crew in crewAI is a collaborative group of agents working together to complete tasks. Crews define:\n",
    "\n",
    "- Task execution strategy\n",
    "- Agent collaboration methods\n",
    "- Overall workflow coordination\n",
    "- Communication patterns between agents\n",
    "- Task delegation and sequencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-17T05:53:12.404280Z",
     "iopub.status.busy": "2025-04-17T05:53:12.403777Z",
     "iopub.status.idle": "2025-04-17T05:53:12.454160Z",
     "shell.execute_reply": "2025-04-17T05:53:12.453629Z",
     "shell.execute_reply.started": "2025-04-17T05:53:12.404243Z"
    }
   },
   "outputs": [],
   "source": [
    "class AIOpsResearchCrew:\n",
    "    def __init__(self, task, ai_stack):\n",
    "        \"\"\"\n",
    "        Initialize the crew with the task description and existing AI stack.\n",
    "        \n",
    "        Args:\n",
    "            task (str): Description of the task requiring AI tools\n",
    "            ai_stack (str): Comma-separated list of existing tools/frameworks used\n",
    "        \"\"\"\n",
    "        self.task = task\n",
    "        self.ai_stack = ai_stack\n",
    "        \n",
    "        # Ensure output directory exists\n",
    "        os.makedirs(\"output\", exist_ok=True)\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"Execute the research, analysis, and reporting process.\"\"\"\n",
    "        # Initialize agents\n",
    "        agents = AIOpsResearchAgents()\n",
    "        researcher = agents.researcher_agent()\n",
    "        analyst = agents.analyst_agent()\n",
    "\n",
    "        # Initialize tasks\n",
    "        tasks = AIOpsResearchTasks()\n",
    "        search_task = tasks.search_tools_task(researcher, self.task, self.ai_stack)\n",
    "        analyze_task = tasks.analyze_tools_task(analyst, self.task, self.ai_stack)\n",
    "        report_task = tasks.create_report_task(analyst, self.task, self.ai_stack)\n",
    "        \n",
    "        # Create the crew\n",
    "        crew = Crew(\n",
    "            agents=[researcher, analyst],\n",
    "            tasks=[search_task, analyze_task, report_task],\n",
    "            verbose=True,\n",
    "            process=Process.sequential,\n",
    "            memory=True\n",
    "        )\n",
    "        \n",
    "        result = crew.kickoff()\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Run (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-17T05:53:16.688111Z",
     "iopub.status.busy": "2025-04-17T05:53:16.687753Z",
     "iopub.status.idle": "2025-04-17T05:54:48.021310Z",
     "shell.execute_reply": "2025-04-17T05:54:48.020530Z",
     "shell.execute_reply.started": "2025-04-17T05:53:16.688083Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\"> Crew Execution Started </span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\"></span>                                                                                                                 <span style=\"color: #008080; text-decoration-color: #008080\"></span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\"></span>  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Crew Execution Started</span>                                                                                         <span style=\"color: #008080; text-decoration-color: #008080\"></span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\"></span>  <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">Name: </span><span style=\"color: #008080; text-decoration-color: #008080\">crew</span>                                                                                                     <span style=\"color: #008080; text-decoration-color: #008080\"></span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\"></span>  <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">ID: </span><span style=\"color: #008080; text-decoration-color: #008080\">1c9e5c97-c97d-4678-bc5e-23f5b2e7e4aa</span>                                                                       <span style=\"color: #008080; text-decoration-color: #008080\"></span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\"></span>                                                                                                                 <span style=\"color: #008080; text-decoration-color: #008080\"></span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\"></span>                                                                                                                 <span style=\"color: #008080; text-decoration-color: #008080\"></span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\"></span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36m\u001b[0m\u001b[36m\u001b[0m\u001b[36m Crew Execution Started \u001b[0m\u001b[36m\u001b[0m\u001b[36m\u001b[0m\n",
       "\u001b[36m\u001b[0m                                                                                                                 \u001b[36m\u001b[0m\n",
       "\u001b[36m\u001b[0m  \u001b[1;36mCrew Execution Started\u001b[0m                                                                                         \u001b[36m\u001b[0m\n",
       "\u001b[36m\u001b[0m  \u001b[37mName: \u001b[0m\u001b[36mcrew\u001b[0m                                                                                                     \u001b[36m\u001b[0m\n",
       "\u001b[36m\u001b[0m  \u001b[37mID: \u001b[0m\u001b[36m1c9e5c97-c97d-4678-bc5e-23f5b2e7e4aa\u001b[0m                                                                       \u001b[36m\u001b[0m\n",
       "\u001b[36m\u001b[0m                                                                                                                 \u001b[36m\u001b[0m\n",
       "\u001b[36m\u001b[0m                                                                                                                 \u001b[36m\u001b[0m\n",
       "\u001b[36m\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> Crew: crew</span>\n",
       " <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> Task: 27ec6611-922a-4fb3-9047-a980986e67a0</span>\n",
       "    <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">   Status: </span><span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">Executing Task...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m Crew: \u001b[0m\u001b[1;36mcrew\u001b[0m\n",
       " \u001b[1;33m Task: 27ec6611-922a-4fb3-9047-a980986e67a0\u001b[0m\n",
       "    \u001b[37m   Status: \u001b[0m\u001b[2;33mExecuting Task...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> Crew: crew</span>\n",
       " <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> Task: 27ec6611-922a-4fb3-9047-a980986e67a0</span>\n",
       "    <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">   Status: </span><span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">Executing Task...</span>\n",
       "     <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> Agent: </span><span style=\"color: #008000; text-decoration-color: #008000\">Senior Data Researcher</span>\n",
       "        <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">    Status: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">In Progress</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m Crew: \u001b[0m\u001b[1;36mcrew\u001b[0m\n",
       " \u001b[1;33m Task: 27ec6611-922a-4fb3-9047-a980986e67a0\u001b[0m\n",
       "    \u001b[37m   Status: \u001b[0m\u001b[2;33mExecuting Task...\u001b[0m\n",
       "     \u001b[1;32m Agent: \u001b[0m\u001b[32mSenior Data Researcher\u001b[0m\n",
       "        \u001b[37m    Status: \u001b[0m\u001b[1;32mIn Progress\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mSenior Data Researcher\u001b[00m\n",
      "\u001b[95m## Task:\u001b[00m \u001b[92m\n",
      "You are a research agent tasked with finding AI tools for: {task}.\n",
      "Consider compatibility with: {ai_stack}.\n",
      "\n",
      "For each tool, identify:\n",
      "- Name and URL\n",
      "- Primary use case\n",
      "- Brief description (2-3 sentences)\n",
      "\n",
      "Format your response as a JSON list of objects.\n",
      "\u001b[00m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> Agent: </span><span style=\"color: #008000; text-decoration-color: #008000\">Senior Data Researcher</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">    Status: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">In Progress</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32m Agent: \u001b[0m\u001b[32mSenior Data Researcher\u001b[0m\n",
       "\u001b[37m    Status: \u001b[0m\u001b[1;32mIn Progress\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mSenior Data Researcher\u001b[00m\n",
      "\u001b[95m## Thought:\u001b[00m \u001b[92mThought: I need to find relevant AI tools for developing a conversational RAG system that is compatible with LangChain, PostgreSQL, and FastAPI. I will search for AI tools that fit these criteria and provide detailed information including their names, URLs, use cases, and descriptions.\u001b[00m\n",
      "\u001b[95m## Using tool:\u001b[00m \u001b[92mSearch in a specific website\u001b[00m\n",
      "\u001b[95m## Tool Input:\u001b[00m \u001b[92m\n",
      "\"{\\\"search_query\\\": \\\"AI tools for developing conversational Retrieval-Augmented Generation system compatible with LangChain, PostgreSQL, and FastAPI\\\", \\\"website\\\": \\\"https://www.tensorflow.org/\\\"}\"\u001b[00m\n",
      "\u001b[95m## Tool Output:\u001b[00m \u001b[92m\n",
      "Relevant Content:\n",
      "Visionary in Gartner Magic Quadrant: Data Science and Machine Learning 2024 Listed in Gartner Magic Quadrant (Cloud AI Developer Services) in 2020, 2021, 2022, 2023, and 2024 Listed in Gartner Magic Quadrant (Data Science and Machine Learning) in 2018, 2019, 2020, 2021, 2023, and 2024 Get the Report GenAI App Store As the maker of the worlds best AutoML, H2O.ai is converging Predictive AI with Generative AI, and helping leading global brands save billions of dollars a year with purpose-built AI for hundreds of use cases such as: Fraud and scam detection Complaint summarizer Recommendation engine KYC, loan application risk Pricing engines and more Try Sample GenAI Apps Built On h2oGPT Get your API key H2O Generative AI can solve a broad spectrum of natural language use cases, including classification, semantic search, paraphrasing, summarization, and content generation. h2oGPTe is used by companies to build production-ready, scalable and efficient AI-powered applications. Write custom apps with fully featured Python client APIs Seamlessly integrate state-of-the-art text, image, and code generation capabilities into your projects Get your API Key\n",
      "\n",
      "H2O.ai | Convergence of The World's Best Predictive & Generative AI The World's Best Deep Research Chat Now Chat on Mobile Learn More Companies powered by H2O.ai H2Os flagship SLMs H2OVL Mississippi SVLM Series Our newest economical multimodal OCR model developed for Document AI H2OVL Mississippi-2B, based on H2O Danube2, is trained on 17.3M conversation pairs for high-res image handling. The .8B model, built on Danube3, leads OCR benchmarks with 19M pairs, outperforming all SLMs in text recognition. For Developers  1 Text Recognition H2O Danube SLM Series Our most economical small model for fast, lightweight tasks We trained H2O Danube3 models from scratch on ~100 H100 GPUs using our own curated dataset of 6T tokens. H2O Danube3-4B and .5B open-weight SLMs outperform the latest Apple OpenELM-3B and .5B instruct models. Perfect for developers who want to fine-tune their own SLMs for offline use cases. For Developers Get the Mobile App H2O Generative AI in use with customers H2O Document AI GPTs and Digital Agents in production worldwide In-Store Sales Assistant HR & IT Support Agent Fraud & Scam Detection RFP Assistant Procurement GPT Personalized Pricing Complaint Summarizer Distilled SLMs for Call Center Classification First-ever model risk management for Generative AI Generative AI is herebut can you audit it? H2O.ai brings Model Risk Management (MRM) to GenAI, combining enterprise-grade model evaluation with h2oGPTe and H2O Model Validation to make AI measurable, explainable, and compliant for banks and financial services. Automated testing, human-calibrated evaluations, and real-time risk monitoring ensure transparencyboosting compliance, risk management, and user confidence. H2O Model Validation Multi-modal Document AI with enterprise scale and security Convert Data to JSON Document AI ProcurementGPT Complaint Summarizer Convert structured data like financial tables to JSON Get answers from diagrams, flowcharts, web pages, audio files, video files and\n",
      "\n",
      "LangChain Sign up to be the first to access recordings from Interrupt, The AI Agent Conference!Learn MoreFrom startups to global enterprises, ambitious builders choose LangChain products.BuildLangChain is a composable framework to build with LLMs. LangGraph is the orchestration framework for controllable agentic workflows.RunDeploy your LLM applications at scale with LangGraph Platform, our infrastructure purpose-built for agents.ManageLangSmith is a unified agent observability and evals platform to optimize the performance of your AI agents - whether they're built with a LangChain framework or not. Build your app with LangChainBuild context-aware, reasoning applications with LangChains flexible framework that leverages your companys data and APIs. Future-proof your application by making vendor optionality part of your LLM infrastructure design.Learn more about LangChainRun at scale with LangGraph PlatformUse LangGraph Platforms APIs to design agent-driven user experiences featuring human-in-the-loop, multi-agent collaboration, conversation history, long-term memory, and time-travel. Deploy with fault-tolerant scalability.Learn more about LangGraph PlatformManage agent observability & performance with LangSmithGet visibility into your LLM calls and agent reasoning with LangSmith to quickly debug failures. Run evals to assess your agent performance over production traffic  so you can ensure high-quality responses. Learn more about LangSmithHear from our happy customersLangChain, LangGraph, and LangSmith help teams of all sizes, across all industries - from ambitious startups to established enterprises.By combining the benefits of LangSmith and standing on the shoulders of a gigantic open-source community, were able to identify the right approaches of using LLMs in an enterprise-setting faster.Yusuke KajiGeneral Manager of AIWorking with LangChain and LangSmith on the Elastic AI Assistant had a significant positive impact on the overall pace and quality of the\u001b[00m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> Agent: </span><span style=\"color: #008000; text-decoration-color: #008000\">Senior Data Researcher</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">    Status: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">In Progress</span>\n",
       " <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\"> </span><span style=\"color: #000080; text-decoration-color: #000080\">Thinking...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32m Agent: \u001b[0m\u001b[32mSenior Data Researcher\u001b[0m\n",
       "\u001b[37m    Status: \u001b[0m\u001b[1;32mIn Progress\u001b[0m\n",
       " \u001b[1;34m \u001b[0m\u001b[34mThinking...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> Agent: </span><span style=\"color: #008000; text-decoration-color: #008000\">Senior Data Researcher</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">    Status: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">In Progress</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32m Agent: \u001b[0m\u001b[32mSenior Data Researcher\u001b[0m\n",
       "\u001b[37m    Status: \u001b[0m\u001b[1;32mIn Progress\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mSenior Data Researcher\u001b[00m\n",
      "\u001b[95m## Final Answer:\u001b[00m \u001b[92m\n",
      "[\n",
      "    {\n",
      "        \"name\": \"H2O.ai\",\n",
      "        \"url\": \"https://www.h2o.ai/\",\n",
      "        \"primary_use_case\": \"Automated Machine Learning and AI Model Deployment\",\n",
      "        \"description\": \"H2O.ai converges Predictive AI with Generative AI, providing tools for various use cases such as fraud detection and recommendation engines. It simplifies the building and deployment of scalable, efficient AI applications with a strong focus on natural language use cases, making it ideal for conversational systems.\"\n",
      "    },\n",
      "    {\n",
      "        \"name\": \"LangChain\",\n",
      "        \"url\": \"https://langchain.com/\",\n",
      "        \"primary_use_case\": \"Framework for LLM applications\",\n",
      "        \"description\": \"LangChain is a composable framework designed to build applications powered by language models (LLMs). It facilitates the creation of context-aware applications and integrates seamlessly with various data sources and APIs, making it a perfect fit for building a conversational RAG system.\"\n",
      "    },\n",
      "    {\n",
      "        \"name\": \"TensorFlow\",\n",
      "        \"url\": \"https://www.tensorflow.org/\",\n",
      "        \"primary_use_case\": \"Deep Learning Framework\",\n",
      "        \"description\": \"TensorFlow, developed by Google, is an open-source library that provides a comprehensive ecosystem for building and training machine learning models, particularly deep learning models. Its robust capabilities make it suitable for complex tasks such as conversational AI systems.\"\n",
      "    },\n",
      "    {\n",
      "        \"name\": \"PyTorch\",\n",
      "        \"url\": \"https://pytorch.org/\",\n",
      "        \"primary_use_case\": \"Flexibility in Deep Learning Frameworks\",\n",
      "        \"description\": \"Developed by Facebook, PyTorch is a widely used deep learning framework known for its flexibility and ease of use. It supports dynamic computation graphs, making it ideal for research and prototyping applications, including those related to conversational technologies.\"\n",
      "    },\n",
      "    {\n",
      "        \"name\": \"Keras\",\n",
      "        \"url\": \"https://keras.io/\",\n",
      "        \"primary_use_case\": \"High-level Neural Network API\",\n",
      "        \"description\": \"Keras is a high-level API for building and training deep learning models, acting as an interface for TensorFlow. It allows for rapid prototyping and experimentation, which is beneficial for developing conversational AI models.\"\n",
      "    }\n",
      "]\u001b[00m\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages/instructor/process_response.py:222: DeprecationWarning: FUNCTIONS is deprecated and will be removed in future versions\n",
      "  if mode == Mode.FUNCTIONS:\n",
      "/Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages/instructor/function_calls.py:118: DeprecationWarning: FUNCTIONS is deprecated and will be removed in future versions\n",
      "  if mode == Mode.FUNCTIONS:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> Crew: crew</span>\n",
       " <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> Task: 27ec6611-922a-4fb3-9047-a980986e67a0</span>\n",
       "    <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">   Status: </span><span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">Executing Task...</span>\n",
       "     <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> Agent: </span><span style=\"color: #008000; text-decoration-color: #008000\">Senior Data Researcher</span>\n",
       "        <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">    Status: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> Completed</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m Crew: \u001b[0m\u001b[1;36mcrew\u001b[0m\n",
       " \u001b[1;33m Task: 27ec6611-922a-4fb3-9047-a980986e67a0\u001b[0m\n",
       "    \u001b[37m   Status: \u001b[0m\u001b[2;33mExecuting Task...\u001b[0m\n",
       "     \u001b[1;32m Agent: \u001b[0m\u001b[32mSenior Data Researcher\u001b[0m\n",
       "        \u001b[37m    Status: \u001b[0m\u001b[1;32m Completed\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> Crew: crew</span>\n",
       " <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> Task: 27ec6611-922a-4fb3-9047-a980986e67a0</span>\n",
       "    <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">   Assigned to: </span><span style=\"color: #008000; text-decoration-color: #008000\">Senior Data Researcher</span>\n",
       "    <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">   Status: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> Completed</span>\n",
       "     <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> Agent: </span><span style=\"color: #008000; text-decoration-color: #008000\">Senior Data Researcher</span>\n",
       "        <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">    Status: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> Completed</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m Crew: \u001b[0m\u001b[1;36mcrew\u001b[0m\n",
       " \u001b[1;32m Task: 27ec6611-922a-4fb3-9047-a980986e67a0\u001b[0m\n",
       "    \u001b[37m   Assigned to: \u001b[0m\u001b[32mSenior Data Researcher\u001b[0m\n",
       "    \u001b[37m   Status: \u001b[0m\u001b[1;32m Completed\u001b[0m\n",
       "     \u001b[1;32m Agent: \u001b[0m\u001b[32mSenior Data Researcher\u001b[0m\n",
       "        \u001b[37m    Status: \u001b[0m\u001b[1;32m Completed\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\"> Task Completion </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\"></span>                                                                                                                 <span style=\"color: #008000; text-decoration-color: #008000\"></span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\"></span>  <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Task Completed</span>                                                                                                 <span style=\"color: #008000; text-decoration-color: #008000\"></span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\"></span>  <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">Name: </span><span style=\"color: #008000; text-decoration-color: #008000\">27ec6611-922a-4fb3-9047-a980986e67a0</span>                                                                     <span style=\"color: #008000; text-decoration-color: #008000\"></span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\"></span>  <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">Agent: </span><span style=\"color: #008000; text-decoration-color: #008000\">Senior Data Researcher</span>                                                                                  <span style=\"color: #008000; text-decoration-color: #008000\"></span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\"></span>                                                                                                                 <span style=\"color: #008000; text-decoration-color: #008000\"></span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\"></span>                                                                                                                 <span style=\"color: #008000; text-decoration-color: #008000\"></span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\"></span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32m\u001b[0m\u001b[32m\u001b[0m\u001b[32m Task Completion \u001b[0m\u001b[32m\u001b[0m\u001b[32m\u001b[0m\n",
       "\u001b[32m\u001b[0m                                                                                                                 \u001b[32m\u001b[0m\n",
       "\u001b[32m\u001b[0m  \u001b[1;32mTask Completed\u001b[0m                                                                                                 \u001b[32m\u001b[0m\n",
       "\u001b[32m\u001b[0m  \u001b[37mName: \u001b[0m\u001b[32m27ec6611-922a-4fb3-9047-a980986e67a0\u001b[0m                                                                     \u001b[32m\u001b[0m\n",
       "\u001b[32m\u001b[0m  \u001b[37mAgent: \u001b[0m\u001b[32mSenior Data Researcher\u001b[0m                                                                                  \u001b[32m\u001b[0m\n",
       "\u001b[32m\u001b[0m                                                                                                                 \u001b[32m\u001b[0m\n",
       "\u001b[32m\u001b[0m                                                                                                                 \u001b[32m\u001b[0m\n",
       "\u001b[32m\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> Crew: crew</span>\n",
       " <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> Task: 27ec6611-922a-4fb3-9047-a980986e67a0</span>\n",
       "   <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">   Assigned to: </span><span style=\"color: #008000; text-decoration-color: #008000\">Senior Data Researcher</span>\n",
       "   <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">   Status: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> Completed</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> Agent: </span><span style=\"color: #008000; text-decoration-color: #008000\">Senior Data Researcher</span>\n",
       "       <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">    Status: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> Completed</span>\n",
       " <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> Task: a61dcf5a-644b-4bbb-a4da-8e36eea143e7</span>\n",
       "    <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">   Status: </span><span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">Executing Task...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m Crew: \u001b[0m\u001b[1;36mcrew\u001b[0m\n",
       " \u001b[1;32m Task: 27ec6611-922a-4fb3-9047-a980986e67a0\u001b[0m\n",
       "   \u001b[37m   Assigned to: \u001b[0m\u001b[32mSenior Data Researcher\u001b[0m\n",
       "   \u001b[37m   Status: \u001b[0m\u001b[1;32m Completed\u001b[0m\n",
       "    \u001b[1;32m Agent: \u001b[0m\u001b[32mSenior Data Researcher\u001b[0m\n",
       "       \u001b[37m    Status: \u001b[0m\u001b[1;32m Completed\u001b[0m\n",
       " \u001b[1;33m Task: a61dcf5a-644b-4bbb-a4da-8e36eea143e7\u001b[0m\n",
       "    \u001b[37m   Status: \u001b[0m\u001b[2;33mExecuting Task...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> Crew: crew</span>\n",
       " <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> Task: 27ec6611-922a-4fb3-9047-a980986e67a0</span>\n",
       "   <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">   Assigned to: </span><span style=\"color: #008000; text-decoration-color: #008000\">Senior Data Researcher</span>\n",
       "   <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">   Status: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> Completed</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> Agent: </span><span style=\"color: #008000; text-decoration-color: #008000\">Senior Data Researcher</span>\n",
       "       <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">    Status: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> Completed</span>\n",
       " <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> Task: a61dcf5a-644b-4bbb-a4da-8e36eea143e7</span>\n",
       "    <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">   Status: </span><span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">Executing Task...</span>\n",
       "     <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> Agent: </span><span style=\"color: #008000; text-decoration-color: #008000\">Reporting Analyst</span>\n",
       "        <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">    Status: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">In Progress</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m Crew: \u001b[0m\u001b[1;36mcrew\u001b[0m\n",
       " \u001b[1;32m Task: 27ec6611-922a-4fb3-9047-a980986e67a0\u001b[0m\n",
       "   \u001b[37m   Assigned to: \u001b[0m\u001b[32mSenior Data Researcher\u001b[0m\n",
       "   \u001b[37m   Status: \u001b[0m\u001b[1;32m Completed\u001b[0m\n",
       "    \u001b[1;32m Agent: \u001b[0m\u001b[32mSenior Data Researcher\u001b[0m\n",
       "       \u001b[37m    Status: \u001b[0m\u001b[1;32m Completed\u001b[0m\n",
       " \u001b[1;33m Task: a61dcf5a-644b-4bbb-a4da-8e36eea143e7\u001b[0m\n",
       "    \u001b[37m   Status: \u001b[0m\u001b[2;33mExecuting Task...\u001b[0m\n",
       "     \u001b[1;32m Agent: \u001b[0m\u001b[32mReporting Analyst\u001b[0m\n",
       "        \u001b[37m    Status: \u001b[0m\u001b[1;32mIn Progress\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mReporting Analyst\u001b[00m\n",
      "\u001b[95m## Task:\u001b[00m \u001b[92m\n",
      "Read the tool list from the previous task's result and perform a detailed analysis of each tool.\n",
      "\n",
      "For each tool:\n",
      "1. Research its capabilities, limitations, community adoption, documentation quality\n",
      "2. Evaluate how well it addresses the specified task: 'Develop a conversational RAG system that can answer questions based on a large PDF document collection'\n",
      "3. Consider its compatibility with 'LangChain, PogreSQL, FastAPI'\n",
      "4. Identify pros and cons\n",
      "\n",
      "Your output should be a JSON list of these detailed analysis objects.\n",
      "\u001b[00m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> Agent: </span><span style=\"color: #008000; text-decoration-color: #008000\">Reporting Analyst</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">    Status: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">In Progress</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32m Agent: \u001b[0m\u001b[32mReporting Analyst\u001b[0m\n",
       "\u001b[37m    Status: \u001b[0m\u001b[1;32mIn Progress\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mReporting Analyst\u001b[00m\n",
      "\u001b[95m## Thought:\u001b[00m \u001b[92mThought: I need to gather detailed information on each AI tool mentioned to analyze their suitability for developing a conversational RAG system.\u001b[00m\n",
      "\u001b[95m## Using tool:\u001b[00m \u001b[92mSearch in a specific website\u001b[00m\n",
      "\u001b[95m## Tool Input:\u001b[00m \u001b[92m\n",
      "\"{\\\"search_query\\\": \\\"capabilities limitations community adoption documentation quality\\\", \\\"website\\\": \\\"https://www.h2o.ai/\\\"}\"\u001b[00m\n",
      "\u001b[95m## Tool Output:\u001b[00m \u001b[92m\n",
      "Relevant Content:\n",
      "images. Intelligent document comparison with ProcurementGPT ProcurementGPT streamlines document analysis by automatically parsing documents into sections or clauses, then identifying the most similar counterparts across documentsworking symmetrically in both directions. It further breaks down clauses into key requirements, determining where each one is fulfilled and highlighting met and unmet requirements for easy review. The system leverages the top 10 most similar requirements to assess compliance, with an overlay for quick navigation. Users can also manually select sections for comparison, ensuring flexibility when automated parsing needs refinement. Learn More Summary of complaints and key issues Provides a high-level summary of the most common complaints, the top issues faced, and recommended actions to address these concerns. Learn More Built for air-gapped, on-premises, private or managed cloud deployments. Own your data, own your prompts. Only H2O.ai provides an end-to-end GenAI platform where you can own every part of the stack. H2O.ai offers a highly flexible solution, that can run fully managed in the cloud, or in hybrid or on-premise and air-gapped environments. Fully scalable with Kubernetes. H2O.ai achieves SOC2 Type 2 +HIPAA/HITECH Cost control and flexibility Customers can decide to mix and match from a large choice of over 30 LLMs: Proprietary LLMs: GPT-4o, o1, Gemini, Claude, Mistral, etc. Reliance on third-party provider No control, no ownership Token-based pricing, unbounded cost Open source LLMs: Llama3, DeepSeek, Qwen2.5, Mixtral, Mistral, H2O Danube, H2OVL Mississippi, any Fine-Tuned OSS model. Any model from Open-Source ecosystem Hosted on your own GPU infrastructure Full control and ownership of models Fixed hardware cost, most economical choice Cost savings on H2O.ai Platform. 25x cheaper on query cost. Serve more users. Better accuracy with up to 100% cheaper on Document Processing. h2oGPTe is designed to work with any LLM of your choice\n",
      "\n",
      "development and shipping experience. We couldnt have achieved the product experience delivered to our customers without LangChain, and we couldnt have done it at the same pace without LangSmith.James SpiteriDirector of Security ProductsAs soon as we heard about LangSmith, we moved our entire development stack onto it. We could have built evaluation, testing and monitoring tools in house, but with LangSmith it took us 10x less time to get a 1000x better tool.Jose PeaSenior Manager The reference architecture enterprises adopt for success.LangChains suite of products can be used independently or stacked together for multiplicative impact  guiding you through building, running, and managing your LLM apps.20M+Monthly Downloads100K+Apps Powered100K+GitHub Stars4K+ContributorsThe biggest developer community in GenAILearn alongside the 1M+ developers who are pushing the industry forward.Explore LangChainGet started with LangChain's suite of products.Get a demoSign up for freeReady to start shipping reliable GenAI apps faster?Get started with LangChain, LangSmith, and LangGraph to enhance your LLM app development, from prototype to production.Get a demoSign up for free\n",
      "\n",
      "Services PyTorch on AWS Amazon SageMaker AWS Deep Learning Containers AWS Deep Learning AMIs Google Cloud Platform Cloud Deep Learning VM Image Deep Learning Containers Microsoft Azure PyTorch on Azure Azure Machine Learning Azure Functions Lightning Studios lightning.ai Ecosystem Feature Projects See all Projects Explore a rich ecosystem of libraries, tools, and more to support development. Captum Captum (comprehension in Latin) is an open source, extensible library for model interpretability built on PyTorch. PyTorch Geometric PyTorch Geometric is a library for deep learning on irregular input data such as graphs, point clouds, and manifolds. skorch skorch is a high-level library for PyTorch that provides full scikit-learn compatibility. Community Join the PyTorch developer community to contribute, learn, and get your questions answered. PyTorchDiscuss Browse and join discussions on deep learning with PyTorch. Slack Discuss advanced topics.  Docs and tutorials in Chinese, translated by the community. (PyTorch) Tutorials in Korean, translated by the community. (PyTorch) Tutorials in Japanese, translated by the community. Maintainers Learn about the PyTorch core and module maintainers. ContributionGuide Learn how you can contribute to PyTorch code and documentation. DesignPhilosophy PyTorch design principles for contributors and maintainers. Governance Learn about the PyTorch governance hierarchy. Newsletter Stay up-to-date with the latest updates. PyTorchTraining Further your education and career goals. Companies & UniversitiesUsing PyTorch Reduce inference costs by 71% and drive scale out using PyTorch, TorchServe, and AWS Inferentia. Learn More Pushing the state of the art in NLP and Multi-task learning. Learn More Using PyTorchs flexibility to efficiently research new algorithmic approaches. Learn More Docs Access comprehensive developer documentation for PyTorch View Docs Tutorials Get in-depth tutorials for beginners and advanced developers View\u001b[00m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> Agent: </span><span style=\"color: #008000; text-decoration-color: #008000\">Reporting Analyst</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">    Status: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">In Progress</span>\n",
       " <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\"> </span><span style=\"color: #000080; text-decoration-color: #000080\">Thinking...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32m Agent: \u001b[0m\u001b[32mReporting Analyst\u001b[0m\n",
       "\u001b[37m    Status: \u001b[0m\u001b[1;32mIn Progress\u001b[0m\n",
       " \u001b[1;34m \u001b[0m\u001b[34mThinking...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> Agent: </span><span style=\"color: #008000; text-decoration-color: #008000\">Reporting Analyst</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">    Status: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">In Progress</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32m Agent: \u001b[0m\u001b[32mReporting Analyst\u001b[0m\n",
       "\u001b[37m    Status: \u001b[0m\u001b[1;32mIn Progress\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mReporting Analyst\u001b[00m\n",
      "\u001b[95m## Final Answer:\u001b[00m \u001b[92m\n",
      "```json\n",
      "[\n",
      "    {\n",
      "        \"name\": \"H2O.ai\",\n",
      "        \"capabilities\": \"Automated Machine Learning and AI Model Deployment, designed for efficiency with a focus on rapid model deployment across various environments.\",\n",
      "        \"limitations\": \"Less control over model internals and limited customization options.\",\n",
      "        \"community_adoption\": \"Gaining traction in business implementations, especially for quick deployment scenarios.\",\n",
      "        \"documentation_quality\": \"Well-documented, providing clear guidelines for users but limited in depth for advanced customization.\",\n",
      "        \"evaluation_for_task\": \"H2O.ai can assist in the deployment of conversational RAG systems; however, its flexibility may be less than desirable for complex query handling involved in language models.\",\n",
      "        \"compatibility\": {\n",
      "            \"LangChain\": \"Moderate: Possible but requires additional integration efforts.\",\n",
      "            \"PostgreSQL\": \"Low: Limited direct support.\",\n",
      "            \"FastAPI\": \"Moderate: Can be integrated but with some effort.\"\n",
      "        },\n",
      "        \"pros\": [\"Automated machine learning\", \"Rapid model training\", \"User-friendly interface\"],\n",
      "        \"cons\": [\"Limited control and flexibility\", \"Not ideal for complex conversational AI tasks\", \"Higher cost associated with extensive use\"],\n",
      "        \"recommendation_score\": \"6/10\"\n",
      "    },\n",
      "    {\n",
      "        \"name\": \"LangChain\",\n",
      "        \"capabilities\": \"Framework designed to create context-aware applications powered by language models (LLMs), facilitates integration with various data sources.\",\n",
      "        \"limitations\": \"Complex for initial setup; requires understanding of several components.\",\n",
      "        \"community_adoption\": \"Rapidly growing with significant attention from developers in the LLM space.\",\n",
      "        \"documentation_quality\": \"Extensive documentation and tutorials available, fostering ease of understanding and use.\",\n",
      "        \"evaluation_for_task\": \"Highly suitable for developing a conversational RAG system due to its focus on LLM integration and context handling.\",\n",
      "        \"compatibility\": {\n",
      "            \"LangChain\": \"High: Directly designed around LLM applications.\",\n",
      "            \"PostgreSQL\": \"High: Supports various database integrations.\",\n",
      "            \"FastAPI\": \"High: Compatible for building APIs.\"\n",
      "        },\n",
      "        \"pros\": [\"Highly modular and composable\", \"Strong community support\", \"Rich set of features for LLM applications\"],\n",
      "        \"cons\": [\"Initial setup complexity\", \"Can be resource-intensive\"],\n",
      "        \"recommendation_score\": \"9/10\"\n",
      "    },\n",
      "    {\n",
      "        \"name\": \"TensorFlow\",\n",
      "        \"capabilities\": \"An extensive ecosystem for building and training deep learning models, particularly suited for complex architectures.\",\n",
      "        \"limitations\": \"Steep learning curve for beginners and potentially high memory usage.\",\n",
      "        \"community_adoption\": \"Widely adopted with a large community, rich ecosystem of models and resources.\",\n",
      "        \"documentation_quality\": \"High-quality documentation with numerous tutorials and use cases.\",\n",
      "        \"evaluation_for_task\": \"Excellent for building conversational RAG systems due to robust NLP support and extensibility.\",\n",
      "        \"compatibility\": {\n",
      "            \"LangChain\": \"Moderate: Can integrate but may require intricate setup.\",\n",
      "            \"PostgreSQL\": \"Low: Not directly related.\",\n",
      "            \"FastAPI\": \"High: Compatible for creating web APIs.\"\n",
      "        },\n",
      "        \"pros\": [\"Strong support for production use\", \"Active community\", \"Extensive libraries and tools\"],\n",
      "        \"cons\": [\"Higher operational overhead\", \"More complex than other frameworks\"],\n",
      "        \"recommendation_score\": \"9/10\"\n",
      "    },\n",
      "    {\n",
      "        \"name\": \"PyTorch\",\n",
      "        \"capabilities\": \"Flexible deep learning framework focusing on dynamic computation, ideal for research and prototyping.\",\n",
      "        \"limitations\": \"Less mature in deployment mechanisms compared to TensorFlow.\",\n",
      "        \"community_adoption\": \"Very popular in the research community, increasingly adopted for production tasks.\",\n",
      "        \"documentation_quality\": \"Good documentation with various tutorials available.\",\n",
      "        \"evaluation_for_task\": \"Highly suitable for RAG systems that require flexible model experimentation and development.\",\n",
      "        \"compatibility\": {\n",
      "            \"LangChain\": \"High: Integration available and straightforward.\",\n",
      "            \"PostgreSQL\": \"Moderate: Can be integrated but not directly supported.\",\n",
      "            \"FastAPI\": \"High: Well-suited for creating REST APIs.\"\n",
      "        },\n",
      "        \"pros\": [\"User-friendly\", \"Strong for prototyping\", \"Good community support\"],\n",
      "        \"cons\": [\"Deployment complexity\", \"API stability issues with updates\"],\n",
      "        \"recommendation_score\": \"8/10\"\n",
      "    },\n",
      "    {\n",
      "        \"name\": \"Keras\",\n",
      "        \"capabilities\": \"High-level API for building and training deep learning models, great for rapid development.\",\n",
      "        \"limitations\": \"Dependent on TensorFlow for advanced functionalities, less flexibility.\",\n",
      "        \"community_adoption\": \"Widely used, especially in educational settings and for quick model prototyping.\",\n",
      "        \"documentation_quality\": \"Excellent for beginners, but lacks depth for advanced users.\",\n",
      "        \"evaluation_for_task\": \"Useful for early-stage development of conversational AI; not as powerful for complex systems.\",\n",
      "        \"compatibility\": {\n",
      "            \"LangChain\": \"Moderate: Can work with some integration effort.\",\n",
      "            \"PostgreSQL\": \"Low: Limited direct support.\",\n",
      "            \"FastAPI\": \"High: Good compatibility for API development.\"\n",
      "        },\n",
      "        \"pros\": [\"User-friendly\", \"Suitable for educational purposes\", \"Good for rapid prototyping\"],\n",
      "        \"cons\": [\"Less flexible due to dependency on TensorFlow\", \"Limited depth for complex tasks\"],\n",
      "        \"recommendation_score\": \"7/10\"\n",
      "    }\n",
      "]\n",
      "```\n",
      "\u001b[00m\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages/instructor/process_response.py:222: DeprecationWarning: FUNCTIONS is deprecated and will be removed in future versions\n",
      "  if mode == Mode.FUNCTIONS:\n",
      "/Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages/instructor/function_calls.py:118: DeprecationWarning: FUNCTIONS is deprecated and will be removed in future versions\n",
      "  if mode == Mode.FUNCTIONS:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> Crew: crew</span>\n",
       " <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> Task: 27ec6611-922a-4fb3-9047-a980986e67a0</span>\n",
       "   <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">   Assigned to: </span><span style=\"color: #008000; text-decoration-color: #008000\">Senior Data Researcher</span>\n",
       "   <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">   Status: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> Completed</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> Agent: </span><span style=\"color: #008000; text-decoration-color: #008000\">Senior Data Researcher</span>\n",
       "       <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">    Status: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> Completed</span>\n",
       " <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> Task: a61dcf5a-644b-4bbb-a4da-8e36eea143e7</span>\n",
       "    <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">   Status: </span><span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">Executing Task...</span>\n",
       "     <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> Agent: </span><span style=\"color: #008000; text-decoration-color: #008000\">Reporting Analyst</span>\n",
       "        <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">    Status: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> Completed</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m Crew: \u001b[0m\u001b[1;36mcrew\u001b[0m\n",
       " \u001b[1;32m Task: 27ec6611-922a-4fb3-9047-a980986e67a0\u001b[0m\n",
       "   \u001b[37m   Assigned to: \u001b[0m\u001b[32mSenior Data Researcher\u001b[0m\n",
       "   \u001b[37m   Status: \u001b[0m\u001b[1;32m Completed\u001b[0m\n",
       "    \u001b[1;32m Agent: \u001b[0m\u001b[32mSenior Data Researcher\u001b[0m\n",
       "       \u001b[37m    Status: \u001b[0m\u001b[1;32m Completed\u001b[0m\n",
       " \u001b[1;33m Task: a61dcf5a-644b-4bbb-a4da-8e36eea143e7\u001b[0m\n",
       "    \u001b[37m   Status: \u001b[0m\u001b[2;33mExecuting Task...\u001b[0m\n",
       "     \u001b[1;32m Agent: \u001b[0m\u001b[32mReporting Analyst\u001b[0m\n",
       "        \u001b[37m    Status: \u001b[0m\u001b[1;32m Completed\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> Crew: crew</span>\n",
       " <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> Task: 27ec6611-922a-4fb3-9047-a980986e67a0</span>\n",
       "   <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">   Assigned to: </span><span style=\"color: #008000; text-decoration-color: #008000\">Senior Data Researcher</span>\n",
       "   <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">   Status: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> Completed</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> Agent: </span><span style=\"color: #008000; text-decoration-color: #008000\">Senior Data Researcher</span>\n",
       "       <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">    Status: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> Completed</span>\n",
       " <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> Task: a61dcf5a-644b-4bbb-a4da-8e36eea143e7</span>\n",
       "    <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">   Assigned to: </span><span style=\"color: #008000; text-decoration-color: #008000\">Reporting Analyst</span>\n",
       "    <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">   Status: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> Completed</span>\n",
       "     <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> Agent: </span><span style=\"color: #008000; text-decoration-color: #008000\">Reporting Analyst</span>\n",
       "        <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">    Status: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> Completed</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m Crew: \u001b[0m\u001b[1;36mcrew\u001b[0m\n",
       " \u001b[1;32m Task: 27ec6611-922a-4fb3-9047-a980986e67a0\u001b[0m\n",
       "   \u001b[37m   Assigned to: \u001b[0m\u001b[32mSenior Data Researcher\u001b[0m\n",
       "   \u001b[37m   Status: \u001b[0m\u001b[1;32m Completed\u001b[0m\n",
       "    \u001b[1;32m Agent: \u001b[0m\u001b[32mSenior Data Researcher\u001b[0m\n",
       "       \u001b[37m    Status: \u001b[0m\u001b[1;32m Completed\u001b[0m\n",
       " \u001b[1;32m Task: a61dcf5a-644b-4bbb-a4da-8e36eea143e7\u001b[0m\n",
       "    \u001b[37m   Assigned to: \u001b[0m\u001b[32mReporting Analyst\u001b[0m\n",
       "    \u001b[37m   Status: \u001b[0m\u001b[1;32m Completed\u001b[0m\n",
       "     \u001b[1;32m Agent: \u001b[0m\u001b[32mReporting Analyst\u001b[0m\n",
       "        \u001b[37m    Status: \u001b[0m\u001b[1;32m Completed\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\"> Task Completion </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\"></span>                                                                                                                 <span style=\"color: #008000; text-decoration-color: #008000\"></span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\"></span>  <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Task Completed</span>                                                                                                 <span style=\"color: #008000; text-decoration-color: #008000\"></span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\"></span>  <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">Name: </span><span style=\"color: #008000; text-decoration-color: #008000\">a61dcf5a-644b-4bbb-a4da-8e36eea143e7</span>                                                                     <span style=\"color: #008000; text-decoration-color: #008000\"></span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\"></span>  <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">Agent: </span><span style=\"color: #008000; text-decoration-color: #008000\">Reporting Analyst</span>                                                                                       <span style=\"color: #008000; text-decoration-color: #008000\"></span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\"></span>                                                                                                                 <span style=\"color: #008000; text-decoration-color: #008000\"></span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\"></span>                                                                                                                 <span style=\"color: #008000; text-decoration-color: #008000\"></span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\"></span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32m\u001b[0m\u001b[32m\u001b[0m\u001b[32m Task Completion \u001b[0m\u001b[32m\u001b[0m\u001b[32m\u001b[0m\n",
       "\u001b[32m\u001b[0m                                                                                                                 \u001b[32m\u001b[0m\n",
       "\u001b[32m\u001b[0m  \u001b[1;32mTask Completed\u001b[0m                                                                                                 \u001b[32m\u001b[0m\n",
       "\u001b[32m\u001b[0m  \u001b[37mName: \u001b[0m\u001b[32ma61dcf5a-644b-4bbb-a4da-8e36eea143e7\u001b[0m                                                                     \u001b[32m\u001b[0m\n",
       "\u001b[32m\u001b[0m  \u001b[37mAgent: \u001b[0m\u001b[32mReporting Analyst\u001b[0m                                                                                       \u001b[32m\u001b[0m\n",
       "\u001b[32m\u001b[0m                                                                                                                 \u001b[32m\u001b[0m\n",
       "\u001b[32m\u001b[0m                                                                                                                 \u001b[32m\u001b[0m\n",
       "\u001b[32m\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> Crew: crew</span>\n",
       " <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> Task: 27ec6611-922a-4fb3-9047-a980986e67a0</span>\n",
       "   <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">   Assigned to: </span><span style=\"color: #008000; text-decoration-color: #008000\">Senior Data Researcher</span>\n",
       "   <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">   Status: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> Completed</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> Agent: </span><span style=\"color: #008000; text-decoration-color: #008000\">Senior Data Researcher</span>\n",
       "       <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">    Status: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> Completed</span>\n",
       " <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> Task: a61dcf5a-644b-4bbb-a4da-8e36eea143e7</span>\n",
       "   <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">   Assigned to: </span><span style=\"color: #008000; text-decoration-color: #008000\">Reporting Analyst</span>\n",
       "   <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">   Status: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> Completed</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> Agent: </span><span style=\"color: #008000; text-decoration-color: #008000\">Reporting Analyst</span>\n",
       "       <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">    Status: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> Completed</span>\n",
       " <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> Task: 921e9ce5-1e12-4a66-89af-19846ee2f50f</span>\n",
       "    <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">   Status: </span><span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">Executing Task...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m Crew: \u001b[0m\u001b[1;36mcrew\u001b[0m\n",
       " \u001b[1;32m Task: 27ec6611-922a-4fb3-9047-a980986e67a0\u001b[0m\n",
       "   \u001b[37m   Assigned to: \u001b[0m\u001b[32mSenior Data Researcher\u001b[0m\n",
       "   \u001b[37m   Status: \u001b[0m\u001b[1;32m Completed\u001b[0m\n",
       "    \u001b[1;32m Agent: \u001b[0m\u001b[32mSenior Data Researcher\u001b[0m\n",
       "       \u001b[37m    Status: \u001b[0m\u001b[1;32m Completed\u001b[0m\n",
       " \u001b[1;32m Task: a61dcf5a-644b-4bbb-a4da-8e36eea143e7\u001b[0m\n",
       "   \u001b[37m   Assigned to: \u001b[0m\u001b[32mReporting Analyst\u001b[0m\n",
       "   \u001b[37m   Status: \u001b[0m\u001b[1;32m Completed\u001b[0m\n",
       "    \u001b[1;32m Agent: \u001b[0m\u001b[32mReporting Analyst\u001b[0m\n",
       "       \u001b[37m    Status: \u001b[0m\u001b[1;32m Completed\u001b[0m\n",
       " \u001b[1;33m Task: 921e9ce5-1e12-4a66-89af-19846ee2f50f\u001b[0m\n",
       "    \u001b[37m   Status: \u001b[0m\u001b[2;33mExecuting Task...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> Crew: crew</span>\n",
       " <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> Task: 27ec6611-922a-4fb3-9047-a980986e67a0</span>\n",
       "   <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">   Assigned to: </span><span style=\"color: #008000; text-decoration-color: #008000\">Senior Data Researcher</span>\n",
       "   <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">   Status: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> Completed</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> Agent: </span><span style=\"color: #008000; text-decoration-color: #008000\">Senior Data Researcher</span>\n",
       "       <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">    Status: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> Completed</span>\n",
       " <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> Task: a61dcf5a-644b-4bbb-a4da-8e36eea143e7</span>\n",
       "   <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">   Assigned to: </span><span style=\"color: #008000; text-decoration-color: #008000\">Reporting Analyst</span>\n",
       "   <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">   Status: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> Completed</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> Agent: </span><span style=\"color: #008000; text-decoration-color: #008000\">Reporting Analyst</span>\n",
       "       <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">    Status: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> Completed</span>\n",
       " <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> Task: 921e9ce5-1e12-4a66-89af-19846ee2f50f</span>\n",
       "    <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">   Status: </span><span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">Executing Task...</span>\n",
       "     <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> Agent: </span><span style=\"color: #008000; text-decoration-color: #008000\">Reporting Analyst</span>\n",
       "        <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">    Status: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">In Progress</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m Crew: \u001b[0m\u001b[1;36mcrew\u001b[0m\n",
       " \u001b[1;32m Task: 27ec6611-922a-4fb3-9047-a980986e67a0\u001b[0m\n",
       "   \u001b[37m   Assigned to: \u001b[0m\u001b[32mSenior Data Researcher\u001b[0m\n",
       "   \u001b[37m   Status: \u001b[0m\u001b[1;32m Completed\u001b[0m\n",
       "    \u001b[1;32m Agent: \u001b[0m\u001b[32mSenior Data Researcher\u001b[0m\n",
       "       \u001b[37m    Status: \u001b[0m\u001b[1;32m Completed\u001b[0m\n",
       " \u001b[1;32m Task: a61dcf5a-644b-4bbb-a4da-8e36eea143e7\u001b[0m\n",
       "   \u001b[37m   Assigned to: \u001b[0m\u001b[32mReporting Analyst\u001b[0m\n",
       "   \u001b[37m   Status: \u001b[0m\u001b[1;32m Completed\u001b[0m\n",
       "    \u001b[1;32m Agent: \u001b[0m\u001b[32mReporting Analyst\u001b[0m\n",
       "       \u001b[37m    Status: \u001b[0m\u001b[1;32m Completed\u001b[0m\n",
       " \u001b[1;33m Task: 921e9ce5-1e12-4a66-89af-19846ee2f50f\u001b[0m\n",
       "    \u001b[37m   Status: \u001b[0m\u001b[2;33mExecuting Task...\u001b[0m\n",
       "     \u001b[1;32m Agent: \u001b[0m\u001b[32mReporting Analyst\u001b[0m\n",
       "        \u001b[37m    Status: \u001b[0m\u001b[1;32mIn Progress\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mReporting Analyst\u001b[00m\n",
      "\u001b[95m## Task:\u001b[00m \u001b[92m\n",
      "Read the analysis from the previous task's result and create a comprehensive Markdown report.\n",
      "\n",
      "The report should include:\n",
      "\n",
      "1. An introduction explaining the task ('Develop a conversational RAG system that can answer questions based on a large PDF document collection') and existing stack ('LangChain, PogreSQL, FastAPI')\n",
      "2. For each tool, create a section with:\n",
      "   - Tool name and URL as a heading\n",
      "   - Description\n",
      "   - Features (as bullet points)\n",
      "   - Pros (as bullet points)\n",
      "   - Cons (as bullet points)\n",
      "   - Integration complexity\n",
      "   - Recommendation score with justification\n",
      "3. A summary/conclusion comparing the tools and providing final recommendations\n",
      "\n",
      "Use proper Markdown formatting with headings, bullet points, and emphasis where appropriate.\n",
      "Sort tools by recommendation score (descending).\n",
      "\n",
      "Your output should be a complete, well-formatted Markdown document.\n",
      "\u001b[00m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> Agent: </span><span style=\"color: #008000; text-decoration-color: #008000\">Reporting Analyst</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">    Status: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">In Progress</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32m Agent: \u001b[0m\u001b[32mReporting Analyst\u001b[0m\n",
       "\u001b[37m    Status: \u001b[0m\u001b[1;32mIn Progress\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inserting batches in chromadb: 100%|| 1/1 [00:00<00:00,  1.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mReporting Analyst\u001b[00m\n",
      "\u001b[95m## Using tool:\u001b[00m \u001b[92mSearch in a specific website\u001b[00m\n",
      "\u001b[95m## Tool Input:\u001b[00m \u001b[92m\n",
      "\"{\\\"search_query\\\": \\\"Develop a conversational RAG system that can answer questions based on a large PDF document collection LangChain, PogreSQL, FastAPI\\\", \\\"website\\\": \\\"https://langchain.com/\\\"}\"\u001b[00m\n",
      "\u001b[95m## Tool Output:\u001b[00m \u001b[92m\n",
      "Relevant Content:\n",
      "LangChain Sign up to be the first to access recordings from Interrupt, The AI Agent Conference!Learn MoreFrom startups to global enterprises, ambitious builders choose LangChain products.BuildLangChain is a composable framework to build with LLMs. LangGraph is the orchestration framework for controllable agentic workflows.RunDeploy your LLM applications at scale with LangGraph Platform, our infrastructure purpose-built for agents.ManageLangSmith is a unified agent observability and evals platform to optimize the performance of your AI agents - whether they're built with a LangChain framework or not. Build your app with LangChainBuild context-aware, reasoning applications with LangChains flexible framework that leverages your companys data and APIs. Future-proof your application by making vendor optionality part of your LLM infrastructure design.Learn more about LangChainRun at scale with LangGraph PlatformUse LangGraph Platforms APIs to design agent-driven user experiences featuring human-in-the-loop, multi-agent collaboration, conversation history, long-term memory, and time-travel. Deploy with fault-tolerant scalability.Learn more about LangGraph PlatformManage agent observability & performance with LangSmithGet visibility into your LLM calls and agent reasoning with LangSmith to quickly debug failures. Run evals to assess your agent performance over production traffic  so you can ensure high-quality responses. Learn more about LangSmithHear from our happy customersLangChain, LangGraph, and LangSmith help teams of all sizes, across all industries - from ambitious startups to established enterprises.By combining the benefits of LangSmith and standing on the shoulders of a gigantic open-source community, were able to identify the right approaches of using LLMs in an enterprise-setting faster.Yusuke KajiGeneral Manager of AIWorking with LangChain and LangSmith on the Elastic AI Assistant had a significant positive impact on the overall pace and quality of the\n",
      "\n",
      "LangChain Sign up to be the first to access recordings from Interrupt, The AI Agent Conference!Learn MoreFrom startups to global enterprises, ambitious builders choose LangChain products.BuildLangChain is a composable framework to build with LLMs. LangGraph is the orchestration framework for controllable agentic workflows.RunDeploy your LLM applications at scale with LangGraph Platform, our infrastructure purpose-built for agents.ManageLangSmith is a unified agent observability and evals platform to optimize the performance of your AI agents - whether they're built with a LangChain framework or not. Build your app with LangChainBuild context-aware, reasoning applications with LangChains flexible framework that leverages your companys data and APIs. Future-proof your application by making vendor optionality part of your LLM infrastructure design.Learn more about LangChainRun at scale with LangGraph PlatformUse LangGraph Platforms APIs to design agent-driven user experiences featuring human-in-the-loop, multi-agent collaboration, conversation history, long-term memory, and time-travel. Deploy with fault-tolerant scalability.Learn more about LangGraph PlatformManage agent observability & performance with LangSmithGet visibility into your LLM calls and agent reasoning with LangSmith to quickly debug failures. Run evals to assess your agent performance over production traffic  so you can ensure high-quality responses. Learn more about LangSmithHear from our happy customersLangChain, LangGraph, and LangSmith help teams of all sizes, across all industries - from ambitious startups to established enterprises.By combining the benefits of LangSmith and standing on the shoulders of a gigantic open-source community, were able to identify the right approaches of using LLMs in an enterprise-setting faster.Yusuke KajiGeneral Manager of AIWorking with LangChain and LangSmith on the Elastic AI Assistant had a significant positive impact on the overall pace and quality of the\n",
      "\n",
      "H2O.ai | Convergence of The World's Best Predictive & Generative AI The World's Best Deep Research Chat Now Chat on Mobile Learn More Companies powered by H2O.ai H2Os flagship SLMs H2OVL Mississippi SVLM Series Our newest economical multimodal OCR model developed for Document AI H2OVL Mississippi-2B, based on H2O Danube2, is trained on 17.3M conversation pairs for high-res image handling. The .8B model, built on Danube3, leads OCR benchmarks with 19M pairs, outperforming all SLMs in text recognition. For Developers  1 Text Recognition H2O Danube SLM Series Our most economical small model for fast, lightweight tasks We trained H2O Danube3 models from scratch on ~100 H100 GPUs using our own curated dataset of 6T tokens. H2O Danube3-4B and .5B open-weight SLMs outperform the latest Apple OpenELM-3B and .5B instruct models. Perfect for developers who want to fine-tune their own SLMs for offline use cases. For Developers Get the Mobile App H2O Generative AI in use with customers H2O Document AI GPTs and Digital Agents in production worldwide In-Store Sales Assistant HR & IT Support Agent Fraud & Scam Detection RFP Assistant Procurement GPT Personalized Pricing Complaint Summarizer Distilled SLMs for Call Center Classification First-ever model risk management for Generative AI Generative AI is herebut can you audit it? H2O.ai brings Model Risk Management (MRM) to GenAI, combining enterprise-grade model evaluation with h2oGPTe and H2O Model Validation to make AI measurable, explainable, and compliant for banks and financial services. Automated testing, human-calibrated evaluations, and real-time risk monitoring ensure transparencyboosting compliance, risk management, and user confidence. H2O Model Validation Multi-modal Document AI with enterprise scale and security Convert Data to JSON Document AI ProcurementGPT Complaint Summarizer Convert structured data like financial tables to JSON Get answers from diagrams, flowcharts, web pages, audio files, video files and\u001b[00m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> Agent: </span><span style=\"color: #008000; text-decoration-color: #008000\">Reporting Analyst</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">    Status: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">In Progress</span>\n",
       " <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\"> </span><span style=\"color: #000080; text-decoration-color: #000080\">Thinking...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32m Agent: \u001b[0m\u001b[32mReporting Analyst\u001b[0m\n",
       "\u001b[37m    Status: \u001b[0m\u001b[1;32mIn Progress\u001b[0m\n",
       " \u001b[1;34m \u001b[0m\u001b[34mThinking...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> Agent: </span><span style=\"color: #008000; text-decoration-color: #008000\">Reporting Analyst</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">    Status: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">In Progress</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32m Agent: \u001b[0m\u001b[32mReporting Analyst\u001b[0m\n",
       "\u001b[37m    Status: \u001b[0m\u001b[1;32mIn Progress\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mReporting Analyst\u001b[00m\n",
      "\u001b[95m## Final Answer:\u001b[00m \u001b[92m\n",
      "# Comprehensive Markdown Report: Analysis of Tools for Developing a Conversational RAG System\n",
      "\n",
      "## Introduction\n",
      "The aim of this task is to **develop a conversational Retrieval-Augmented Generation (RAG) system** that can efficiently answer questions based on a large collection of PDF documents. The existing technology stack includes **LangChain**, **PostgreSQL**, and **FastAPI**. This report evaluates various AI libraries and frameworks, focusing on their suitability for this specific use case.\n",
      "\n",
      "## Tool Analysis\n",
      "\n",
      "### 1. [LangChain](https://langchain.com/)\n",
      "**Description:** LangChain is a composable framework designed to build applications powered by language models (LLMs). It facilitates integration with various data sources and APIs, making it a perfect fit for building conversational RAG systems.\n",
      "\n",
      "- **Features:**\n",
      "  - Modular architecture\n",
      "  - Strong focus on LLM integration\n",
      "  - Extensive API support\n",
      "\n",
      "- **Pros:**\n",
      "  - Highly modular and composable\n",
      "  - Strong community support with rapid growth\n",
      "  - Rich features for LLM applications\n",
      "\n",
      "- **Cons:**\n",
      "  - Initial setup complexity\n",
      "  - Potentially resource-intensive\n",
      "\n",
      "- **Integration Complexity:** High compatibility with PostgreSQL and FastAPI, requires effort for initial setup.\n",
      "\n",
      "- **Recommendation Score:** **9/10** \n",
      "  - Justification: Highly suitable for RAG due to context handling and LLM integration support.\n",
      "\n",
      "### 2. [TensorFlow](https://www.tensorflow.org/)\n",
      "**Description:** TensorFlow is an open-source library developed by Google for building and training machine learning models, particularly deep learning models. Its robust capabilities make it suitable for complex tasks.\n",
      "\n",
      "- **Features:**\n",
      "  - Extensive ecosystem for deep learning\n",
      "  - Strong support for production use\n",
      "\n",
      "- **Pros:**\n",
      "  - Active community and well-supported libraries\n",
      "  - High-quality documentation and tutorials\n",
      "\n",
      "- **Cons:**\n",
      "  - Steep learning curve for beginners\n",
      "  - Higher operational overhead\n",
      "\n",
      "- **Integration Complexity:** Moderate compatibility with LangChain and FastAPI, low with PostgreSQL.\n",
      "\n",
      "- **Recommendation Score:** **9/10** \n",
      "  - Justification: Excellent for conversational RAG systems due to NLP extensibility.\n",
      "\n",
      "### 3. [PyTorch](https://pytorch.org/)\n",
      "**Description:** Developed by Facebook, PyTorch is known for its flexibility and ease of use, especially in research and prototyping applications.\n",
      "\n",
      "- **Features:**\n",
      "  - Dynamic computation graphs\n",
      "  - User-friendly API\n",
      "\n",
      "- **Pros:**\n",
      "  - Strong prototyping capabilities\n",
      "  - Popular amongst the research community\n",
      "\n",
      "- **Cons:**\n",
      "  - Deployment complexity\n",
      "  - API stability issues with updates\n",
      "\n",
      "- **Integration Complexity:** High compatibility with LangChain and FastAPI; moderate with PostgreSQL.\n",
      "\n",
      "- **Recommendation Score:** **8/10** \n",
      "  - Justification: Highly suitable for flexible model experimentation in RAG systems.\n",
      "\n",
      "### 4. [Keras](https://keras.io/)\n",
      "**Description:** Keras is a high-level API for building and training deep learning models, acting as an interface for TensorFlow.\n",
      "\n",
      "- **Features:**\n",
      "  - Simplifies model building and training\n",
      "  - Quick prototyping capabilities\n",
      "\n",
      "- **Pros:**\n",
      "  - User-friendly for beginners\n",
      "  - Suitable for educational purposes\n",
      "\n",
      "- **Cons:**\n",
      "  - Dependent on TensorFlow for advanced functionalities\n",
      "  - Less flexible for complex systems\n",
      "\n",
      "- **Integration Complexity:** Moderate compatibility with LangChain and FastAPI; low with PostgreSQL.\n",
      "\n",
      "- **Recommendation Score:** **7/10** \n",
      "  - Justification: Useful for early-stage development, but limited for complex conversational tasks.\n",
      "\n",
      "### 5. [H2O.ai](https://www.h2o.ai/)\n",
      "**Description:** H2O.ai converges Predictive AI with Generative AI, providing tools for a variety of use cases, particularly focusing on rapid model deployment.\n",
      "\n",
      "- **Features:**\n",
      "  - Automated machine learning\n",
      "  - Rapid model training capabilities\n",
      "\n",
      "- **Pros:**\n",
      "  - User-friendly interface\n",
      "  - Strong focus on efficiency\n",
      "\n",
      "- **Cons:**\n",
      "  - Limited control and flexibility\n",
      "  - Higher cost associated with extensive use\n",
      "\n",
      "- **Integration Complexity:** Moderate compatibility with LangChain; low with PostgreSQL and FastAPI.\n",
      "\n",
      "- **Recommendation Score:** **6/10** \n",
      "  - Justification: May assist in deployment, but not ideal for complex query handling in conversational AI.\n",
      "\n",
      "## Summary/Conclusion\n",
      "After analyzing the tools, **LangChain** and **TensorFlow** emerge as the top contenders for developing a conversational RAG system, both scoring **9/10**. LangChain stands out for its modular design and direct focus on LLM integration, while TensorFlow offers robust deep-learning capabilities. **PyTorch** follows closely with a score of **8/10**, providing flexibility for RAG applications. **Keras** and **H2O.ai** come in at **7/10** and **6/10** respectively, with limited effectiveness for more complex systems and specific use cases in conversational AI.\n",
      "\n",
      "In conclusion, for building a sophisticated conversational RAG system, investing resources in **LangChain** or **TensorFlow** is highly recommended, while **PyTorch** can serve as a viable alternative for flexible model experimentation.\u001b[00m\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages/instructor/process_response.py:222: DeprecationWarning: FUNCTIONS is deprecated and will be removed in future versions\n",
      "  if mode == Mode.FUNCTIONS:\n",
      "/Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages/instructor/function_calls.py:118: DeprecationWarning: FUNCTIONS is deprecated and will be removed in future versions\n",
      "  if mode == Mode.FUNCTIONS:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> Crew: crew</span>\n",
       " <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> Task: 27ec6611-922a-4fb3-9047-a980986e67a0</span>\n",
       "   <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">   Assigned to: </span><span style=\"color: #008000; text-decoration-color: #008000\">Senior Data Researcher</span>\n",
       "   <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">   Status: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> Completed</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> Agent: </span><span style=\"color: #008000; text-decoration-color: #008000\">Senior Data Researcher</span>\n",
       "       <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">    Status: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> Completed</span>\n",
       " <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> Task: a61dcf5a-644b-4bbb-a4da-8e36eea143e7</span>\n",
       "   <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">   Assigned to: </span><span style=\"color: #008000; text-decoration-color: #008000\">Reporting Analyst</span>\n",
       "   <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">   Status: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> Completed</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> Agent: </span><span style=\"color: #008000; text-decoration-color: #008000\">Reporting Analyst</span>\n",
       "       <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">    Status: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> Completed</span>\n",
       " <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> Task: 921e9ce5-1e12-4a66-89af-19846ee2f50f</span>\n",
       "    <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">   Status: </span><span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">Executing Task...</span>\n",
       "     <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> Agent: </span><span style=\"color: #008000; text-decoration-color: #008000\">Reporting Analyst</span>\n",
       "        <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">    Status: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> Completed</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m Crew: \u001b[0m\u001b[1;36mcrew\u001b[0m\n",
       " \u001b[1;32m Task: 27ec6611-922a-4fb3-9047-a980986e67a0\u001b[0m\n",
       "   \u001b[37m   Assigned to: \u001b[0m\u001b[32mSenior Data Researcher\u001b[0m\n",
       "   \u001b[37m   Status: \u001b[0m\u001b[1;32m Completed\u001b[0m\n",
       "    \u001b[1;32m Agent: \u001b[0m\u001b[32mSenior Data Researcher\u001b[0m\n",
       "       \u001b[37m    Status: \u001b[0m\u001b[1;32m Completed\u001b[0m\n",
       " \u001b[1;32m Task: a61dcf5a-644b-4bbb-a4da-8e36eea143e7\u001b[0m\n",
       "   \u001b[37m   Assigned to: \u001b[0m\u001b[32mReporting Analyst\u001b[0m\n",
       "   \u001b[37m   Status: \u001b[0m\u001b[1;32m Completed\u001b[0m\n",
       "    \u001b[1;32m Agent: \u001b[0m\u001b[32mReporting Analyst\u001b[0m\n",
       "       \u001b[37m    Status: \u001b[0m\u001b[1;32m Completed\u001b[0m\n",
       " \u001b[1;33m Task: 921e9ce5-1e12-4a66-89af-19846ee2f50f\u001b[0m\n",
       "    \u001b[37m   Status: \u001b[0m\u001b[2;33mExecuting Task...\u001b[0m\n",
       "     \u001b[1;32m Agent: \u001b[0m\u001b[32mReporting Analyst\u001b[0m\n",
       "        \u001b[37m    Status: \u001b[0m\u001b[1;32m Completed\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> Crew: crew</span>\n",
       " <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> Task: 27ec6611-922a-4fb3-9047-a980986e67a0</span>\n",
       "   <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">   Assigned to: </span><span style=\"color: #008000; text-decoration-color: #008000\">Senior Data Researcher</span>\n",
       "   <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">   Status: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> Completed</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> Agent: </span><span style=\"color: #008000; text-decoration-color: #008000\">Senior Data Researcher</span>\n",
       "       <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">    Status: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> Completed</span>\n",
       " <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> Task: a61dcf5a-644b-4bbb-a4da-8e36eea143e7</span>\n",
       "   <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">   Assigned to: </span><span style=\"color: #008000; text-decoration-color: #008000\">Reporting Analyst</span>\n",
       "   <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">   Status: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> Completed</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> Agent: </span><span style=\"color: #008000; text-decoration-color: #008000\">Reporting Analyst</span>\n",
       "       <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">    Status: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> Completed</span>\n",
       " <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> Task: 921e9ce5-1e12-4a66-89af-19846ee2f50f</span>\n",
       "    <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">   Assigned to: </span><span style=\"color: #008000; text-decoration-color: #008000\">Reporting Analyst</span>\n",
       "    <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">   Status: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> Completed</span>\n",
       "     <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> Agent: </span><span style=\"color: #008000; text-decoration-color: #008000\">Reporting Analyst</span>\n",
       "        <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">    Status: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> Completed</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m Crew: \u001b[0m\u001b[1;36mcrew\u001b[0m\n",
       " \u001b[1;32m Task: 27ec6611-922a-4fb3-9047-a980986e67a0\u001b[0m\n",
       "   \u001b[37m   Assigned to: \u001b[0m\u001b[32mSenior Data Researcher\u001b[0m\n",
       "   \u001b[37m   Status: \u001b[0m\u001b[1;32m Completed\u001b[0m\n",
       "    \u001b[1;32m Agent: \u001b[0m\u001b[32mSenior Data Researcher\u001b[0m\n",
       "       \u001b[37m    Status: \u001b[0m\u001b[1;32m Completed\u001b[0m\n",
       " \u001b[1;32m Task: a61dcf5a-644b-4bbb-a4da-8e36eea143e7\u001b[0m\n",
       "   \u001b[37m   Assigned to: \u001b[0m\u001b[32mReporting Analyst\u001b[0m\n",
       "   \u001b[37m   Status: \u001b[0m\u001b[1;32m Completed\u001b[0m\n",
       "    \u001b[1;32m Agent: \u001b[0m\u001b[32mReporting Analyst\u001b[0m\n",
       "       \u001b[37m    Status: \u001b[0m\u001b[1;32m Completed\u001b[0m\n",
       " \u001b[1;32m Task: 921e9ce5-1e12-4a66-89af-19846ee2f50f\u001b[0m\n",
       "    \u001b[37m   Assigned to: \u001b[0m\u001b[32mReporting Analyst\u001b[0m\n",
       "    \u001b[37m   Status: \u001b[0m\u001b[1;32m Completed\u001b[0m\n",
       "     \u001b[1;32m Agent: \u001b[0m\u001b[32mReporting Analyst\u001b[0m\n",
       "        \u001b[37m    Status: \u001b[0m\u001b[1;32m Completed\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\"> Task Completion </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\"></span>                                                                                                                 <span style=\"color: #008000; text-decoration-color: #008000\"></span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\"></span>  <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Task Completed</span>                                                                                                 <span style=\"color: #008000; text-decoration-color: #008000\"></span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\"></span>  <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">Name: </span><span style=\"color: #008000; text-decoration-color: #008000\">921e9ce5-1e12-4a66-89af-19846ee2f50f</span>                                                                     <span style=\"color: #008000; text-decoration-color: #008000\"></span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\"></span>  <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">Agent: </span><span style=\"color: #008000; text-decoration-color: #008000\">Reporting Analyst</span>                                                                                       <span style=\"color: #008000; text-decoration-color: #008000\"></span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\"></span>                                                                                                                 <span style=\"color: #008000; text-decoration-color: #008000\"></span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\"></span>                                                                                                                 <span style=\"color: #008000; text-decoration-color: #008000\"></span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\"></span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32m\u001b[0m\u001b[32m\u001b[0m\u001b[32m Task Completion \u001b[0m\u001b[32m\u001b[0m\u001b[32m\u001b[0m\n",
       "\u001b[32m\u001b[0m                                                                                                                 \u001b[32m\u001b[0m\n",
       "\u001b[32m\u001b[0m  \u001b[1;32mTask Completed\u001b[0m                                                                                                 \u001b[32m\u001b[0m\n",
       "\u001b[32m\u001b[0m  \u001b[37mName: \u001b[0m\u001b[32m921e9ce5-1e12-4a66-89af-19846ee2f50f\u001b[0m                                                                     \u001b[32m\u001b[0m\n",
       "\u001b[32m\u001b[0m  \u001b[37mAgent: \u001b[0m\u001b[32mReporting Analyst\u001b[0m                                                                                       \u001b[32m\u001b[0m\n",
       "\u001b[32m\u001b[0m                                                                                                                 \u001b[32m\u001b[0m\n",
       "\u001b[32m\u001b[0m                                                                                                                 \u001b[32m\u001b[0m\n",
       "\u001b[32m\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\"> Crew Completion </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\"></span>                                                                                                                 <span style=\"color: #008000; text-decoration-color: #008000\"></span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\"></span>  <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Crew Execution Completed</span>                                                                                       <span style=\"color: #008000; text-decoration-color: #008000\"></span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\"></span>  <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">Name: </span><span style=\"color: #008000; text-decoration-color: #008000\">crew</span>                                                                                                     <span style=\"color: #008000; text-decoration-color: #008000\"></span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\"></span>  <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">ID: </span><span style=\"color: #008000; text-decoration-color: #008000\">1c9e5c97-c97d-4678-bc5e-23f5b2e7e4aa</span>                                                                       <span style=\"color: #008000; text-decoration-color: #008000\"></span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\"></span>                                                                                                                 <span style=\"color: #008000; text-decoration-color: #008000\"></span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\"></span>                                                                                                                 <span style=\"color: #008000; text-decoration-color: #008000\"></span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\"></span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32m\u001b[0m\u001b[32m\u001b[0m\u001b[32m Crew Completion \u001b[0m\u001b[32m\u001b[0m\u001b[32m\u001b[0m\n",
       "\u001b[32m\u001b[0m                                                                                                                 \u001b[32m\u001b[0m\n",
       "\u001b[32m\u001b[0m  \u001b[1;32mCrew Execution Completed\u001b[0m                                                                                       \u001b[32m\u001b[0m\n",
       "\u001b[32m\u001b[0m  \u001b[37mName: \u001b[0m\u001b[32mcrew\u001b[0m                                                                                                     \u001b[32m\u001b[0m\n",
       "\u001b[32m\u001b[0m  \u001b[37mID: \u001b[0m\u001b[32m1c9e5c97-c97d-4678-bc5e-23f5b2e7e4aa\u001b[0m                                                                       \u001b[32m\u001b[0m\n",
       "\u001b[32m\u001b[0m                                                                                                                 \u001b[32m\u001b[0m\n",
       "\u001b[32m\u001b[0m                                                                                                                 \u001b[32m\u001b[0m\n",
       "\u001b[32m\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<div>\n",
       "  <style scoped>\n",
       "  button {\n",
       "    border: none;\n",
       "    border-radius: 4px;\n",
       "    background-color: rgb(34, 114, 180);\n",
       "    font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, \"Helvetica Neue\", Arial;\n",
       "    font-size: 13px;\n",
       "    color: white;\n",
       "    margin-top: 8px;\n",
       "    margin-bottom: 8px;\n",
       "    padding: 8px 16px;\n",
       "    cursor: pointer;\n",
       "  }\n",
       "  button:hover {\n",
       "    background-color: rgb(66, 153, 224);\n",
       "  }\n",
       "  </style>\n",
       "  <button\n",
       "    onclick=\"\n",
       "        const display = this.nextElementSibling.style.display;\n",
       "        const isCollapsed = display === 'none';\n",
       "        this.nextElementSibling.style.display = isCollapsed ? null : 'none';\n",
       "\n",
       "        const verb = isCollapsed ? 'Collapse' : 'Expand';\n",
       "        this.innerText = `${verb} MLflow Trace`;\n",
       "    \"\n",
       "  >Collapse MLflow Trace</button>\n",
       "  <iframe\n",
       "    id=\"trace-renderer\"\n",
       "    style=\"width: 100%; height: 500px; border: none; resize: vertical;\"\n",
       "    src=\"https://public-tracking.mlflow-e00rfd6s4w3cdtk9fx.backbone-e00rt16p71ew33dg9e.msp.eu-north1.nebius.cloud/static-files/lib/notebook-trace-renderer/index.html?trace_id=6387270d7e444426bcaf8e39caf96db4&amp;experiment_id=6&amp;version=2.21.2\"\n",
       "  />\n",
       "</div>\n"
      ],
      "text/plain": [
       "Trace(request_id=6387270d7e444426bcaf8e39caf96db4)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let's run it once without MLflow to see the typical verbose console output.\n",
    "\n",
    "task_description = \"Develop a conversational RAG system that can answer questions based on a large PDF document collection\"\n",
    "existing_stack = \"LangChain, PogreSQL, FastAPI\"\n",
    "\n",
    "ai_dev_crew = AIOpsResearchCrew(task_description, existing_stack)\n",
    "result = ai_dev_crew.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-17T05:54:48.022491Z",
     "iopub.status.busy": "2025-04-17T05:54:48.022319Z",
     "iopub.status.idle": "2025-04-17T05:54:48.066139Z",
     "shell.execute_reply": "2025-04-17T05:54:48.065702Z",
     "shell.execute_reply.started": "2025-04-17T05:54:48.022479Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Final Result (Base Run) ===\n",
      "# Comprehensive Markdown Report: Analysis of Tools for Developing a Conversational RAG System\n",
      "\n",
      "## Introduction\n",
      "The aim of this task is to **develop a conversational Retrieval-Augmented Generation (RAG) system** that can efficiently answer questions based on a large collection of PDF documents. The existing technology stack includes **LangChain**, **PostgreSQL**, and **FastAPI**. This report evaluates various AI libraries and frameworks, focusing on their suitability for this specific use case.\n",
      "\n",
      "## Tool Analysis\n",
      "\n",
      "### 1. [LangChain](https://langchain.com/)\n",
      "**Description:** LangChain is a composable framework designed to build applications powered by language models (LLMs). It facilitates integration with various data sources and APIs, making it a perfect fit for building conversational RAG systems.\n",
      "\n",
      "- **Features:**\n",
      "  - Modular architecture\n",
      "  - Strong focus on LLM integration\n",
      "  - Extensive API support\n",
      "\n",
      "- **Pros:**\n",
      "  - Highly modular and composable\n",
      "  - Strong community support with rapid growth\n",
      "  - Rich features for LLM applications\n",
      "\n",
      "- **Cons:**\n",
      "  - Initial setup complexity\n",
      "  - Potentially resource-intensive\n",
      "\n",
      "- **Integration Complexity:** High compatibility with PostgreSQL and FastAPI, requires effort for initial setup.\n",
      "\n",
      "- **Recommendation Score:** **9/10** \n",
      "  - Justification: Highly suitable for RAG due to context handling and LLM integration support.\n",
      "\n",
      "### 2. [TensorFlow](https://www.tensorflow.org/)\n",
      "**Description:** TensorFlow is an open-source library developed by Google for building and training machine learning models, particularly deep learning models. Its robust capabilities make it suitable for complex tasks.\n",
      "\n",
      "- **Features:**\n",
      "  - Extensive ecosystem for deep learning\n",
      "  - Strong support for production use\n",
      "\n",
      "- **Pros:**\n",
      "  - Active community and well-supported libraries\n",
      "  - High-quality documentation and tutorials\n",
      "\n",
      "- **Cons:**\n",
      "  - Steep learning curve for beginners\n",
      "  - Higher operational overhead\n",
      "\n",
      "- **Integration Complexity:** Moderate compatibility with LangChain and FastAPI, low with PostgreSQL.\n",
      "\n",
      "- **Recommendation Score:** **9/10** \n",
      "  - Justification: Excellent for conversational RAG systems due to NLP extensibility.\n",
      "\n",
      "### 3. [PyTorch](https://pytorch.org/)\n",
      "**Description:** Developed by Facebook, PyTorch is known for its flexibility and ease of use, especially in research and prototyping applications.\n",
      "\n",
      "- **Features:**\n",
      "  - Dynamic computation graphs\n",
      "  - User-friendly API\n",
      "\n",
      "- **Pros:**\n",
      "  - Strong prototyping capabilities\n",
      "  - Popular amongst the research community\n",
      "\n",
      "- **Cons:**\n",
      "  - Deployment complexity\n",
      "  - API stability issues with updates\n",
      "\n",
      "- **Integration Complexity:** High compatibility with LangChain and FastAPI; moderate with PostgreSQL.\n",
      "\n",
      "- **Recommendation Score:** **8/10** \n",
      "  - Justification: Highly suitable for flexible model experimentation in RAG systems.\n",
      "\n",
      "### 4. [Keras](https://keras.io/)\n",
      "**Description:** Keras is a high-level API for building and training deep learning models, acting as an interface for TensorFlow.\n",
      "\n",
      "- **Features:**\n",
      "  - Simplifies model building and training\n",
      "  - Quick prototyping capabilities\n",
      "\n",
      "- **Pros:**\n",
      "  - User-friendly for beginners\n",
      "  - Suitable for educational purposes\n",
      "\n",
      "- **Cons:**\n",
      "  - Dependent on TensorFlow for advanced functionalities\n",
      "  - Less flexible for complex systems\n",
      "\n",
      "- **Integration Complexity:** Moderate compatibility with LangChain and FastAPI; low with PostgreSQL.\n",
      "\n",
      "- **Recommendation Score:** **7/10** \n",
      "  - Justification: Useful for early-stage development, but limited for complex conversational tasks.\n",
      "\n",
      "### 5. [H2O.ai](https://www.h2o.ai/)\n",
      "**Description:** H2O.ai converges Predictive AI with Generative AI, providing tools for a variety of use cases, particularly focusing on rapid model deployment.\n",
      "\n",
      "- **Features:**\n",
      "  - Automated machine learning\n",
      "  - Rapid model training capabilities\n",
      "\n",
      "- **Pros:**\n",
      "  - User-friendly interface\n",
      "  - Strong focus on efficiency\n",
      "\n",
      "- **Cons:**\n",
      "  - Limited control and flexibility\n",
      "  - Higher cost associated with extensive use\n",
      "\n",
      "- **Integration Complexity:** Moderate compatibility with LangChain; low with PostgreSQL and FastAPI.\n",
      "\n",
      "- **Recommendation Score:** **6/10** \n",
      "  - Justification: May assist in deployment, but not ideal for complex query handling in conversational AI.\n",
      "\n",
      "## Summary/Conclusion\n",
      "After analyzing the tools, **LangChain** and **TensorFlow** emerge as the top contenders for developing a conversational RAG system, both scoring **9/10**. LangChain stands out for its modular design and direct focus on LLM integration, while TensorFlow offers robust deep-learning capabilities. **PyTorch** follows closely with a score of **8/10**, providing flexibility for RAG applications. **Keras** and **H2O.ai** come in at **7/10** and **6/10** respectively, with limited effectiveness for more complex systems and specific use cases in conversational AI.\n",
      "\n",
      "In conclusion, for building a sophisticated conversational RAG system, investing resources in **LangChain** or **TensorFlow** is highly recommended, while **PyTorch** can serve as a viable alternative for flexible model experimentation.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Final Result (Base Run) ===\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observe \n",
    "\n",
    "- Logs are great! But it's a lot of them! \n",
    "- Difficult to understand the process: How was the report generated? What internal steps were taken?\n",
    "- No cost tracking: How many LLM tokens were used? What was the cost?\n",
    "- Debugging Difficulty: Errors are hard to trace without visibility.\n",
    "- Subjective Quality: Is the output report actually good? Accurate? Relevant? No way to know objectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Aautolog Agent execution and LLM calls with MLflow\n",
    "\n",
    "**Problem:** \n",
    "- The console output scrolls by. How can we reliably capture *exactly* what the agent did, including its reasoning steps and tool usage, for later inspection or debugging?\n",
    "\n",
    "**MLflow Solution:** \n",
    "- Turn on auto tracing by calling mlflow.crewai.autolog()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-17T05:55:37.829437Z",
     "iopub.status.busy": "2025-04-17T05:55:37.829025Z",
     "iopub.status.idle": "2025-04-17T05:57:17.905051Z",
     "shell.execute_reply": "2025-04-17T05:57:17.904529Z",
     "shell.execute_reply.started": "2025-04-17T05:55:37.829407Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\"> Crew Execution Started </span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\"></span>                                                                                                                 <span style=\"color: #008080; text-decoration-color: #008080\"></span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\"></span>  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Crew Execution Started</span>                                                                                         <span style=\"color: #008080; text-decoration-color: #008080\"></span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\"></span>  <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">Name: </span><span style=\"color: #008080; text-decoration-color: #008080\">crew</span>                                                                                                     <span style=\"color: #008080; text-decoration-color: #008080\"></span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\"></span>  <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">ID: </span><span style=\"color: #008080; text-decoration-color: #008080\">840727b7-e14c-4420-baea-712748a0939c</span>                                                                       <span style=\"color: #008080; text-decoration-color: #008080\"></span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\"></span>                                                                                                                 <span style=\"color: #008080; text-decoration-color: #008080\"></span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\"></span>                                                                                                                 <span style=\"color: #008080; text-decoration-color: #008080\"></span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\"></span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36m\u001b[0m\u001b[36m\u001b[0m\u001b[36m Crew Execution Started \u001b[0m\u001b[36m\u001b[0m\u001b[36m\u001b[0m\n",
       "\u001b[36m\u001b[0m                                                                                                                 \u001b[36m\u001b[0m\n",
       "\u001b[36m\u001b[0m  \u001b[1;36mCrew Execution Started\u001b[0m                                                                                         \u001b[36m\u001b[0m\n",
       "\u001b[36m\u001b[0m  \u001b[37mName: \u001b[0m\u001b[36mcrew\u001b[0m                                                                                                     \u001b[36m\u001b[0m\n",
       "\u001b[36m\u001b[0m  \u001b[37mID: \u001b[0m\u001b[36m840727b7-e14c-4420-baea-712748a0939c\u001b[0m                                                                       \u001b[36m\u001b[0m\n",
       "\u001b[36m\u001b[0m                                                                                                                 \u001b[36m\u001b[0m\n",
       "\u001b[36m\u001b[0m                                                                                                                 \u001b[36m\u001b[0m\n",
       "\u001b[36m\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> Crew: crew</span>\n",
       " <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> Task: 05ce6d78-654b-4e32-bffe-42e4b0db2d93</span>\n",
       "    <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">   Status: </span><span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">Executing Task...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m Crew: \u001b[0m\u001b[1;36mcrew\u001b[0m\n",
       " \u001b[1;33m Task: 05ce6d78-654b-4e32-bffe-42e4b0db2d93\u001b[0m\n",
       "    \u001b[37m   Status: \u001b[0m\u001b[2;33mExecuting Task...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> Crew: crew</span>\n",
       " <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> Task: 05ce6d78-654b-4e32-bffe-42e4b0db2d93</span>\n",
       "    <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">   Status: </span><span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">Executing Task...</span>\n",
       "     <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> Agent: </span><span style=\"color: #008000; text-decoration-color: #008000\">Senior Data Researcher</span>\n",
       "        <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">    Status: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">In Progress</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m Crew: \u001b[0m\u001b[1;36mcrew\u001b[0m\n",
       " \u001b[1;33m Task: 05ce6d78-654b-4e32-bffe-42e4b0db2d93\u001b[0m\n",
       "    \u001b[37m   Status: \u001b[0m\u001b[2;33mExecuting Task...\u001b[0m\n",
       "     \u001b[1;32m Agent: \u001b[0m\u001b[32mSenior Data Researcher\u001b[0m\n",
       "        \u001b[37m    Status: \u001b[0m\u001b[1;32mIn Progress\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mSenior Data Researcher\u001b[00m\n",
      "\u001b[95m## Task:\u001b[00m \u001b[92m\n",
      "You are a research agent tasked with finding AI tools for: {task}.\n",
      "Consider compatibility with: {ai_stack}.\n",
      "\n",
      "For each tool, identify:\n",
      "- Name and URL\n",
      "- Primary use case\n",
      "- Brief description (2-3 sentences)\n",
      "\n",
      "Format your response as a JSON list of objects.\n",
      "\u001b[00m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> Agent: </span><span style=\"color: #008000; text-decoration-color: #008000\">Senior Data Researcher</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">    Status: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">In Progress</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32m Agent: \u001b[0m\u001b[32mSenior Data Researcher\u001b[0m\n",
       "\u001b[37m    Status: \u001b[0m\u001b[1;32mIn Progress\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mSenior Data Researcher\u001b[00m\n",
      "\u001b[95m## Thought:\u001b[00m \u001b[92mI need to search for relevant AI tools that can be utilized for specific tasks compatible with the given AI stack. This requires identifying suitable tools, their primary use cases, and providing brief descriptions.\u001b[00m\n",
      "\u001b[95m## Using tool:\u001b[00m \u001b[92mSearch in a specific website\u001b[00m\n",
      "\u001b[95m## Tool Input:\u001b[00m \u001b[92m\n",
      "\"{\\\"search_query\\\": \\\"AI tools for developing conversational RAG systems\\\", \\\"website\\\": \\\"https://www.h2o.ai/\\\"}\"\u001b[00m\n",
      "\u001b[95m## Tool Output:\u001b[00m \u001b[92m\n",
      "Relevant Content:\n",
      "H2O.ai | Convergence of The World's Best Predictive & Generative AI The World's Best Deep Research Chat Now Chat on Mobile Learn More Companies powered by H2O.ai H2Os flagship SLMs H2OVL Mississippi SVLM Series Our newest economical multimodal OCR model developed for Document AI H2OVL Mississippi-2B, based on H2O Danube2, is trained on 17.3M conversation pairs for high-res image handling. The .8B model, built on Danube3, leads OCR benchmarks with 19M pairs, outperforming all SLMs in text recognition. For Developers  1 Text Recognition H2O Danube SLM Series Our most economical small model for fast, lightweight tasks We trained H2O Danube3 models from scratch on ~100 H100 GPUs using our own curated dataset of 6T tokens. H2O Danube3-4B and .5B open-weight SLMs outperform the latest Apple OpenELM-3B and .5B instruct models. Perfect for developers who want to fine-tune their own SLMs for offline use cases. For Developers Get the Mobile App H2O Generative AI in use with customers H2O Document AI GPTs and Digital Agents in production worldwide In-Store Sales Assistant HR & IT Support Agent Fraud & Scam Detection RFP Assistant Procurement GPT Personalized Pricing Complaint Summarizer Distilled SLMs for Call Center Classification First-ever model risk management for Generative AI Generative AI is herebut can you audit it? H2O.ai brings Model Risk Management (MRM) to GenAI, combining enterprise-grade model evaluation with h2oGPTe and H2O Model Validation to make AI measurable, explainable, and compliant for banks and financial services. Automated testing, human-calibrated evaluations, and real-time risk monitoring ensure transparencyboosting compliance, risk management, and user confidence. H2O Model Validation Multi-modal Document AI with enterprise scale and security Convert Data to JSON Document AI ProcurementGPT Complaint Summarizer Convert structured data like financial tables to JSON Get answers from diagrams, flowcharts, web pages, audio files, video files and\n",
      "\n",
      "Visionary in Gartner Magic Quadrant: Data Science and Machine Learning 2024 Listed in Gartner Magic Quadrant (Cloud AI Developer Services) in 2020, 2021, 2022, 2023, and 2024 Listed in Gartner Magic Quadrant (Data Science and Machine Learning) in 2018, 2019, 2020, 2021, 2023, and 2024 Get the Report GenAI App Store As the maker of the worlds best AutoML, H2O.ai is converging Predictive AI with Generative AI, and helping leading global brands save billions of dollars a year with purpose-built AI for hundreds of use cases such as: Fraud and scam detection Complaint summarizer Recommendation engine KYC, loan application risk Pricing engines and more Try Sample GenAI Apps Built On h2oGPT Get your API key H2O Generative AI can solve a broad spectrum of natural language use cases, including classification, semantic search, paraphrasing, summarization, and content generation. h2oGPTe is used by companies to build production-ready, scalable and efficient AI-powered applications. Write custom apps with fully featured Python client APIs Seamlessly integrate state-of-the-art text, image, and code generation capabilities into your projects Get your API Key\n",
      "\n",
      "LangChain Sign up to be the first to access recordings from Interrupt, The AI Agent Conference!Learn MoreFrom startups to global enterprises, ambitious builders choose LangChain products.BuildLangChain is a composable framework to build with LLMs. LangGraph is the orchestration framework for controllable agentic workflows.RunDeploy your LLM applications at scale with LangGraph Platform, our infrastructure purpose-built for agents.ManageLangSmith is a unified agent observability and evals platform to optimize the performance of your AI agents - whether they're built with a LangChain framework or not. Build your app with LangChainBuild context-aware, reasoning applications with LangChains flexible framework that leverages your companys data and APIs. Future-proof your application by making vendor optionality part of your LLM infrastructure design.Learn more about LangChainRun at scale with LangGraph PlatformUse LangGraph Platforms APIs to design agent-driven user experiences featuring human-in-the-loop, multi-agent collaboration, conversation history, long-term memory, and time-travel. Deploy with fault-tolerant scalability.Learn more about LangGraph PlatformManage agent observability & performance with LangSmithGet visibility into your LLM calls and agent reasoning with LangSmith to quickly debug failures. Run evals to assess your agent performance over production traffic  so you can ensure high-quality responses. Learn more about LangSmithHear from our happy customersLangChain, LangGraph, and LangSmith help teams of all sizes, across all industries - from ambitious startups to established enterprises.By combining the benefits of LangSmith and standing on the shoulders of a gigantic open-source community, were able to identify the right approaches of using LLMs in an enterprise-setting faster.Yusuke KajiGeneral Manager of AIWorking with LangChain and LangSmith on the Elastic AI Assistant had a significant positive impact on the overall pace and quality of the\u001b[00m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> Agent: </span><span style=\"color: #008000; text-decoration-color: #008000\">Senior Data Researcher</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">    Status: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">In Progress</span>\n",
       " <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\"> </span><span style=\"color: #000080; text-decoration-color: #000080\">Thinking...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32m Agent: \u001b[0m\u001b[32mSenior Data Researcher\u001b[0m\n",
       "\u001b[37m    Status: \u001b[0m\u001b[1;32mIn Progress\u001b[0m\n",
       " \u001b[1;34m \u001b[0m\u001b[34mThinking...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> Agent: </span><span style=\"color: #008000; text-decoration-color: #008000\">Senior Data Researcher</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">    Status: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">In Progress</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32m Agent: \u001b[0m\u001b[32mSenior Data Researcher\u001b[0m\n",
       "\u001b[37m    Status: \u001b[0m\u001b[1;32mIn Progress\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mSenior Data Researcher\u001b[00m\n",
      "\u001b[95m## Final Answer:\u001b[00m \u001b[92m\n",
      "```json\n",
      "[\n",
      "    {\n",
      "        \"name\": \"H2O.ai\",\n",
      "        \"url\": \"https://www.h2o.ai/\",\n",
      "        \"primary_use_case\": \"Automated Machine Learning and Generative AI\",\n",
      "        \"description\": \"H2O.ai is a platform that combines predictive and generative AI capabilities. It provides tools for various use cases including fraud detection, recommendation engines, and document AI capabilities, making it beneficial for developing conversational systems.\"\n",
      "    },\n",
      "    {\n",
      "        \"name\": \"LangChain\",\n",
      "        \"url\": \"https://langchain.com/\",\n",
      "        \"primary_use_case\": \"Framework for LLM applications\",\n",
      "        \"description\": \"LangChain is a modular framework designed for building applications powered by language models (LLMs). It enables seamless integration with various data sources and APIs, optimizing the creation of context-aware conversational systems.\"\n",
      "    },\n",
      "    {\n",
      "        \"name\": \"TensorFlow\",\n",
      "        \"url\": \"https://www.tensorflow.org/\",\n",
      "        \"primary_use_case\": \"Deep Learning Framework\",\n",
      "        \"description\": \"TensorFlow is an open-source library developed by Google for building and training machine learning models, particularly suitable for deep learning tasks. Its extensive features make it a strong choice for developing complex conversational AI systems.\"\n",
      "    },\n",
      "    {\n",
      "        \"name\": \"PyTorch\",\n",
      "        \"url\": \"https://pytorch.org/\",\n",
      "        \"primary_use_case\": \"Flexible Deep Learning Framework\",\n",
      "        \"description\": \"PyTorch, developed by Facebook, is a widely used deep learning framework that provides exceptional flexibility and ease of use. It is particularly favored in research and prototyping, ideal for developing innovative conversational technologies.\"\n",
      "    },\n",
      "    {\n",
      "        \"name\": \"Keras\",\n",
      "        \"url\": \"https://keras.io/\",\n",
      "        \"primary_use_case\": \"High-level Neural Network API\",\n",
      "        \"description\": \"Keras is a user-friendly, high-level API for building and training neural networks. It simplifies experimentation and model development, making it a good option for rapid prototyping in conversational AI applications.\"\n",
      "    }\n",
      "]\n",
      "```\u001b[00m\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages/instructor/process_response.py:222: DeprecationWarning: FUNCTIONS is deprecated and will be removed in future versions\n",
      "  if mode == Mode.FUNCTIONS:\n",
      "/Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages/instructor/function_calls.py:118: DeprecationWarning: FUNCTIONS is deprecated and will be removed in future versions\n",
      "  if mode == Mode.FUNCTIONS:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> Crew: crew</span>\n",
       " <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> Task: 05ce6d78-654b-4e32-bffe-42e4b0db2d93</span>\n",
       "    <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">   Status: </span><span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">Executing Task...</span>\n",
       "     <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> Agent: </span><span style=\"color: #008000; text-decoration-color: #008000\">Senior Data Researcher</span>\n",
       "        <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">    Status: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> Completed</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m Crew: \u001b[0m\u001b[1;36mcrew\u001b[0m\n",
       " \u001b[1;33m Task: 05ce6d78-654b-4e32-bffe-42e4b0db2d93\u001b[0m\n",
       "    \u001b[37m   Status: \u001b[0m\u001b[2;33mExecuting Task...\u001b[0m\n",
       "     \u001b[1;32m Agent: \u001b[0m\u001b[32mSenior Data Researcher\u001b[0m\n",
       "        \u001b[37m    Status: \u001b[0m\u001b[1;32m Completed\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> Crew: crew</span>\n",
       " <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> Task: 05ce6d78-654b-4e32-bffe-42e4b0db2d93</span>\n",
       "    <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">   Assigned to: </span><span style=\"color: #008000; text-decoration-color: #008000\">Senior Data Researcher</span>\n",
       "    <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">   Status: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> Completed</span>\n",
       "     <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> Agent: </span><span style=\"color: #008000; text-decoration-color: #008000\">Senior Data Researcher</span>\n",
       "        <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">    Status: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> Completed</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m Crew: \u001b[0m\u001b[1;36mcrew\u001b[0m\n",
       " \u001b[1;32m Task: 05ce6d78-654b-4e32-bffe-42e4b0db2d93\u001b[0m\n",
       "    \u001b[37m   Assigned to: \u001b[0m\u001b[32mSenior Data Researcher\u001b[0m\n",
       "    \u001b[37m   Status: \u001b[0m\u001b[1;32m Completed\u001b[0m\n",
       "     \u001b[1;32m Agent: \u001b[0m\u001b[32mSenior Data Researcher\u001b[0m\n",
       "        \u001b[37m    Status: \u001b[0m\u001b[1;32m Completed\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\"> Task Completion </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\"></span>                                                                                                                 <span style=\"color: #008000; text-decoration-color: #008000\"></span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\"></span>  <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Task Completed</span>                                                                                                 <span style=\"color: #008000; text-decoration-color: #008000\"></span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\"></span>  <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">Name: </span><span style=\"color: #008000; text-decoration-color: #008000\">05ce6d78-654b-4e32-bffe-42e4b0db2d93</span>                                                                     <span style=\"color: #008000; text-decoration-color: #008000\"></span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\"></span>  <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">Agent: </span><span style=\"color: #008000; text-decoration-color: #008000\">Senior Data Researcher</span>                                                                                  <span style=\"color: #008000; text-decoration-color: #008000\"></span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\"></span>                                                                                                                 <span style=\"color: #008000; text-decoration-color: #008000\"></span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\"></span>                                                                                                                 <span style=\"color: #008000; text-decoration-color: #008000\"></span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\"></span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32m\u001b[0m\u001b[32m\u001b[0m\u001b[32m Task Completion \u001b[0m\u001b[32m\u001b[0m\u001b[32m\u001b[0m\n",
       "\u001b[32m\u001b[0m                                                                                                                 \u001b[32m\u001b[0m\n",
       "\u001b[32m\u001b[0m  \u001b[1;32mTask Completed\u001b[0m                                                                                                 \u001b[32m\u001b[0m\n",
       "\u001b[32m\u001b[0m  \u001b[37mName: \u001b[0m\u001b[32m05ce6d78-654b-4e32-bffe-42e4b0db2d93\u001b[0m                                                                     \u001b[32m\u001b[0m\n",
       "\u001b[32m\u001b[0m  \u001b[37mAgent: \u001b[0m\u001b[32mSenior Data Researcher\u001b[0m                                                                                  \u001b[32m\u001b[0m\n",
       "\u001b[32m\u001b[0m                                                                                                                 \u001b[32m\u001b[0m\n",
       "\u001b[32m\u001b[0m                                                                                                                 \u001b[32m\u001b[0m\n",
       "\u001b[32m\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> Crew: crew</span>\n",
       " <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> Task: 05ce6d78-654b-4e32-bffe-42e4b0db2d93</span>\n",
       "   <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">   Assigned to: </span><span style=\"color: #008000; text-decoration-color: #008000\">Senior Data Researcher</span>\n",
       "   <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">   Status: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> Completed</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> Agent: </span><span style=\"color: #008000; text-decoration-color: #008000\">Senior Data Researcher</span>\n",
       "       <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">    Status: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> Completed</span>\n",
       " <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> Task: 364f262f-aa7f-4ef8-bf97-62ec371b1f19</span>\n",
       "    <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">   Status: </span><span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">Executing Task...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m Crew: \u001b[0m\u001b[1;36mcrew\u001b[0m\n",
       " \u001b[1;32m Task: 05ce6d78-654b-4e32-bffe-42e4b0db2d93\u001b[0m\n",
       "   \u001b[37m   Assigned to: \u001b[0m\u001b[32mSenior Data Researcher\u001b[0m\n",
       "   \u001b[37m   Status: \u001b[0m\u001b[1;32m Completed\u001b[0m\n",
       "    \u001b[1;32m Agent: \u001b[0m\u001b[32mSenior Data Researcher\u001b[0m\n",
       "       \u001b[37m    Status: \u001b[0m\u001b[1;32m Completed\u001b[0m\n",
       " \u001b[1;33m Task: 364f262f-aa7f-4ef8-bf97-62ec371b1f19\u001b[0m\n",
       "    \u001b[37m   Status: \u001b[0m\u001b[2;33mExecuting Task...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> Crew: crew</span>\n",
       " <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> Task: 05ce6d78-654b-4e32-bffe-42e4b0db2d93</span>\n",
       "   <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">   Assigned to: </span><span style=\"color: #008000; text-decoration-color: #008000\">Senior Data Researcher</span>\n",
       "   <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">   Status: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> Completed</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> Agent: </span><span style=\"color: #008000; text-decoration-color: #008000\">Senior Data Researcher</span>\n",
       "       <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">    Status: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> Completed</span>\n",
       " <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> Task: 364f262f-aa7f-4ef8-bf97-62ec371b1f19</span>\n",
       "    <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">   Status: </span><span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">Executing Task...</span>\n",
       "     <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> Agent: </span><span style=\"color: #008000; text-decoration-color: #008000\">Reporting Analyst</span>\n",
       "        <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">    Status: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">In Progress</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m Crew: \u001b[0m\u001b[1;36mcrew\u001b[0m\n",
       " \u001b[1;32m Task: 05ce6d78-654b-4e32-bffe-42e4b0db2d93\u001b[0m\n",
       "   \u001b[37m   Assigned to: \u001b[0m\u001b[32mSenior Data Researcher\u001b[0m\n",
       "   \u001b[37m   Status: \u001b[0m\u001b[1;32m Completed\u001b[0m\n",
       "    \u001b[1;32m Agent: \u001b[0m\u001b[32mSenior Data Researcher\u001b[0m\n",
       "       \u001b[37m    Status: \u001b[0m\u001b[1;32m Completed\u001b[0m\n",
       " \u001b[1;33m Task: 364f262f-aa7f-4ef8-bf97-62ec371b1f19\u001b[0m\n",
       "    \u001b[37m   Status: \u001b[0m\u001b[2;33mExecuting Task...\u001b[0m\n",
       "     \u001b[1;32m Agent: \u001b[0m\u001b[32mReporting Analyst\u001b[0m\n",
       "        \u001b[37m    Status: \u001b[0m\u001b[1;32mIn Progress\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mReporting Analyst\u001b[00m\n",
      "\u001b[95m## Task:\u001b[00m \u001b[92m\n",
      "Read the tool list from the previous task's result and perform a detailed analysis of each tool.\n",
      "\n",
      "For each tool:\n",
      "1. Research its capabilities, limitations, community adoption, documentation quality\n",
      "2. Evaluate how well it addresses the specified task: 'Develop a conversational RAG system that can answer questions based on a large PDF document collection'\n",
      "3. Consider its compatibility with 'LangChain, PogreSQL, FastAPI'\n",
      "4. Identify pros and cons\n",
      "\n",
      "Your output should be a JSON list of these detailed analysis objects.\n",
      "\u001b[00m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> Agent: </span><span style=\"color: #008000; text-decoration-color: #008000\">Reporting Analyst</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">    Status: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">In Progress</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32m Agent: \u001b[0m\u001b[32mReporting Analyst\u001b[0m\n",
       "\u001b[37m    Status: \u001b[0m\u001b[1;32mIn Progress\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mReporting Analyst\u001b[00m\n",
      "\u001b[95m## Thought:\u001b[00m \u001b[92mThought: I need to research the capabilities, limitations, community adoption, and documentation quality of each tool indicated in the context provided to analyze their suitability for building a conversational RAG system.\u001b[00m\n",
      "\u001b[95m## Using tool:\u001b[00m \u001b[92mSearch in a specific website\u001b[00m\n",
      "\u001b[95m## Tool Input:\u001b[00m \u001b[92m\n",
      "\"{\\\"search_query\\\": \\\"H2O.ai capabilities, limitations, community adoption, documentation quality\\\", \\\"website\\\": \\\"https://www.h2o.ai/\\\"}\"\u001b[00m\n",
      "\u001b[95m## Tool Output:\u001b[00m \u001b[92m\n",
      "Relevant Content:\n",
      "H2O.ai | Convergence of The World's Best Predictive & Generative AI The World's Best Deep Research Chat Now Chat on Mobile Learn More Companies powered by H2O.ai H2Os flagship SLMs H2OVL Mississippi SVLM Series Our newest economical multimodal OCR model developed for Document AI H2OVL Mississippi-2B, based on H2O Danube2, is trained on 17.3M conversation pairs for high-res image handling. The .8B model, built on Danube3, leads OCR benchmarks with 19M pairs, outperforming all SLMs in text recognition. For Developers  1 Text Recognition H2O Danube SLM Series Our most economical small model for fast, lightweight tasks We trained H2O Danube3 models from scratch on ~100 H100 GPUs using our own curated dataset of 6T tokens. H2O Danube3-4B and .5B open-weight SLMs outperform the latest Apple OpenELM-3B and .5B instruct models. Perfect for developers who want to fine-tune their own SLMs for offline use cases. For Developers Get the Mobile App H2O Generative AI in use with customers H2O Document AI GPTs and Digital Agents in production worldwide In-Store Sales Assistant HR & IT Support Agent Fraud & Scam Detection RFP Assistant Procurement GPT Personalized Pricing Complaint Summarizer Distilled SLMs for Call Center Classification First-ever model risk management for Generative AI Generative AI is herebut can you audit it? H2O.ai brings Model Risk Management (MRM) to GenAI, combining enterprise-grade model evaluation with h2oGPTe and H2O Model Validation to make AI measurable, explainable, and compliant for banks and financial services. Automated testing, human-calibrated evaluations, and real-time risk monitoring ensure transparencyboosting compliance, risk management, and user confidence. H2O Model Validation Multi-modal Document AI with enterprise scale and security Convert Data to JSON Document AI ProcurementGPT Complaint Summarizer Convert structured data like financial tables to JSON Get answers from diagrams, flowcharts, web pages, audio files, video files and\n",
      "\n",
      "Visionary in Gartner Magic Quadrant: Data Science and Machine Learning 2024 Listed in Gartner Magic Quadrant (Cloud AI Developer Services) in 2020, 2021, 2022, 2023, and 2024 Listed in Gartner Magic Quadrant (Data Science and Machine Learning) in 2018, 2019, 2020, 2021, 2023, and 2024 Get the Report GenAI App Store As the maker of the worlds best AutoML, H2O.ai is converging Predictive AI with Generative AI, and helping leading global brands save billions of dollars a year with purpose-built AI for hundreds of use cases such as: Fraud and scam detection Complaint summarizer Recommendation engine KYC, loan application risk Pricing engines and more Try Sample GenAI Apps Built On h2oGPT Get your API key H2O Generative AI can solve a broad spectrum of natural language use cases, including classification, semantic search, paraphrasing, summarization, and content generation. h2oGPTe is used by companies to build production-ready, scalable and efficient AI-powered applications. Write custom apps with fully featured Python client APIs Seamlessly integrate state-of-the-art text, image, and code generation capabilities into your projects Get your API Key\n",
      "\n",
      "images. Intelligent document comparison with ProcurementGPT ProcurementGPT streamlines document analysis by automatically parsing documents into sections or clauses, then identifying the most similar counterparts across documentsworking symmetrically in both directions. It further breaks down clauses into key requirements, determining where each one is fulfilled and highlighting met and unmet requirements for easy review. The system leverages the top 10 most similar requirements to assess compliance, with an overlay for quick navigation. Users can also manually select sections for comparison, ensuring flexibility when automated parsing needs refinement. Learn More Summary of complaints and key issues Provides a high-level summary of the most common complaints, the top issues faced, and recommended actions to address these concerns. Learn More Built for air-gapped, on-premises, private or managed cloud deployments. Own your data, own your prompts. Only H2O.ai provides an end-to-end GenAI platform where you can own every part of the stack. H2O.ai offers a highly flexible solution, that can run fully managed in the cloud, or in hybrid or on-premise and air-gapped environments. Fully scalable with Kubernetes. H2O.ai achieves SOC2 Type 2 +HIPAA/HITECH Cost control and flexibility Customers can decide to mix and match from a large choice of over 30 LLMs: Proprietary LLMs: GPT-4o, o1, Gemini, Claude, Mistral, etc. Reliance on third-party provider No control, no ownership Token-based pricing, unbounded cost Open source LLMs: Llama3, DeepSeek, Qwen2.5, Mixtral, Mistral, H2O Danube, H2OVL Mississippi, any Fine-Tuned OSS model. Any model from Open-Source ecosystem Hosted on your own GPU infrastructure Full control and ownership of models Fixed hardware cost, most economical choice Cost savings on H2O.ai Platform. 25x cheaper on query cost. Serve more users. Better accuracy with up to 100% cheaper on Document Processing. h2oGPTe is designed to work with any LLM of your choice\u001b[00m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> Agent: </span><span style=\"color: #008000; text-decoration-color: #008000\">Reporting Analyst</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">    Status: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">In Progress</span>\n",
       " <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\"> </span><span style=\"color: #000080; text-decoration-color: #000080\">Thinking...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32m Agent: \u001b[0m\u001b[32mReporting Analyst\u001b[0m\n",
       "\u001b[37m    Status: \u001b[0m\u001b[1;32mIn Progress\u001b[0m\n",
       " \u001b[1;34m \u001b[0m\u001b[34mThinking...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> Agent: </span><span style=\"color: #008000; text-decoration-color: #008000\">Reporting Analyst</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">    Status: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">In Progress</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32m Agent: \u001b[0m\u001b[32mReporting Analyst\u001b[0m\n",
       "\u001b[37m    Status: \u001b[0m\u001b[1;32mIn Progress\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> Agent: </span><span style=\"color: #008000; text-decoration-color: #008000\">Reporting Analyst</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">    Status: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">In Progress</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32m Agent: \u001b[0m\u001b[32mReporting Analyst\u001b[0m\n",
       "\u001b[37m    Status: \u001b[0m\u001b[1;32mIn Progress\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mReporting Analyst\u001b[00m\n",
      "\u001b[95m## Thought:\u001b[00m \u001b[92mThought: I need to continue analyzing the remaining tools: LangChain, TensorFlow, PyTorch, and Keras for their capabilities, limitations, community adoption, and documentation quality.\u001b[00m\n",
      "\u001b[95m## Using tool:\u001b[00m \u001b[92mSearch in a specific website\u001b[00m\n",
      "\u001b[95m## Tool Input:\u001b[00m \u001b[92m\n",
      "\"{\\\"search_query\\\": \\\"LangChain capabilities, limitations, community adoption, documentation quality\\\", \\\"website\\\": \\\"https://langchain.com/\\\"}\"\u001b[00m\n",
      "\u001b[95m## Tool Output:\u001b[00m \u001b[92m\n",
      "Relevant Content:\n",
      "development and shipping experience. We couldnt have achieved the product experience delivered to our customers without LangChain, and we couldnt have done it at the same pace without LangSmith.James SpiteriDirector of Security ProductsAs soon as we heard about LangSmith, we moved our entire development stack onto it. We could have built evaluation, testing and monitoring tools in house, but with LangSmith it took us 10x less time to get a 1000x better tool.Jose PeaSenior Manager The reference architecture enterprises adopt for success.LangChains suite of products can be used independently or stacked together for multiplicative impact  guiding you through building, running, and managing your LLM apps.20M+Monthly Downloads100K+Apps Powered100K+GitHub Stars4K+ContributorsThe biggest developer community in GenAILearn alongside the 1M+ developers who are pushing the industry forward.Explore LangChainGet started with LangChain's suite of products.Get a demoSign up for freeReady to start shipping reliable GenAI apps faster?Get started with LangChain, LangSmith, and LangGraph to enhance your LLM app development, from prototype to production.Get a demoSign up for free\n",
      "\n",
      "development and shipping experience. We couldnt have achieved the product experience delivered to our customers without LangChain, and we couldnt have done it at the same pace without LangSmith.James SpiteriDirector of Security ProductsAs soon as we heard about LangSmith, we moved our entire development stack onto it. We could have built evaluation, testing and monitoring tools in house, but with LangSmith it took us 10x less time to get a 1000x better tool.Jose PeaSenior Manager The reference architecture enterprises adopt for success.LangChains suite of products can be used independently or stacked together for multiplicative impact  guiding you through building, running, and managing your LLM apps.20M+Monthly Downloads100K+Apps Powered100K+GitHub Stars4K+ContributorsThe biggest developer community in GenAILearn alongside the 1M+ developers who are pushing the industry forward.Explore LangChainGet started with LangChain's suite of products.Get a demoSign up for freeReady to start shipping reliable GenAI apps faster?Get started with LangChain, LangSmith, and LangGraph to enhance your LLM app development, from prototype to production.Get a demoSign up for free\n",
      "\n",
      "LangChain Sign up to be the first to access recordings from Interrupt, The AI Agent Conference!Learn MoreFrom startups to global enterprises, ambitious builders choose LangChain products.BuildLangChain is a composable framework to build with LLMs. LangGraph is the orchestration framework for controllable agentic workflows.RunDeploy your LLM applications at scale with LangGraph Platform, our infrastructure purpose-built for agents.ManageLangSmith is a unified agent observability and evals platform to optimize the performance of your AI agents - whether they're built with a LangChain framework or not. Build your app with LangChainBuild context-aware, reasoning applications with LangChains flexible framework that leverages your companys data and APIs. Future-proof your application by making vendor optionality part of your LLM infrastructure design.Learn more about LangChainRun at scale with LangGraph PlatformUse LangGraph Platforms APIs to design agent-driven user experiences featuring human-in-the-loop, multi-agent collaboration, conversation history, long-term memory, and time-travel. Deploy with fault-tolerant scalability.Learn more about LangGraph PlatformManage agent observability & performance with LangSmithGet visibility into your LLM calls and agent reasoning with LangSmith to quickly debug failures. Run evals to assess your agent performance over production traffic  so you can ensure high-quality responses. Learn more about LangSmithHear from our happy customersLangChain, LangGraph, and LangSmith help teams of all sizes, across all industries - from ambitious startups to established enterprises.By combining the benefits of LangSmith and standing on the shoulders of a gigantic open-source community, were able to identify the right approaches of using LLMs in an enterprise-setting faster.Yusuke KajiGeneral Manager of AIWorking with LangChain and LangSmith on the Elastic AI Assistant had a significant positive impact on the overall pace and quality of the\u001b[00m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> Agent: </span><span style=\"color: #008000; text-decoration-color: #008000\">Reporting Analyst</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">    Status: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">In Progress</span>\n",
       " <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\"> </span><span style=\"color: #000080; text-decoration-color: #000080\">Thinking...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32m Agent: \u001b[0m\u001b[32mReporting Analyst\u001b[0m\n",
       "\u001b[37m    Status: \u001b[0m\u001b[1;32mIn Progress\u001b[0m\n",
       " \u001b[1;34m \u001b[0m\u001b[34mThinking...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> Agent: </span><span style=\"color: #008000; text-decoration-color: #008000\">Reporting Analyst</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">    Status: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">In Progress</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32m Agent: \u001b[0m\u001b[32mReporting Analyst\u001b[0m\n",
       "\u001b[37m    Status: \u001b[0m\u001b[1;32mIn Progress\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> Agent: </span><span style=\"color: #008000; text-decoration-color: #008000\">Reporting Analyst</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">    Status: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">In Progress</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32m Agent: \u001b[0m\u001b[32mReporting Analyst\u001b[0m\n",
       "\u001b[37m    Status: \u001b[0m\u001b[1;32mIn Progress\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mReporting Analyst\u001b[00m\n",
      "\u001b[95m## Thought:\u001b[00m \u001b[92mThought: I need to gather similar information for TensorFlow, PyTorch, and Keras to complete the detailed analysis required for each tool.\u001b[00m\n",
      "\u001b[95m## Using tool:\u001b[00m \u001b[92mSearch in a specific website\u001b[00m\n",
      "\u001b[95m## Tool Input:\u001b[00m \u001b[92m\n",
      "\"{\\\"search_query\\\": \\\"TensorFlow capabilities, limitations, community adoption, documentation quality\\\", \\\"website\\\": \\\"https://www.tensorflow.org/\\\"}\"\u001b[00m\n",
      "\u001b[95m## Tool Output:\u001b[00m \u001b[92m\n",
      "Relevant Content:\n",
      "announcements from the TensorFlow team and community. Explore the ecosystem Discover production-tested tools to accelerate modeling, deployment, and other workflows. Library TensorFlow.js Train and run models directly in the browser using JavaScript or Node.js. Library LiteRT Deploy ML on mobile and edge devices such as Android, iOS, Raspberry Pi, and Edge TPU. API tf.data Preprocess data and create input pipelines for ML models. Library TFX Create production ML pipelines and implement MLOps best practices. API tf.keras Create ML models with TensorFlow's high-level API. Resource Kaggle Models Find pre-trained models ready for fine-tuning and deployment. Resource TensorFlow Datasets Browse the collection of standard datasets for initial training and validation. Tool TensorBoard Visualize and track development of ML models. ML models & datasets Pretrained models and ready-to-use datasets for image, text, audio, and video use cases. Libraries & extensions Packages for domain-specific applications and APIs for languages other than Python. Developer tools Tools to evaluate models, optimize performance, and productionize ML workflows. Join the community Collaborate, find support, and share your projects by joining interest groups or attending developer events. Get involved Learn ML New to machine learning? Begin with TensorFlow's curated curriculums or browse the resource library of books, online courses, and videos. Explore resources Stay connected Learn the latest in machine learning and TensorFlow by following our channels or signing up for the newsletter. View past newsletters in the archive. Forum X YouTube Linkedin Forum X YouTube Linkedin Start building with TensorFlow Install TensorFlow Explore tutorials\n",
      "\n",
      "TensorFlow Skip to main content English Espaol Espaol  Amrica Latina Franais Indonesia Italiano Polski Portugus Portugus  Brasil Ting Vit Trke                GitHub Sign in TensorFlow Stay organized with collections Save and categorize content based on your preferences. An end-to-end platform for machine learning Install TensorFlow Get started with TensorFlow TensorFlow makes it easy to create ML models that can run in any environment. Learn how to use the intuitive APIs through interactive code samples. View tutorials import tensorflow as tf mnist = tf.keras.datasets.mnist (x_train, y_train),(x_test, y_test) = mnist.load_data() x_train, x_test = x_train / 255.0, x_test / 255.0 model = tf.keras.models.Sequential([ tf.keras.layers.Flatten(input_shape=(28, 28), tf.keras.layers.Dense(128, activation='relu'), tf.keras.layers.Dropout(0.2), tf.keras.layers.Dense(10, activation='softmax') ]) model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy']) model.fit(x_train, y_train, epochs=5) model.evaluate(x_test, y_test) Run quickstart Solve real-world problems with ML Explore examples of how TensorFlow is used to advance research and build AI-powered applications. TENSORFLOW.JS Catch up on the latest from the Web AI Summit Explore the latest advancements in running models client-side with speakers from Chrome, MediaPipe, Intel, Hugging Face, Microsoft, LangChain, and more. Watch now close TensorFlow GNN Analyze relational data using graph neural networks GNNs can process complex relationships between objects, making them a powerful technique for traffic forecasting, medical discovery, and more. Learn about TF GNN TensorFlow Agents Build recommendation systems with reinforcement learning Learn how Spotify uses the TensorFlow ecosystem to design an extendable offline simulator and train RL Agents to generate playlists. Read the blog What's new in TensorFlow Read the latest\n",
      "\n",
      "Stay up-to-date on the latest news and technical topics from the PyTorch Foundation. Read more PyTorch 2.6 Featuring Python 3.13 support for torch.compile, several AOTInductor enhancements, FP16 support on X86 CPUs, and more. Learn more Membership Available Become an integral part of the PyTorch Foundation, to build and shape the future of AI. Join Key Features &Capabilities See all Features Production Ready Transition seamlessly between eager and graph modes with TorchScript, and accelerate the path to production with TorchServe. Distributed Training Scalable distributed training and performance optimization in research and production is enabled by the torch.distributed backend. Robust Ecosystem A rich ecosystem of tools and libraries extends PyTorch and supports development in computer vision, NLP and more. Cloud Support PyTorch is well supported on major cloud platforms, providing frictionless development and easy scaling. Install PyTorch Select your preferences and run the install command. Stable represents the most currently tested and supported version of PyTorch. This should be suitable for many users. Preview is available if you want the latest, not fully tested and supported, builds that are generated nightly. Please ensure that you have met the prerequisites below (e.g., numpy), depending on your package manager. You can also install previous versions of PyTorch. Note that LibTorch is only available for C+. NOTE: Latest PyTorch requires Python 3.9 or later. PyTorch Build Your OS Package Language Compute Platform Run this Command: PyTorch Build Stable (1.13.0) Preview (Nightly) Your OS Linux Mac Windows Package Conda Pip LibTorch Source Language Python C+ / Java Compute Platform CUDA 11.8 CUDA 12.1 CUDA 12.4 ROCm 5.2 CPU Run this Command: conda install pytorch torchvision -c pytorch Previous versions of PyTorch Quick Start WithCloud Partners Get up and running with PyTorch quickly through popular cloud platforms and machine learning services. Amazon Web\n",
      "\n",
      "\n",
      "You ONLY have access to the following tools, and should NEVER make up tools that are not listed here:\n",
      "\n",
      "Tool Name: Search in a specific website\n",
      "Tool Arguments: {'search_query': {'description': 'Mandatory search query you want to use to search a specific website', 'type': 'str'}, 'website': {'description': 'Mandatory valid website URL you want to search on', 'type': 'str'}}\n",
      "Tool Description: A tool that can be used to semantic search a query from a specific URL content.\n",
      "\n",
      "IMPORTANT: Use the following format in your response:\n",
      "\n",
      "```\n",
      "Thought: you should always think about what to do\n",
      "Action: the action to take, only one name of [Search in a specific website], just the name, exactly as it's written.\n",
      "Action Input: the input to the action, just a simple JSON object, enclosed in curly braces, using \" to wrap keys and values.\n",
      "Observation: the result of the action\n",
      "```\n",
      "\n",
      "Once all necessary information is gathered, return the following format:\n",
      "\n",
      "```\n",
      "Thought: I now know the final answer\n",
      "Final Answer: the final answer to the original input question\n",
      "```\u001b[00m\n",
      "\u001b[93m Maximum iterations reached. Requesting final answer.\u001b[00m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> Agent: </span><span style=\"color: #008000; text-decoration-color: #008000\">Reporting Analyst</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">    Status: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">In Progress</span>\n",
       " <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\"> </span><span style=\"color: #000080; text-decoration-color: #000080\">Thinking...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32m Agent: \u001b[0m\u001b[32mReporting Analyst\u001b[0m\n",
       "\u001b[37m    Status: \u001b[0m\u001b[1;32mIn Progress\u001b[0m\n",
       " \u001b[1;34m \u001b[0m\u001b[34mThinking...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> Agent: </span><span style=\"color: #008000; text-decoration-color: #008000\">Reporting Analyst</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">    Status: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">In Progress</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32m Agent: \u001b[0m\u001b[32mReporting Analyst\u001b[0m\n",
       "\u001b[37m    Status: \u001b[0m\u001b[1;32mIn Progress\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mReporting Analyst\u001b[00m\n",
      "\u001b[95m## Final Answer:\u001b[00m \u001b[92m\n",
      "[\n",
      "    {\n",
      "        \"name\": \"H2O.ai\",\n",
      "        \"capabilities\": \"Combines predictive and generative AI, strong in document AI and automation for machine learning tasks.\",\n",
      "        \"limitations\": \"Limited control and flexibility; higher costs associated with extensive use in complex deployments.\",\n",
      "        \"community_adoption\": \"Growing user base, recognized for robust applications in enterprise AI.\",\n",
      "        \"documentation_quality\": \"Comprehensive and user-friendly documentation available.\",\n",
      "        \"suitability_for_RAG\": \"Moderate; can assist with document querying but struggles with complex conversational contexts.\",\n",
      "        \"compatibility\": {\n",
      "            \"LangChain\": \"Moderate compatibility.\",\n",
      "            \"PostgreSQL\": \"Low compatibility.\",\n",
      "            \"FastAPI\": \"Low compatibility.\"\n",
      "        },\n",
      "        \"pros\": [\n",
      "            \"User-friendly interface.\",\n",
      "            \"Fast model deployment capabilities.\",\n",
      "            \"Comprehensive support for different AI tasks.\"\n",
      "        ],\n",
      "        \"cons\": [\n",
      "            \"Less control over fine-tuning.\",\n",
      "            \"Higher operational costs.\",\n",
      "            \"May not cater to complex conversational AI needs.\"\n",
      "        ],\n",
      "        \"recommendation_score\": 6\n",
      "    },\n",
      "    {\n",
      "        \"name\": \"LangChain\",\n",
      "        \"capabilities\": \"Modular framework for building applications with LLMs, optimized for context-aware systems and integration with various data sources.\",\n",
      "        \"limitations\": \"Potentially resource-intensive and complex to set up initially.\",\n",
      "        \"community_adoption\": \"High adoption rate with active contributions; rapid growth.\",\n",
      "        \"documentation_quality\": \"Excellent documentation with many tutorials and examples.\",\n",
      "        \"suitability_for_RAG\": \"High; built specifically for conversational AI and context management.\",\n",
      "        \"compatibility\": {\n",
      "            \"LangChain\": \"N/A, being the framework itself.\",\n",
      "            \"PostgreSQL\": \"High compatibility.\",\n",
      "            \"FastAPI\": \"High compatibility.\"\n",
      "        },\n",
      "        \"pros\": [\n",
      "            \"Highly modular and flexible.\",\n",
      "            \"Strong community support.\",\n",
      "            \"Rich feature set for LLM applications.\"\n",
      "        ],\n",
      "        \"cons\": [\n",
      "            \"Learning curve for new users.\",\n",
      "            \"Initial complexity in setup.\"\n",
      "        ],\n",
      "        \"recommendation_score\": 9\n",
      "    },\n",
      "    {\n",
      "        \"name\": \"TensorFlow\",\n",
      "        \"capabilities\": \"Extensive library for building and training complex machine learning models, particularly effective for deep learning tasks.\",\n",
      "        \"limitations\": \"Steep learning curve for beginners and may have higher operational overhead.\",\n",
      "        \"community_adoption\": \"Very high adoption due to being supported by Google with a vast ecosystem.\",\n",
      "        \"documentation_quality\": \"High-quality documentation with diverse resources and tutorials.\",\n",
      "        \"suitability_for_RAG\": \"Excellent; well-suited for complex model training across various AI tasks.\",\n",
      "        \"compatibility\": {\n",
      "            \"LangChain\": \"Moderate compatibility.\",\n",
      "            \"PostgreSQL\": \"Low compatibility.\",\n",
      "            \"FastAPI\": \"Moderate compatibility.\"\n",
      "        },\n",
      "        \"pros\": [\n",
      "            \"Robust ecosystem and tools for deployment.\",\n",
      "            \"Active community and support.\",\n",
      "            \"High extensibility and tooling.\"\n",
      "        ],\n",
      "        \"cons\": [\n",
      "            \"Complexity can hinder beginners.\",\n",
      "            \"Requires significant computational resources.\"\n",
      "        ],\n",
      "        \"recommendation_score\": 9\n",
      "    },\n",
      "    {\n",
      "        \"name\": \"PyTorch\",\n",
      "        \"capabilities\": \"Flexible framework known for its dynamic computation graphs, ideal for research and prototyping in AI applications.\",\n",
      "        \"limitations\": \"Deployment complexity can be a barrier; updates can lead to API instability.\",\n",
      "        \"community_adoption\": \"Extensively adopted in the research community, growing in production as well.\",\n",
      "        \"documentation_quality\": \"Good documentation but can be lacking in comprehensive guides.\",\n",
      "        \"suitability_for_RAG\": \"High; excellent for experimentation and flexible model construction.\",\n",
      "        \"compatibility\": {\n",
      "            \"LangChain\": \"Moderate compatibility.\",\n",
      "            \"PostgreSQL\": \"Moderate compatibility.\",\n",
      "            \"FastAPI\": \"High compatibility.\"\n",
      "        },\n",
      "        \"pros\": [\n",
      "            \"Strong prototyping capabilities.\",\n",
      "            \"Flexibility in model building.\",\n",
      "            \"User-friendly API.\"\n",
      "        ],\n",
      "        \"cons\": [\n",
      "            \"Challenges in deployment.\",\n",
      "            \"Can have stability issues with updates.\"\n",
      "        ],\n",
      "        \"recommendation_score\": 8\n",
      "    },\n",
      "    {\n",
      "        \"name\": \"Keras\",\n",
      "        \"capabilities\": \"High-level API for building and training neural networks with a focus on simplicity and ease of use.\",\n",
      "        \"limitations\": \"Less flexible for complex architectures; dependent on TensorFlow for advanced functionalities.\",\n",
      "        \"community_adoption\": \"Widely used for educational purposes and quick prototyping.\",\n",
      "        \"documentation_quality\": \"Good documentation that is beginner-friendly.\",\n",
      "        \"suitability_for_RAG\": \"Moderate; effective for simple use cases but limited for complex conversational systems.\",\n",
      "        \"compatibility\": {\n",
      "            \"LangChain\": \"Moderate compatibility.\",\n",
      "            \"PostgreSQL\": \"Low compatibility.\",\n",
      "            \"FastAPI\": \"Moderate compatibility.\"\n",
      "        },\n",
      "        \"pros\": [\n",
      "            \"User-friendly for beginners.\",\n",
      "            \"Quick prototyping capabilities.\",\n",
      "            \"Access to TensorFlow's backend.\"\n",
      "        ],\n",
      "        \"cons\": [\n",
      "            \"Less flexible for more sophisticated architectures.\",\n",
      "            \"Not suitable for production scale without TensorFlow.\"\n",
      "        ],\n",
      "        \"recommendation_score\": 7\n",
      "    }\n",
      "]\u001b[00m\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages/instructor/process_response.py:222: DeprecationWarning: FUNCTIONS is deprecated and will be removed in future versions\n",
      "  if mode == Mode.FUNCTIONS:\n",
      "/Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages/instructor/function_calls.py:118: DeprecationWarning: FUNCTIONS is deprecated and will be removed in future versions\n",
      "  if mode == Mode.FUNCTIONS:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> Crew: crew</span>\n",
       " <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> Task: 05ce6d78-654b-4e32-bffe-42e4b0db2d93</span>\n",
       "   <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">   Assigned to: </span><span style=\"color: #008000; text-decoration-color: #008000\">Senior Data Researcher</span>\n",
       "   <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">   Status: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> Completed</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> Agent: </span><span style=\"color: #008000; text-decoration-color: #008000\">Senior Data Researcher</span>\n",
       "       <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">    Status: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> Completed</span>\n",
       " <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> Task: 364f262f-aa7f-4ef8-bf97-62ec371b1f19</span>\n",
       "    <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">   Status: </span><span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">Executing Task...</span>\n",
       "     <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> Agent: </span><span style=\"color: #008000; text-decoration-color: #008000\">Reporting Analyst</span>\n",
       "        <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">    Status: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> Completed</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m Crew: \u001b[0m\u001b[1;36mcrew\u001b[0m\n",
       " \u001b[1;32m Task: 05ce6d78-654b-4e32-bffe-42e4b0db2d93\u001b[0m\n",
       "   \u001b[37m   Assigned to: \u001b[0m\u001b[32mSenior Data Researcher\u001b[0m\n",
       "   \u001b[37m   Status: \u001b[0m\u001b[1;32m Completed\u001b[0m\n",
       "    \u001b[1;32m Agent: \u001b[0m\u001b[32mSenior Data Researcher\u001b[0m\n",
       "       \u001b[37m    Status: \u001b[0m\u001b[1;32m Completed\u001b[0m\n",
       " \u001b[1;33m Task: 364f262f-aa7f-4ef8-bf97-62ec371b1f19\u001b[0m\n",
       "    \u001b[37m   Status: \u001b[0m\u001b[2;33mExecuting Task...\u001b[0m\n",
       "     \u001b[1;32m Agent: \u001b[0m\u001b[32mReporting Analyst\u001b[0m\n",
       "        \u001b[37m    Status: \u001b[0m\u001b[1;32m Completed\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> Crew: crew</span>\n",
       " <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> Task: 05ce6d78-654b-4e32-bffe-42e4b0db2d93</span>\n",
       "   <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">   Assigned to: </span><span style=\"color: #008000; text-decoration-color: #008000\">Senior Data Researcher</span>\n",
       "   <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">   Status: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> Completed</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> Agent: </span><span style=\"color: #008000; text-decoration-color: #008000\">Senior Data Researcher</span>\n",
       "       <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">    Status: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> Completed</span>\n",
       " <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> Task: 364f262f-aa7f-4ef8-bf97-62ec371b1f19</span>\n",
       "    <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">   Assigned to: </span><span style=\"color: #008000; text-decoration-color: #008000\">Reporting Analyst</span>\n",
       "    <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">   Status: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> Completed</span>\n",
       "     <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> Agent: </span><span style=\"color: #008000; text-decoration-color: #008000\">Reporting Analyst</span>\n",
       "        <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">    Status: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> Completed</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m Crew: \u001b[0m\u001b[1;36mcrew\u001b[0m\n",
       " \u001b[1;32m Task: 05ce6d78-654b-4e32-bffe-42e4b0db2d93\u001b[0m\n",
       "   \u001b[37m   Assigned to: \u001b[0m\u001b[32mSenior Data Researcher\u001b[0m\n",
       "   \u001b[37m   Status: \u001b[0m\u001b[1;32m Completed\u001b[0m\n",
       "    \u001b[1;32m Agent: \u001b[0m\u001b[32mSenior Data Researcher\u001b[0m\n",
       "       \u001b[37m    Status: \u001b[0m\u001b[1;32m Completed\u001b[0m\n",
       " \u001b[1;32m Task: 364f262f-aa7f-4ef8-bf97-62ec371b1f19\u001b[0m\n",
       "    \u001b[37m   Assigned to: \u001b[0m\u001b[32mReporting Analyst\u001b[0m\n",
       "    \u001b[37m   Status: \u001b[0m\u001b[1;32m Completed\u001b[0m\n",
       "     \u001b[1;32m Agent: \u001b[0m\u001b[32mReporting Analyst\u001b[0m\n",
       "        \u001b[37m    Status: \u001b[0m\u001b[1;32m Completed\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\"> Task Completion </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\"></span>                                                                                                                 <span style=\"color: #008000; text-decoration-color: #008000\"></span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\"></span>  <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Task Completed</span>                                                                                                 <span style=\"color: #008000; text-decoration-color: #008000\"></span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\"></span>  <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">Name: </span><span style=\"color: #008000; text-decoration-color: #008000\">364f262f-aa7f-4ef8-bf97-62ec371b1f19</span>                                                                     <span style=\"color: #008000; text-decoration-color: #008000\"></span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\"></span>  <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">Agent: </span><span style=\"color: #008000; text-decoration-color: #008000\">Reporting Analyst</span>                                                                                       <span style=\"color: #008000; text-decoration-color: #008000\"></span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\"></span>                                                                                                                 <span style=\"color: #008000; text-decoration-color: #008000\"></span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\"></span>                                                                                                                 <span style=\"color: #008000; text-decoration-color: #008000\"></span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\"></span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32m\u001b[0m\u001b[32m\u001b[0m\u001b[32m Task Completion \u001b[0m\u001b[32m\u001b[0m\u001b[32m\u001b[0m\n",
       "\u001b[32m\u001b[0m                                                                                                                 \u001b[32m\u001b[0m\n",
       "\u001b[32m\u001b[0m  \u001b[1;32mTask Completed\u001b[0m                                                                                                 \u001b[32m\u001b[0m\n",
       "\u001b[32m\u001b[0m  \u001b[37mName: \u001b[0m\u001b[32m364f262f-aa7f-4ef8-bf97-62ec371b1f19\u001b[0m                                                                     \u001b[32m\u001b[0m\n",
       "\u001b[32m\u001b[0m  \u001b[37mAgent: \u001b[0m\u001b[32mReporting Analyst\u001b[0m                                                                                       \u001b[32m\u001b[0m\n",
       "\u001b[32m\u001b[0m                                                                                                                 \u001b[32m\u001b[0m\n",
       "\u001b[32m\u001b[0m                                                                                                                 \u001b[32m\u001b[0m\n",
       "\u001b[32m\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> Crew: crew</span>\n",
       " <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> Task: 05ce6d78-654b-4e32-bffe-42e4b0db2d93</span>\n",
       "   <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">   Assigned to: </span><span style=\"color: #008000; text-decoration-color: #008000\">Senior Data Researcher</span>\n",
       "   <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">   Status: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> Completed</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> Agent: </span><span style=\"color: #008000; text-decoration-color: #008000\">Senior Data Researcher</span>\n",
       "       <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">    Status: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> Completed</span>\n",
       " <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> Task: 364f262f-aa7f-4ef8-bf97-62ec371b1f19</span>\n",
       "   <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">   Assigned to: </span><span style=\"color: #008000; text-decoration-color: #008000\">Reporting Analyst</span>\n",
       "   <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">   Status: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> Completed</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> Agent: </span><span style=\"color: #008000; text-decoration-color: #008000\">Reporting Analyst</span>\n",
       "       <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">    Status: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> Completed</span>\n",
       " <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> Task: b4d94b91-9ada-42a9-b26c-88e9d8159b79</span>\n",
       "    <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">   Status: </span><span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">Executing Task...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m Crew: \u001b[0m\u001b[1;36mcrew\u001b[0m\n",
       " \u001b[1;32m Task: 05ce6d78-654b-4e32-bffe-42e4b0db2d93\u001b[0m\n",
       "   \u001b[37m   Assigned to: \u001b[0m\u001b[32mSenior Data Researcher\u001b[0m\n",
       "   \u001b[37m   Status: \u001b[0m\u001b[1;32m Completed\u001b[0m\n",
       "    \u001b[1;32m Agent: \u001b[0m\u001b[32mSenior Data Researcher\u001b[0m\n",
       "       \u001b[37m    Status: \u001b[0m\u001b[1;32m Completed\u001b[0m\n",
       " \u001b[1;32m Task: 364f262f-aa7f-4ef8-bf97-62ec371b1f19\u001b[0m\n",
       "   \u001b[37m   Assigned to: \u001b[0m\u001b[32mReporting Analyst\u001b[0m\n",
       "   \u001b[37m   Status: \u001b[0m\u001b[1;32m Completed\u001b[0m\n",
       "    \u001b[1;32m Agent: \u001b[0m\u001b[32mReporting Analyst\u001b[0m\n",
       "       \u001b[37m    Status: \u001b[0m\u001b[1;32m Completed\u001b[0m\n",
       " \u001b[1;33m Task: b4d94b91-9ada-42a9-b26c-88e9d8159b79\u001b[0m\n",
       "    \u001b[37m   Status: \u001b[0m\u001b[2;33mExecuting Task...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> Crew: crew</span>\n",
       " <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> Task: 05ce6d78-654b-4e32-bffe-42e4b0db2d93</span>\n",
       "   <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">   Assigned to: </span><span style=\"color: #008000; text-decoration-color: #008000\">Senior Data Researcher</span>\n",
       "   <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">   Status: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> Completed</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> Agent: </span><span style=\"color: #008000; text-decoration-color: #008000\">Senior Data Researcher</span>\n",
       "       <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">    Status: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> Completed</span>\n",
       " <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> Task: 364f262f-aa7f-4ef8-bf97-62ec371b1f19</span>\n",
       "   <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">   Assigned to: </span><span style=\"color: #008000; text-decoration-color: #008000\">Reporting Analyst</span>\n",
       "   <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">   Status: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> Completed</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> Agent: </span><span style=\"color: #008000; text-decoration-color: #008000\">Reporting Analyst</span>\n",
       "       <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">    Status: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> Completed</span>\n",
       " <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> Task: b4d94b91-9ada-42a9-b26c-88e9d8159b79</span>\n",
       "    <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">   Status: </span><span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">Executing Task...</span>\n",
       "     <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> Agent: </span><span style=\"color: #008000; text-decoration-color: #008000\">Reporting Analyst</span>\n",
       "        <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">    Status: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">In Progress</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m Crew: \u001b[0m\u001b[1;36mcrew\u001b[0m\n",
       " \u001b[1;32m Task: 05ce6d78-654b-4e32-bffe-42e4b0db2d93\u001b[0m\n",
       "   \u001b[37m   Assigned to: \u001b[0m\u001b[32mSenior Data Researcher\u001b[0m\n",
       "   \u001b[37m   Status: \u001b[0m\u001b[1;32m Completed\u001b[0m\n",
       "    \u001b[1;32m Agent: \u001b[0m\u001b[32mSenior Data Researcher\u001b[0m\n",
       "       \u001b[37m    Status: \u001b[0m\u001b[1;32m Completed\u001b[0m\n",
       " \u001b[1;32m Task: 364f262f-aa7f-4ef8-bf97-62ec371b1f19\u001b[0m\n",
       "   \u001b[37m   Assigned to: \u001b[0m\u001b[32mReporting Analyst\u001b[0m\n",
       "   \u001b[37m   Status: \u001b[0m\u001b[1;32m Completed\u001b[0m\n",
       "    \u001b[1;32m Agent: \u001b[0m\u001b[32mReporting Analyst\u001b[0m\n",
       "       \u001b[37m    Status: \u001b[0m\u001b[1;32m Completed\u001b[0m\n",
       " \u001b[1;33m Task: b4d94b91-9ada-42a9-b26c-88e9d8159b79\u001b[0m\n",
       "    \u001b[37m   Status: \u001b[0m\u001b[2;33mExecuting Task...\u001b[0m\n",
       "     \u001b[1;32m Agent: \u001b[0m\u001b[32mReporting Analyst\u001b[0m\n",
       "        \u001b[37m    Status: \u001b[0m\u001b[1;32mIn Progress\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mReporting Analyst\u001b[00m\n",
      "\u001b[95m## Task:\u001b[00m \u001b[92m\n",
      "Read the analysis from the previous task's result and create a comprehensive Markdown report.\n",
      "\n",
      "The report should include:\n",
      "\n",
      "1. An introduction explaining the task ('Develop a conversational RAG system that can answer questions based on a large PDF document collection') and existing stack ('LangChain, PogreSQL, FastAPI')\n",
      "2. For each tool, create a section with:\n",
      "   - Tool name and URL as a heading\n",
      "   - Description\n",
      "   - Features (as bullet points)\n",
      "   - Pros (as bullet points)\n",
      "   - Cons (as bullet points)\n",
      "   - Integration complexity\n",
      "   - Recommendation score with justification\n",
      "3. A summary/conclusion comparing the tools and providing final recommendations\n",
      "\n",
      "Use proper Markdown formatting with headings, bullet points, and emphasis where appropriate.\n",
      "Sort tools by recommendation score (descending).\n",
      "\n",
      "Your output should be a complete, well-formatted Markdown document.\n",
      "\u001b[00m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> Agent: </span><span style=\"color: #008000; text-decoration-color: #008000\">Reporting Analyst</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">    Status: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">In Progress</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32m Agent: \u001b[0m\u001b[32mReporting Analyst\u001b[0m\n",
       "\u001b[37m    Status: \u001b[0m\u001b[1;32mIn Progress\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mReporting Analyst\u001b[00m\n",
      "\u001b[95m## Using tool:\u001b[00m \u001b[92mSearch in a specific website\u001b[00m\n",
      "\u001b[95m## Tool Input:\u001b[00m \u001b[92m\n",
      "\"{\\\"search_query\\\": \\\"Develop a conversational RAG system that can answer questions based on a large PDF document collection\\\", \\\"website\\\": \\\"https://langchain.com/\\\"}\"\u001b[00m\n",
      "\u001b[95m## Tool Output:\u001b[00m \u001b[92m\n",
      "Relevant Content:\n",
      "H2O.ai | Convergence of The World's Best Predictive & Generative AI The World's Best Deep Research Chat Now Chat on Mobile Learn More Companies powered by H2O.ai H2Os flagship SLMs H2OVL Mississippi SVLM Series Our newest economical multimodal OCR model developed for Document AI H2OVL Mississippi-2B, based on H2O Danube2, is trained on 17.3M conversation pairs for high-res image handling. The .8B model, built on Danube3, leads OCR benchmarks with 19M pairs, outperforming all SLMs in text recognition. For Developers  1 Text Recognition H2O Danube SLM Series Our most economical small model for fast, lightweight tasks We trained H2O Danube3 models from scratch on ~100 H100 GPUs using our own curated dataset of 6T tokens. H2O Danube3-4B and .5B open-weight SLMs outperform the latest Apple OpenELM-3B and .5B instruct models. Perfect for developers who want to fine-tune their own SLMs for offline use cases. For Developers Get the Mobile App H2O Generative AI in use with customers H2O Document AI GPTs and Digital Agents in production worldwide In-Store Sales Assistant HR & IT Support Agent Fraud & Scam Detection RFP Assistant Procurement GPT Personalized Pricing Complaint Summarizer Distilled SLMs for Call Center Classification First-ever model risk management for Generative AI Generative AI is herebut can you audit it? H2O.ai brings Model Risk Management (MRM) to GenAI, combining enterprise-grade model evaluation with h2oGPTe and H2O Model Validation to make AI measurable, explainable, and compliant for banks and financial services. Automated testing, human-calibrated evaluations, and real-time risk monitoring ensure transparencyboosting compliance, risk management, and user confidence. H2O Model Validation Multi-modal Document AI with enterprise scale and security Convert Data to JSON Document AI ProcurementGPT Complaint Summarizer Convert structured data like financial tables to JSON Get answers from diagrams, flowcharts, web pages, audio files, video files and\n",
      "\n",
      "images. Intelligent document comparison with ProcurementGPT ProcurementGPT streamlines document analysis by automatically parsing documents into sections or clauses, then identifying the most similar counterparts across documentsworking symmetrically in both directions. It further breaks down clauses into key requirements, determining where each one is fulfilled and highlighting met and unmet requirements for easy review. The system leverages the top 10 most similar requirements to assess compliance, with an overlay for quick navigation. Users can also manually select sections for comparison, ensuring flexibility when automated parsing needs refinement. Learn More Summary of complaints and key issues Provides a high-level summary of the most common complaints, the top issues faced, and recommended actions to address these concerns. Learn More Built for air-gapped, on-premises, private or managed cloud deployments. Own your data, own your prompts. Only H2O.ai provides an end-to-end GenAI platform where you can own every part of the stack. H2O.ai offers a highly flexible solution, that can run fully managed in the cloud, or in hybrid or on-premise and air-gapped environments. Fully scalable with Kubernetes. H2O.ai achieves SOC2 Type 2 +HIPAA/HITECH Cost control and flexibility Customers can decide to mix and match from a large choice of over 30 LLMs: Proprietary LLMs: GPT-4o, o1, Gemini, Claude, Mistral, etc. Reliance on third-party provider No control, no ownership Token-based pricing, unbounded cost Open source LLMs: Llama3, DeepSeek, Qwen2.5, Mixtral, Mistral, H2O Danube, H2OVL Mississippi, any Fine-Tuned OSS model. Any model from Open-Source ecosystem Hosted on your own GPU infrastructure Full control and ownership of models Fixed hardware cost, most economical choice Cost savings on H2O.ai Platform. 25x cheaper on query cost. Serve more users. Better accuracy with up to 100% cheaper on Document Processing. h2oGPTe is designed to work with any LLM of your choice\n",
      "\n",
      "Visionary in Gartner Magic Quadrant: Data Science and Machine Learning 2024 Listed in Gartner Magic Quadrant (Cloud AI Developer Services) in 2020, 2021, 2022, 2023, and 2024 Listed in Gartner Magic Quadrant (Data Science and Machine Learning) in 2018, 2019, 2020, 2021, 2023, and 2024 Get the Report GenAI App Store As the maker of the worlds best AutoML, H2O.ai is converging Predictive AI with Generative AI, and helping leading global brands save billions of dollars a year with purpose-built AI for hundreds of use cases such as: Fraud and scam detection Complaint summarizer Recommendation engine KYC, loan application risk Pricing engines and more Try Sample GenAI Apps Built On h2oGPT Get your API key H2O Generative AI can solve a broad spectrum of natural language use cases, including classification, semantic search, paraphrasing, summarization, and content generation. h2oGPTe is used by companies to build production-ready, scalable and efficient AI-powered applications. Write custom apps with fully featured Python client APIs Seamlessly integrate state-of-the-art text, image, and code generation capabilities into your projects Get your API Key\u001b[00m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> Agent: </span><span style=\"color: #008000; text-decoration-color: #008000\">Reporting Analyst</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">    Status: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">In Progress</span>\n",
       " <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\"> </span><span style=\"color: #000080; text-decoration-color: #000080\">Thinking...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32m Agent: \u001b[0m\u001b[32mReporting Analyst\u001b[0m\n",
       "\u001b[37m    Status: \u001b[0m\u001b[1;32mIn Progress\u001b[0m\n",
       " \u001b[1;34m \u001b[0m\u001b[34mThinking...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> Agent: </span><span style=\"color: #008000; text-decoration-color: #008000\">Reporting Analyst</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">    Status: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">In Progress</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32m Agent: \u001b[0m\u001b[32mReporting Analyst\u001b[0m\n",
       "\u001b[37m    Status: \u001b[0m\u001b[1;32mIn Progress\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mReporting Analyst\u001b[00m\n",
      "\u001b[95m## Final Answer:\u001b[00m \u001b[92m\n",
      "# Conversational RAG System Analysis Report\n",
      "\n",
      "## Introduction\n",
      "The task focuses on developing a **Conversational Retrieval-Augmented Generation (RAG)** system capable of answering questions based on a large collection of PDF documents. The existing technology stack includes **LangChain**, **PostgreSQL**, and **FastAPI**. This report evaluates each of the tools in the context of this application.\n",
      "\n",
      "## Tools Analysis\n",
      "\n",
      "### 1. LangChain\n",
      "**[LangChain](https://langchain.com/)**\n",
      "- **Description**: A modular framework designed for building applications powered by language models (LLMs). LangChain enables seamless integration with various data sources, optimizing the creation of context-aware conversational systems.\n",
      "- **Features**:\n",
      "  - Modular architecture promoting flexibility.\n",
      "  - Easy integration with different APIs and data sources.\n",
      "  - Rich set of tools for LLMs.\n",
      "- **Pros**:\n",
      "  - Highly modular and flexible.\n",
      "  - Strong community support.\n",
      "  - Excellent documentation with extensive tutorials and examples.\n",
      "- **Cons**:\n",
      "  - Learning curve for new users.\n",
      "  - Initial complexity in setup.\n",
      "- **Integration Complexity**: Moderate complexity with high compatibility to PostgreSQL and FastAPI.\n",
      "- **Recommendation Score**: **9/10** - LangChain is particularly tailored for conversational AI applications, making it the optimal choice.\n",
      "\n",
      "### 2. TensorFlow\n",
      "**[TensorFlow](https://www.tensorflow.org/)**\n",
      "- **Description**: An open-source library developed by Google for building and training machine learning models. It is particularly suitable for deep learning tasks and offers extensive features for developing complex conversational AI systems.\n",
      "- **Features**:\n",
      "  - Extensive offerings for building and training models.\n",
      "  - Strong support for deep learning tasks.\n",
      "  - Robust production deployment tools.\n",
      "- **Pros**:\n",
      "  - Well-supported with a vast ecosystem.\n",
      "  - High extensibility and tooling available.\n",
      "  - Active community with numerous resources.\n",
      "- **Cons**:\n",
      "  - Steep learning curve for beginners.\n",
      "  - Requires significant computational resources.\n",
      "- **Integration Complexity**: Moderate compatibility with LangChain, PostgreSQL, and FastAPI.\n",
      "- **Recommendation Score**: **9/10** - Excellent suitability for complex model training in conversational contexts.\n",
      "\n",
      "### 3. PyTorch\n",
      "**[PyTorch](https://pytorch.org/)**\n",
      "- **Description**: A dynamic deep learning framework favored in research and prototyping. It's recognized for its flexibility and ease of use, making it ideal for developing innovative conversational AI technologies.\n",
      "- **Features**:\n",
      "  - Dynamic computation graphs.\n",
      "  - Active development and research support.\n",
      "  - Easy debugging and flexible model creation.\n",
      "- **Pros**:\n",
      "  - Strong prototyping capabilities.\n",
      "  - User-friendly API.\n",
      "  - Excellent for experimentation.\n",
      "- **Cons**:\n",
      "  - Deployment complexity can be a barrier.\n",
      "  - API stability concerns due to frequent updates.\n",
      "- **Integration Complexity**: Moderate compatibility with PostgreSQL and FastAPI, but offers high compatibility with LangChain.\n",
      "- **Recommendation Score**: **8/10** - Strong choice for prototyping and flexible model construction in conversational systems.\n",
      "\n",
      "### 4. Keras\n",
      "**[Keras](https://keras.io/)**\n",
      "- **Description**: A high-level API for building and training neural networks, focused on user-friendliness and rapid prototyping. It operates as a frontend for TensorFlow.\n",
      "- **Features**:\n",
      "  - Simplifies the process of building neural networks.\n",
      "  - Supports fast experimentation with user-friendly APIs.\n",
      "  - Compatible with TensorFlow backend functions.\n",
      "- **Pros**:\n",
      "  - Great for beginners due to its simplicity.\n",
      "  - Quick model prototyping capabilities.\n",
      "  - Strong integration with TensorFlow.\n",
      "- **Cons**:\n",
      "  - Limited flexibility for complex architectures.\n",
      "  - Less suitable for production-scale models without TensorFlow.\n",
      "- **Integration Complexity**: Moderate compatibility among the stack, but less robust than other tools.\n",
      "- **Recommendation Score**: **7/10** - Suitable for simple use cases but limited for complex conversational AI applications.\n",
      "\n",
      "### 5. H2O.ai\n",
      "**[H2O.ai](https://www.h2o.ai/)**\n",
      "- **Description**: A platform combining predictive and generative AI capabilities, valuable for various AI tasks including conversational AI, though with moderate suitability for complex conversations.\n",
      "- **Features**:\n",
      "  - User-friendly interface with strong model deployment.\n",
      "  - Focus on business applications and automation.\n",
      "  - Supports multiple AI operational tasks.\n",
      "- **Pros**:\n",
      "  - Fast model deployment capabilities.\n",
      "  - Comprehensive support for various AI tasks.\n",
      "  - User-friendly interface.\n",
      "- **Cons**:\n",
      "  - Higher operational costs.\n",
      "  - Limited control over model fine-tuning.\n",
      "  - Not ideally suited for complex conversational AI needs.\n",
      "- **Integration Complexity**: Low compatibility with PostgreSQL and FastAPI; moderate with LangChain.\n",
      "- **Recommendation Score**: **6/10** - While user-friendly and fast, it is less suitable for complex conversational contexts.\n",
      "\n",
      "## Conclusion\n",
      "In summary, among the evaluated tools, **LangChain** and **TensorFlow** stand out with the highest recommendation scores of 9, making them ideal for developing a conversational RAG system. **PyTorch** closely follows with a score of 8, offering excellent prototyping capabilities. **Keras** and **H2O.ai** can be utilized for simpler applications but may not meet the demands of complex interactions needed for a RAG system. \n",
      "\n",
      "Ultimately, for the implementation of a robust conversational RAG system based on a large PDF document collection, leveraging **LangChain** along with **TensorFlow** appears to provide the best balance of flexibility, community support, and technical proficiency.\u001b[00m\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages/instructor/process_response.py:222: DeprecationWarning: FUNCTIONS is deprecated and will be removed in future versions\n",
      "  if mode == Mode.FUNCTIONS:\n",
      "/Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages/instructor/function_calls.py:118: DeprecationWarning: FUNCTIONS is deprecated and will be removed in future versions\n",
      "  if mode == Mode.FUNCTIONS:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> Crew: crew</span>\n",
       " <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> Task: 05ce6d78-654b-4e32-bffe-42e4b0db2d93</span>\n",
       "   <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">   Assigned to: </span><span style=\"color: #008000; text-decoration-color: #008000\">Senior Data Researcher</span>\n",
       "   <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">   Status: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> Completed</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> Agent: </span><span style=\"color: #008000; text-decoration-color: #008000\">Senior Data Researcher</span>\n",
       "       <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">    Status: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> Completed</span>\n",
       " <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> Task: 364f262f-aa7f-4ef8-bf97-62ec371b1f19</span>\n",
       "   <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">   Assigned to: </span><span style=\"color: #008000; text-decoration-color: #008000\">Reporting Analyst</span>\n",
       "   <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">   Status: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> Completed</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> Agent: </span><span style=\"color: #008000; text-decoration-color: #008000\">Reporting Analyst</span>\n",
       "       <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">    Status: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> Completed</span>\n",
       " <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> Task: b4d94b91-9ada-42a9-b26c-88e9d8159b79</span>\n",
       "    <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">   Status: </span><span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">Executing Task...</span>\n",
       "     <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> Agent: </span><span style=\"color: #008000; text-decoration-color: #008000\">Reporting Analyst</span>\n",
       "        <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">    Status: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> Completed</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m Crew: \u001b[0m\u001b[1;36mcrew\u001b[0m\n",
       " \u001b[1;32m Task: 05ce6d78-654b-4e32-bffe-42e4b0db2d93\u001b[0m\n",
       "   \u001b[37m   Assigned to: \u001b[0m\u001b[32mSenior Data Researcher\u001b[0m\n",
       "   \u001b[37m   Status: \u001b[0m\u001b[1;32m Completed\u001b[0m\n",
       "    \u001b[1;32m Agent: \u001b[0m\u001b[32mSenior Data Researcher\u001b[0m\n",
       "       \u001b[37m    Status: \u001b[0m\u001b[1;32m Completed\u001b[0m\n",
       " \u001b[1;32m Task: 364f262f-aa7f-4ef8-bf97-62ec371b1f19\u001b[0m\n",
       "   \u001b[37m   Assigned to: \u001b[0m\u001b[32mReporting Analyst\u001b[0m\n",
       "   \u001b[37m   Status: \u001b[0m\u001b[1;32m Completed\u001b[0m\n",
       "    \u001b[1;32m Agent: \u001b[0m\u001b[32mReporting Analyst\u001b[0m\n",
       "       \u001b[37m    Status: \u001b[0m\u001b[1;32m Completed\u001b[0m\n",
       " \u001b[1;33m Task: b4d94b91-9ada-42a9-b26c-88e9d8159b79\u001b[0m\n",
       "    \u001b[37m   Status: \u001b[0m\u001b[2;33mExecuting Task...\u001b[0m\n",
       "     \u001b[1;32m Agent: \u001b[0m\u001b[32mReporting Analyst\u001b[0m\n",
       "        \u001b[37m    Status: \u001b[0m\u001b[1;32m Completed\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> Crew: crew</span>\n",
       " <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> Task: 05ce6d78-654b-4e32-bffe-42e4b0db2d93</span>\n",
       "   <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">   Assigned to: </span><span style=\"color: #008000; text-decoration-color: #008000\">Senior Data Researcher</span>\n",
       "   <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">   Status: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> Completed</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> Agent: </span><span style=\"color: #008000; text-decoration-color: #008000\">Senior Data Researcher</span>\n",
       "       <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">    Status: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> Completed</span>\n",
       " <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> Task: 364f262f-aa7f-4ef8-bf97-62ec371b1f19</span>\n",
       "   <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">   Assigned to: </span><span style=\"color: #008000; text-decoration-color: #008000\">Reporting Analyst</span>\n",
       "   <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">   Status: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> Completed</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> Agent: </span><span style=\"color: #008000; text-decoration-color: #008000\">Reporting Analyst</span>\n",
       "       <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">    Status: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> Completed</span>\n",
       " <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> Task: b4d94b91-9ada-42a9-b26c-88e9d8159b79</span>\n",
       "    <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">   Assigned to: </span><span style=\"color: #008000; text-decoration-color: #008000\">Reporting Analyst</span>\n",
       "    <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">   Status: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> Completed</span>\n",
       "     <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> Agent: </span><span style=\"color: #008000; text-decoration-color: #008000\">Reporting Analyst</span>\n",
       "        <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">    Status: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> Completed</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m Crew: \u001b[0m\u001b[1;36mcrew\u001b[0m\n",
       " \u001b[1;32m Task: 05ce6d78-654b-4e32-bffe-42e4b0db2d93\u001b[0m\n",
       "   \u001b[37m   Assigned to: \u001b[0m\u001b[32mSenior Data Researcher\u001b[0m\n",
       "   \u001b[37m   Status: \u001b[0m\u001b[1;32m Completed\u001b[0m\n",
       "    \u001b[1;32m Agent: \u001b[0m\u001b[32mSenior Data Researcher\u001b[0m\n",
       "       \u001b[37m    Status: \u001b[0m\u001b[1;32m Completed\u001b[0m\n",
       " \u001b[1;32m Task: 364f262f-aa7f-4ef8-bf97-62ec371b1f19\u001b[0m\n",
       "   \u001b[37m   Assigned to: \u001b[0m\u001b[32mReporting Analyst\u001b[0m\n",
       "   \u001b[37m   Status: \u001b[0m\u001b[1;32m Completed\u001b[0m\n",
       "    \u001b[1;32m Agent: \u001b[0m\u001b[32mReporting Analyst\u001b[0m\n",
       "       \u001b[37m    Status: \u001b[0m\u001b[1;32m Completed\u001b[0m\n",
       " \u001b[1;32m Task: b4d94b91-9ada-42a9-b26c-88e9d8159b79\u001b[0m\n",
       "    \u001b[37m   Assigned to: \u001b[0m\u001b[32mReporting Analyst\u001b[0m\n",
       "    \u001b[37m   Status: \u001b[0m\u001b[1;32m Completed\u001b[0m\n",
       "     \u001b[1;32m Agent: \u001b[0m\u001b[32mReporting Analyst\u001b[0m\n",
       "        \u001b[37m    Status: \u001b[0m\u001b[1;32m Completed\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\"> Task Completion </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\"></span>                                                                                                                 <span style=\"color: #008000; text-decoration-color: #008000\"></span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\"></span>  <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Task Completed</span>                                                                                                 <span style=\"color: #008000; text-decoration-color: #008000\"></span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\"></span>  <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">Name: </span><span style=\"color: #008000; text-decoration-color: #008000\">b4d94b91-9ada-42a9-b26c-88e9d8159b79</span>                                                                     <span style=\"color: #008000; text-decoration-color: #008000\"></span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\"></span>  <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">Agent: </span><span style=\"color: #008000; text-decoration-color: #008000\">Reporting Analyst</span>                                                                                       <span style=\"color: #008000; text-decoration-color: #008000\"></span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\"></span>                                                                                                                 <span style=\"color: #008000; text-decoration-color: #008000\"></span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\"></span>                                                                                                                 <span style=\"color: #008000; text-decoration-color: #008000\"></span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\"></span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32m\u001b[0m\u001b[32m\u001b[0m\u001b[32m Task Completion \u001b[0m\u001b[32m\u001b[0m\u001b[32m\u001b[0m\n",
       "\u001b[32m\u001b[0m                                                                                                                 \u001b[32m\u001b[0m\n",
       "\u001b[32m\u001b[0m  \u001b[1;32mTask Completed\u001b[0m                                                                                                 \u001b[32m\u001b[0m\n",
       "\u001b[32m\u001b[0m  \u001b[37mName: \u001b[0m\u001b[32mb4d94b91-9ada-42a9-b26c-88e9d8159b79\u001b[0m                                                                     \u001b[32m\u001b[0m\n",
       "\u001b[32m\u001b[0m  \u001b[37mAgent: \u001b[0m\u001b[32mReporting Analyst\u001b[0m                                                                                       \u001b[32m\u001b[0m\n",
       "\u001b[32m\u001b[0m                                                                                                                 \u001b[32m\u001b[0m\n",
       "\u001b[32m\u001b[0m                                                                                                                 \u001b[32m\u001b[0m\n",
       "\u001b[32m\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\"> Crew Completion </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\"></span>                                                                                                                 <span style=\"color: #008000; text-decoration-color: #008000\"></span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\"></span>  <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Crew Execution Completed</span>                                                                                       <span style=\"color: #008000; text-decoration-color: #008000\"></span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\"></span>  <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">Name: </span><span style=\"color: #008000; text-decoration-color: #008000\">crew</span>                                                                                                     <span style=\"color: #008000; text-decoration-color: #008000\"></span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\"></span>  <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">ID: </span><span style=\"color: #008000; text-decoration-color: #008000\">840727b7-e14c-4420-baea-712748a0939c</span>                                                                       <span style=\"color: #008000; text-decoration-color: #008000\"></span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\"></span>                                                                                                                 <span style=\"color: #008000; text-decoration-color: #008000\"></span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\"></span>                                                                                                                 <span style=\"color: #008000; text-decoration-color: #008000\"></span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\"></span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32m\u001b[0m\u001b[32m\u001b[0m\u001b[32m Crew Completion \u001b[0m\u001b[32m\u001b[0m\u001b[32m\u001b[0m\n",
       "\u001b[32m\u001b[0m                                                                                                                 \u001b[32m\u001b[0m\n",
       "\u001b[32m\u001b[0m  \u001b[1;32mCrew Execution Completed\u001b[0m                                                                                       \u001b[32m\u001b[0m\n",
       "\u001b[32m\u001b[0m  \u001b[37mName: \u001b[0m\u001b[32mcrew\u001b[0m                                                                                                     \u001b[32m\u001b[0m\n",
       "\u001b[32m\u001b[0m  \u001b[37mID: \u001b[0m\u001b[32m840727b7-e14c-4420-baea-712748a0939c\u001b[0m                                                                       \u001b[32m\u001b[0m\n",
       "\u001b[32m\u001b[0m                                                                                                                 \u001b[32m\u001b[0m\n",
       "\u001b[32m\u001b[0m                                                                                                                 \u001b[32m\u001b[0m\n",
       "\u001b[32m\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<div>\n",
       "  <style scoped>\n",
       "  button {\n",
       "    border: none;\n",
       "    border-radius: 4px;\n",
       "    background-color: rgb(34, 114, 180);\n",
       "    font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, \"Helvetica Neue\", Arial;\n",
       "    font-size: 13px;\n",
       "    color: white;\n",
       "    margin-top: 8px;\n",
       "    margin-bottom: 8px;\n",
       "    padding: 8px 16px;\n",
       "    cursor: pointer;\n",
       "  }\n",
       "  button:hover {\n",
       "    background-color: rgb(66, 153, 224);\n",
       "  }\n",
       "  </style>\n",
       "  <button\n",
       "    onclick=\"\n",
       "        const display = this.nextElementSibling.style.display;\n",
       "        const isCollapsed = display === 'none';\n",
       "        this.nextElementSibling.style.display = isCollapsed ? null : 'none';\n",
       "\n",
       "        const verb = isCollapsed ? 'Collapse' : 'Expand';\n",
       "        this.innerText = `${verb} MLflow Trace`;\n",
       "    \"\n",
       "  >Collapse MLflow Trace</button>\n",
       "  <iframe\n",
       "    id=\"trace-renderer\"\n",
       "    style=\"width: 100%; height: 500px; border: none; resize: vertical;\"\n",
       "    src=\"https://public-tracking.mlflow-e00rfd6s4w3cdtk9fx.backbone-e00rt16p71ew33dg9e.msp.eu-north1.nebius.cloud/static-files/lib/notebook-trace-renderer/index.html?trace_id=8cefc300d8594a5c91c2dfe07879a304&amp;experiment_id=6&amp;version=2.21.2\"\n",
       "  />\n",
       "</div>\n"
      ],
      "text/plain": [
       "Trace(request_id=8cefc300d8594a5c91c2dfe07879a304)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Turn on auto tracing by calling mlflow.crewai.autolog()\n",
    "mlflow.crewai.autolog()\n",
    "mlflow.set_experiment(\"Step 2 - Autolog\")\n",
    "\n",
    "ai_dev_crew = AIOpsResearchCrew(task_description, existing_stack)\n",
    "result = ai_dev_crew.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-17T05:49:44.965799Z",
     "iopub.status.busy": "2025-04-17T05:49:44.965416Z",
     "iopub.status.idle": "2025-04-17T05:49:45.006138Z",
     "shell.execute_reply": "2025-04-17T05:49:45.005505Z",
     "shell.execute_reply.started": "2025-04-17T05:49:44.965770Z"
    }
   },
   "source": [
    "### Observe \n",
    "\n",
    "- Enable auto-tracing for CrewAI in 1 line of code\n",
    "- Optionally create an MLflow experiment to write traces to"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Tracking metrics and artifacts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modify Crew Class for MLflow Run & Log Capture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-17T06:13:10.979216Z",
     "iopub.status.busy": "2025-04-17T06:13:10.978846Z",
     "iopub.status.idle": "2025-04-17T06:13:11.017434Z",
     "shell.execute_reply": "2025-04-17T06:13:11.016989Z",
     "shell.execute_reply.started": "2025-04-17T06:13:10.979185Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import logging\n",
    "import io\n",
    "import sys\n",
    "import contextlib\n",
    "\n",
    "\n",
    "class AIOpsResearchCrew:\n",
    "    def __init__(self, task, ai_stack):\n",
    "        \"\"\"\n",
    "        Initialize the crew with the task description and existing AI stack.\n",
    "        \n",
    "        Args:\n",
    "            task (str): Description of the task requiring AI tools\n",
    "            ai_stack (str): Comma-separated list of existing tools/frameworks used\n",
    "        \"\"\"\n",
    "        self.task = task\n",
    "        self.ai_stack = ai_stack\n",
    "        self.run_id = None\n",
    "        self.crew = None\n",
    "        \n",
    "        # Ensure output directory exists\n",
    "        os.makedirs(\"output\", exist_ok=True)\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"Execute the research, analysis, and reporting process with MLflow tracking.\"\"\"\n",
    "        \n",
    "        # --- UPDATE: Start MLflow run ---\n",
    "        with mlflow.start_run(run_name=f\"Tool_Research_{int(time.time())}\") as mlflow_run:\n",
    "            self.run_id = mlflow_run.info.run_id\n",
    "            \n",
    "\n",
    "            logger.info(\"Initializing agents...\")\n",
    "            agents = AIOpsResearchAgents()\n",
    "            researcher = agents.researcher_agent()\n",
    "            analyst = agents.analyst_agent()\n",
    "            \n",
    "            # Initialize tasks\n",
    "            tasks = AIOpsResearchTasks()\n",
    "            search_task = tasks.search_tools_task(researcher, self.task, self.ai_stack)\n",
    "            analyze_task = tasks.analyze_tools_task(analyst, self.task, self.ai_stack)\n",
    "            report_task = tasks.create_report_task(analyst, self.task, self.ai_stack)\n",
    "            \n",
    "            # Create the crew\n",
    "            self.crew = Crew(\n",
    "                agents=[researcher, analyst],\n",
    "                tasks=[search_task, analyze_task, report_task],\n",
    "                verbose=False,\n",
    "                process=Process.sequential,\n",
    "                memory=True\n",
    "            )\n",
    "            \n",
    "            # Start the crew\n",
    "            result = self.crew.kickoff()\n",
    "\n",
    "            \n",
    "            # --- UPDATE: Log parameters ---\n",
    "            mlflow.log_param(\"task\", self.task)\n",
    "            mlflow.log_param(\"ai_stack\", self.ai_stack)\n",
    "            \n",
    "            # --- UPDATE: Log metrics ---\n",
    "            mlflow.log_metrics(json.loads(self.crew.usage_metrics.json()))\n",
    "\n",
    "            # --- UPDATE: Log artifacts ---\n",
    "            artifact_files = [\n",
    "                \"output/tool_candidates.json\",\n",
    "                \"output/tool_analysis.json\",\n",
    "                \"output/tool_recommendation_report.md\",\n",
    "            ]\n",
    "            for file_path in artifact_files:\n",
    "                if os.path.exists(\"output/tool_candidates.json\"):\n",
    "                    mlflow.log_artifact(file_path)\n",
    "\n",
    "            # --- UPDATE: Set success tag ---\n",
    "            if os.path.exists(\"output/tool_recommendation_report.md\"):\n",
    "                mlflow.set_tag(\"status\", \"SUCCESS\")\n",
    "            else:\n",
    "                mlflow.set_tag(\"status\", \"FAILED\")\n",
    "            \n",
    "            return result\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T20:51:48.263584Z",
     "iopub.status.busy": "2025-03-29T20:51:48.263097Z",
     "iopub.status.idle": "2025-03-29T20:51:48.311052Z",
     "shell.execute_reply": "2025-03-29T20:51:48.309975Z",
     "shell.execute_reply.started": "2025-03-29T20:51:48.263555Z"
    }
   },
   "source": [
    "### Run and Observe in MLflow UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-17T06:13:11.821517Z",
     "iopub.status.busy": "2025-04-17T06:13:11.821222Z",
     "iopub.status.idle": "2025-04-17T06:15:01.864624Z",
     "shell.execute_reply": "2025-04-17T06:15:01.864092Z",
     "shell.execute_reply.started": "2025-04-17T06:13:11.821495Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mSenior Data Researcher\u001b[00m\n",
      "\u001b[95m## Task:\u001b[00m \u001b[92m\n",
      "You are a research agent tasked with finding AI tools for: {task}.\n",
      "Consider compatibility with: {ai_stack}.\n",
      "\n",
      "For each tool, identify:\n",
      "- Name and URL\n",
      "- Primary use case\n",
      "- Brief description (2-3 sentences)\n",
      "\n",
      "Format your response as a JSON list of objects.\n",
      "\u001b[00m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inserting batches in chromadb: 100%|| 1/1 [00:00<00:00,  1.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mSenior Data Researcher\u001b[00m\n",
      "\u001b[95m## Thought:\u001b[00m \u001b[92mI need to gather information about relevant AI tools for developing a conversational RAG (Retrieval-Augmented Generation) system while considering specific criteria. I will search for tools on the internet that match the use case and provide concise descriptions.\u001b[00m\n",
      "\u001b[95m## Using tool:\u001b[00m \u001b[92mSearch in a specific website\u001b[00m\n",
      "\u001b[95m## Tool Input:\u001b[00m \u001b[92m\n",
      "\"{\\\"search_query\\\": \\\"tools for conversational RAG system\\\", \\\"website\\\": \\\"https://www.h2o.ai\\\"}\"\u001b[00m\n",
      "\u001b[95m## Tool Output:\u001b[00m \u001b[92m\n",
      "Relevant Content:\n",
      "LangChain Sign up to be the first to access recordings from Interrupt, The AI Agent Conference!Learn MoreFrom startups to global enterprises, ambitious builders choose LangChain products.BuildLangChain is a composable framework to build with LLMs. LangGraph is the orchestration framework for controllable agentic workflows.RunDeploy your LLM applications at scale with LangGraph Platform, our infrastructure purpose-built for agents.ManageLangSmith is a unified agent observability and evals platform to optimize the performance of your AI agents - whether they're built with a LangChain framework or not. Build your app with LangChainBuild context-aware, reasoning applications with LangChains flexible framework that leverages your companys data and APIs. Future-proof your application by making vendor optionality part of your LLM infrastructure design.Learn more about LangChainRun at scale with LangGraph PlatformUse LangGraph Platforms APIs to design agent-driven user experiences featuring human-in-the-loop, multi-agent collaboration, conversation history, long-term memory, and time-travel. Deploy with fault-tolerant scalability.Learn more about LangGraph PlatformManage agent observability & performance with LangSmithGet visibility into your LLM calls and agent reasoning with LangSmith to quickly debug failures. Run evals to assess your agent performance over production traffic  so you can ensure high-quality responses. Learn more about LangSmithHear from our happy customersLangChain, LangGraph, and LangSmith help teams of all sizes, across all industries - from ambitious startups to established enterprises.By combining the benefits of LangSmith and standing on the shoulders of a gigantic open-source community, were able to identify the right approaches of using LLMs in an enterprise-setting faster.Yusuke KajiGeneral Manager of AIWorking with LangChain and LangSmith on the Elastic AI Assistant had a significant positive impact on the overall pace and quality of the\n",
      "\n",
      "LangChain Sign up to be the first to access recordings from Interrupt, The AI Agent Conference!Learn MoreFrom startups to global enterprises, ambitious builders choose LangChain products.BuildLangChain is a composable framework to build with LLMs. LangGraph is the orchestration framework for controllable agentic workflows.RunDeploy your LLM applications at scale with LangGraph Platform, our infrastructure purpose-built for agents.ManageLangSmith is a unified agent observability and evals platform to optimize the performance of your AI agents - whether they're built with a LangChain framework or not. Build your app with LangChainBuild context-aware, reasoning applications with LangChains flexible framework that leverages your companys data and APIs. Future-proof your application by making vendor optionality part of your LLM infrastructure design.Learn more about LangChainRun at scale with LangGraph PlatformUse LangGraph Platforms APIs to design agent-driven user experiences featuring human-in-the-loop, multi-agent collaboration, conversation history, long-term memory, and time-travel. Deploy with fault-tolerant scalability.Learn more about LangGraph PlatformManage agent observability & performance with LangSmithGet visibility into your LLM calls and agent reasoning with LangSmith to quickly debug failures. Run evals to assess your agent performance over production traffic  so you can ensure high-quality responses. Learn more about LangSmithHear from our happy customersLangChain, LangGraph, and LangSmith help teams of all sizes, across all industries - from ambitious startups to established enterprises.By combining the benefits of LangSmith and standing on the shoulders of a gigantic open-source community, were able to identify the right approaches of using LLMs in an enterprise-setting faster.Yusuke KajiGeneral Manager of AIWorking with LangChain and LangSmith on the Elastic AI Assistant had a significant positive impact on the overall pace and quality of the\n",
      "\n",
      "H2O.ai | Convergence of The World's Best Predictive & Generative AI The World's Best Deep Research Chat Now Chat on Mobile Learn More Companies powered by H2O.ai H2Os flagship SLMs H2OVL Mississippi SVLM Series Our newest economical multimodal OCR model developed for Document AI H2OVL Mississippi-2B, based on H2O Danube2, is trained on 17.3M conversation pairs for high-res image handling. The .8B model, built on Danube3, leads OCR benchmarks with 19M pairs, outperforming all SLMs in text recognition. For Developers  1 Text Recognition H2O Danube SLM Series Our most economical small model for fast, lightweight tasks We trained H2O Danube3 models from scratch on ~100 H100 GPUs using our own curated dataset of 6T tokens. H2O Danube3-4B and .5B open-weight SLMs outperform the latest Apple OpenELM-3B and .5B instruct models. Perfect for developers who want to fine-tune their own SLMs for offline use cases. For Developers Get the Mobile App H2O Generative AI in use with customers H2O Document AI GPTs and Digital Agents in production worldwide In-Store Sales Assistant HR & IT Support Agent Fraud & Scam Detection RFP Assistant Procurement GPT Personalized Pricing Complaint Summarizer Distilled SLMs for Call Center Classification First-ever model risk management for Generative AI Generative AI is herebut can you audit it? H2O.ai brings Model Risk Management (MRM) to GenAI, combining enterprise-grade model evaluation with h2oGPTe and H2O Model Validation to make AI measurable, explainable, and compliant for banks and financial services. Automated testing, human-calibrated evaluations, and real-time risk monitoring ensure transparencyboosting compliance, risk management, and user confidence. H2O Model Validation Multi-modal Document AI with enterprise scale and security Convert Data to JSON Document AI ProcurementGPT Complaint Summarizer Convert structured data like financial tables to JSON Get answers from diagrams, flowcharts, web pages, audio files, video files and\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mSenior Data Researcher\u001b[00m\n",
      "\u001b[95m## Final Answer:\u001b[00m \u001b[92m\n",
      "[\n",
      "    {\n",
      "        \"name\": \"H2O.ai\",\n",
      "        \"url\": \"https://www.h2o.ai/\",\n",
      "        \"primary_use_case\": \"Automated Machine Learning and Generative AI\",\n",
      "        \"description\": \"H2O.ai combines predictive and generative AI capabilities, providing tools for various tasks including document AI and recommendation systems. Its strengths in handling natural language processing make it favorable for developing conversational systems.\"\n",
      "    },\n",
      "    {\n",
      "        \"name\": \"LangChain\",\n",
      "        \"url\": \"https://langchain.com/\",\n",
      "        \"primary_use_case\": \"Framework for LLM applications\",\n",
      "        \"description\": \"LangChain is a versatile framework designed for constructing applications powered by language models (LLMs), enabling context-aware interactions and integration with data sources and APIs. It is particularly suited for building complex conversational systems.\"\n",
      "    },\n",
      "    {\n",
      "        \"name\": \"TensorFlow\",\n",
      "        \"url\": \"https://www.tensorflow.org/\",\n",
      "        \"primary_use_case\": \"Deep Learning Framework\",\n",
      "        \"description\": \"TensorFlow, developed by Google, is an open-source library for building and training machine learning models, particularly those involving deep learning. Its extensive capabilities lend themselves well to complex conversational AI applications.\"\n",
      "    },\n",
      "    {\n",
      "        \"name\": \"PyTorch\",\n",
      "        \"url\": \"https://pytorch.org/\",\n",
      "        \"primary_use_case\": \"Flexible Deep Learning Framework\",\n",
      "        \"description\": \"PyTorch is a popular deep learning framework recognized for its flexibility and user-friendly design. Developed by Facebook, it excels in research and prototyping, making it an excellent tool for developing innovative conversational technologies.\"\n",
      "    },\n",
      "    {\n",
      "        \"name\": \"Keras\",\n",
      "        \"url\": \"https://keras.io/\",\n",
      "        \"primary_use_case\": \"High-level Neural Network API\",\n",
      "        \"description\": \"Keras is a high-level API for building and training deep learning models, often used with TensorFlow. It provides a simplified interface for rapid experimentation and model development in conversational AI.\"\n",
      "    }\n",
      "]\u001b[00m\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages/instructor/process_response.py:222: DeprecationWarning: FUNCTIONS is deprecated and will be removed in future versions\n",
      "  if mode == Mode.FUNCTIONS:\n",
      "/Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages/instructor/function_calls.py:118: DeprecationWarning: FUNCTIONS is deprecated and will be removed in future versions\n",
      "  if mode == Mode.FUNCTIONS:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mReporting Analyst\u001b[00m\n",
      "\u001b[95m## Task:\u001b[00m \u001b[92m\n",
      "Read the tool list from the previous task's result and perform a detailed analysis of each tool.\n",
      "\n",
      "For each tool:\n",
      "1. Research its capabilities, limitations, community adoption, documentation quality\n",
      "2. Evaluate how well it addresses the specified task: 'Develop a conversational RAG system that can answer questions based on a large PDF document collection'\n",
      "3. Consider its compatibility with 'LangChain, PogreSQL, FastAPI'\n",
      "4. Identify pros and cons\n",
      "\n",
      "Your output should be a JSON list of these detailed analysis objects.\n",
      "\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mReporting Analyst\u001b[00m\n",
      "\u001b[95m## Using tool:\u001b[00m \u001b[92mSearch in a specific website\u001b[00m\n",
      "\u001b[95m## Tool Input:\u001b[00m \u001b[92m\n",
      "\"{\\\"search_query\\\": \\\"H2O.ai capabilities limitations community adoption documentation quality\\\", \\\"website\\\": \\\"https://www.h2o.ai/\\\"}\"\u001b[00m\n",
      "\u001b[95m## Tool Output:\u001b[00m \u001b[92m\n",
      "Relevant Content:\n",
      "H2O.ai | Convergence of The World's Best Predictive & Generative AI The World's Best Deep Research Chat Now Chat on Mobile Learn More Companies powered by H2O.ai H2Os flagship SLMs H2OVL Mississippi SVLM Series Our newest economical multimodal OCR model developed for Document AI H2OVL Mississippi-2B, based on H2O Danube2, is trained on 17.3M conversation pairs for high-res image handling. The .8B model, built on Danube3, leads OCR benchmarks with 19M pairs, outperforming all SLMs in text recognition. For Developers  1 Text Recognition H2O Danube SLM Series Our most economical small model for fast, lightweight tasks We trained H2O Danube3 models from scratch on ~100 H100 GPUs using our own curated dataset of 6T tokens. H2O Danube3-4B and .5B open-weight SLMs outperform the latest Apple OpenELM-3B and .5B instruct models. Perfect for developers who want to fine-tune their own SLMs for offline use cases. For Developers Get the Mobile App H2O Generative AI in use with customers H2O Document AI GPTs and Digital Agents in production worldwide In-Store Sales Assistant HR & IT Support Agent Fraud & Scam Detection RFP Assistant Procurement GPT Personalized Pricing Complaint Summarizer Distilled SLMs for Call Center Classification First-ever model risk management for Generative AI Generative AI is herebut can you audit it? H2O.ai brings Model Risk Management (MRM) to GenAI, combining enterprise-grade model evaluation with h2oGPTe and H2O Model Validation to make AI measurable, explainable, and compliant for banks and financial services. Automated testing, human-calibrated evaluations, and real-time risk monitoring ensure transparencyboosting compliance, risk management, and user confidence. H2O Model Validation Multi-modal Document AI with enterprise scale and security Convert Data to JSON Document AI ProcurementGPT Complaint Summarizer Convert structured data like financial tables to JSON Get answers from diagrams, flowcharts, web pages, audio files, video files and\n",
      "\n",
      "H2O.ai | Convergence of The World's Best Predictive & Generative AI The World's Best Deep Research Chat Now Chat on Mobile Learn More Companies powered by H2O.ai H2Os flagship SLMs H2OVL Mississippi SVLM Series Our newest economical multimodal OCR model developed for Document AI H2OVL Mississippi-2B, based on H2O Danube2, is trained on 17.3M conversation pairs for high-res image handling. The .8B model, built on Danube3, leads OCR benchmarks with 19M pairs, outperforming all SLMs in text recognition. For Developers  1 Text Recognition H2O Danube SLM Series Our most economical small model for fast, lightweight tasks We trained H2O Danube3 models from scratch on ~100 H100 GPUs using our own curated dataset of 6T tokens. H2O Danube3-4B and .5B open-weight SLMs outperform the latest Apple OpenELM-3B and .5B instruct models. Perfect for developers who want to fine-tune their own SLMs for offline use cases. For Developers Get the Mobile App H2O Generative AI in use with customers H2O Document AI GPTs and Digital Agents in production worldwide In-Store Sales Assistant HR & IT Support Agent Fraud & Scam Detection RFP Assistant Procurement GPT Personalized Pricing Complaint Summarizer Distilled SLMs for Call Center Classification First-ever model risk management for Generative AI Generative AI is herebut can you audit it? H2O.ai brings Model Risk Management (MRM) to GenAI, combining enterprise-grade model evaluation with h2oGPTe and H2O Model Validation to make AI measurable, explainable, and compliant for banks and financial services. Automated testing, human-calibrated evaluations, and real-time risk monitoring ensure transparencyboosting compliance, risk management, and user confidence. H2O Model Validation Multi-modal Document AI with enterprise scale and security Convert Data to JSON Document AI ProcurementGPT Complaint Summarizer Convert structured data like financial tables to JSON Get answers from diagrams, flowcharts, web pages, audio files, video files and\n",
      "\n",
      "Visionary in Gartner Magic Quadrant: Data Science and Machine Learning 2024 Listed in Gartner Magic Quadrant (Cloud AI Developer Services) in 2020, 2021, 2022, 2023, and 2024 Listed in Gartner Magic Quadrant (Data Science and Machine Learning) in 2018, 2019, 2020, 2021, 2023, and 2024 Get the Report GenAI App Store As the maker of the worlds best AutoML, H2O.ai is converging Predictive AI with Generative AI, and helping leading global brands save billions of dollars a year with purpose-built AI for hundreds of use cases such as: Fraud and scam detection Complaint summarizer Recommendation engine KYC, loan application risk Pricing engines and more Try Sample GenAI Apps Built On h2oGPT Get your API key H2O Generative AI can solve a broad spectrum of natural language use cases, including classification, semantic search, paraphrasing, summarization, and content generation. h2oGPTe is used by companies to build production-ready, scalable and efficient AI-powered applications. Write custom apps with fully featured Python client APIs Seamlessly integrate state-of-the-art text, image, and code generation capabilities into your projects Get your API Key\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mReporting Analyst\u001b[00m\n",
      "\u001b[95m## Using tool:\u001b[00m \u001b[92mSearch in a specific website\u001b[00m\n",
      "\u001b[95m## Tool Input:\u001b[00m \u001b[92m\n",
      "\"{\\\"search_query\\\": \\\"LangChain capabilities limitations community adoption documentation quality\\\", \\\"website\\\": \\\"https://langchain.com/\\\"}\"\u001b[00m\n",
      "\u001b[95m## Tool Output:\u001b[00m \u001b[92m\n",
      "Relevant Content:\n",
      "development and shipping experience. We couldnt have achieved the product experience delivered to our customers without LangChain, and we couldnt have done it at the same pace without LangSmith.James SpiteriDirector of Security ProductsAs soon as we heard about LangSmith, we moved our entire development stack onto it. We could have built evaluation, testing and monitoring tools in house, but with LangSmith it took us 10x less time to get a 1000x better tool.Jose PeaSenior Manager The reference architecture enterprises adopt for success.LangChains suite of products can be used independently or stacked together for multiplicative impact  guiding you through building, running, and managing your LLM apps.20M+Monthly Downloads100K+Apps Powered100K+GitHub Stars4K+ContributorsThe biggest developer community in GenAILearn alongside the 1M+ developers who are pushing the industry forward.Explore LangChainGet started with LangChain's suite of products.Get a demoSign up for freeReady to start shipping reliable GenAI apps faster?Get started with LangChain, LangSmith, and LangGraph to enhance your LLM app development, from prototype to production.Get a demoSign up for free\n",
      "\n",
      "development and shipping experience. We couldnt have achieved the product experience delivered to our customers without LangChain, and we couldnt have done it at the same pace without LangSmith.James SpiteriDirector of Security ProductsAs soon as we heard about LangSmith, we moved our entire development stack onto it. We could have built evaluation, testing and monitoring tools in house, but with LangSmith it took us 10x less time to get a 1000x better tool.Jose PeaSenior Manager The reference architecture enterprises adopt for success.LangChains suite of products can be used independently or stacked together for multiplicative impact  guiding you through building, running, and managing your LLM apps.20M+Monthly Downloads100K+Apps Powered100K+GitHub Stars4K+ContributorsThe biggest developer community in GenAILearn alongside the 1M+ developers who are pushing the industry forward.Explore LangChainGet started with LangChain's suite of products.Get a demoSign up for freeReady to start shipping reliable GenAI apps faster?Get started with LangChain, LangSmith, and LangGraph to enhance your LLM app development, from prototype to production.Get a demoSign up for free\n",
      "\n",
      "LangChain Sign up to be the first to access recordings from Interrupt, The AI Agent Conference!Learn MoreFrom startups to global enterprises, ambitious builders choose LangChain products.BuildLangChain is a composable framework to build with LLMs. LangGraph is the orchestration framework for controllable agentic workflows.RunDeploy your LLM applications at scale with LangGraph Platform, our infrastructure purpose-built for agents.ManageLangSmith is a unified agent observability and evals platform to optimize the performance of your AI agents - whether they're built with a LangChain framework or not. Build your app with LangChainBuild context-aware, reasoning applications with LangChains flexible framework that leverages your companys data and APIs. Future-proof your application by making vendor optionality part of your LLM infrastructure design.Learn more about LangChainRun at scale with LangGraph PlatformUse LangGraph Platforms APIs to design agent-driven user experiences featuring human-in-the-loop, multi-agent collaboration, conversation history, long-term memory, and time-travel. Deploy with fault-tolerant scalability.Learn more about LangGraph PlatformManage agent observability & performance with LangSmithGet visibility into your LLM calls and agent reasoning with LangSmith to quickly debug failures. Run evals to assess your agent performance over production traffic  so you can ensure high-quality responses. Learn more about LangSmithHear from our happy customersLangChain, LangGraph, and LangSmith help teams of all sizes, across all industries - from ambitious startups to established enterprises.By combining the benefits of LangSmith and standing on the shoulders of a gigantic open-source community, were able to identify the right approaches of using LLMs in an enterprise-setting faster.Yusuke KajiGeneral Manager of AIWorking with LangChain and LangSmith on the Elastic AI Assistant had a significant positive impact on the overall pace and quality of the\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mReporting Analyst\u001b[00m\n",
      "\u001b[95m## Using tool:\u001b[00m \u001b[92mSearch in a specific website\u001b[00m\n",
      "\u001b[95m## Tool Input:\u001b[00m \u001b[92m\n",
      "\"{\\\"search_query\\\": \\\"TensorFlow capabilities limitations community adoption documentation quality\\\", \\\"website\\\": \\\"https://www.tensorflow.org/\\\"}\"\u001b[00m\n",
      "\u001b[95m## Tool Output:\u001b[00m \u001b[92m\n",
      "Relevant Content:\n",
      "announcements from the TensorFlow team and community. Explore the ecosystem Discover production-tested tools to accelerate modeling, deployment, and other workflows. Library TensorFlow.js Train and run models directly in the browser using JavaScript or Node.js. Library LiteRT Deploy ML on mobile and edge devices such as Android, iOS, Raspberry Pi, and Edge TPU. API tf.data Preprocess data and create input pipelines for ML models. Library TFX Create production ML pipelines and implement MLOps best practices. API tf.keras Create ML models with TensorFlow's high-level API. Resource Kaggle Models Find pre-trained models ready for fine-tuning and deployment. Resource TensorFlow Datasets Browse the collection of standard datasets for initial training and validation. Tool TensorBoard Visualize and track development of ML models. ML models & datasets Pretrained models and ready-to-use datasets for image, text, audio, and video use cases. Libraries & extensions Packages for domain-specific applications and APIs for languages other than Python. Developer tools Tools to evaluate models, optimize performance, and productionize ML workflows. Join the community Collaborate, find support, and share your projects by joining interest groups or attending developer events. Get involved Learn ML New to machine learning? Begin with TensorFlow's curated curriculums or browse the resource library of books, online courses, and videos. Explore resources Stay connected Learn the latest in machine learning and TensorFlow by following our channels or signing up for the newsletter. View past newsletters in the archive. Forum X YouTube Linkedin Forum X YouTube Linkedin Start building with TensorFlow Install TensorFlow Explore tutorials\n",
      "\n",
      "TensorFlow Skip to main content English Espaol Espaol  Amrica Latina Franais Indonesia Italiano Polski Portugus Portugus  Brasil Ting Vit Trke                GitHub Sign in TensorFlow Stay organized with collections Save and categorize content based on your preferences. An end-to-end platform for machine learning Install TensorFlow Get started with TensorFlow TensorFlow makes it easy to create ML models that can run in any environment. Learn how to use the intuitive APIs through interactive code samples. View tutorials import tensorflow as tf mnist = tf.keras.datasets.mnist (x_train, y_train),(x_test, y_test) = mnist.load_data() x_train, x_test = x_train / 255.0, x_test / 255.0 model = tf.keras.models.Sequential([ tf.keras.layers.Flatten(input_shape=(28, 28), tf.keras.layers.Dense(128, activation='relu'), tf.keras.layers.Dropout(0.2), tf.keras.layers.Dense(10, activation='softmax') ]) model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy']) model.fit(x_train, y_train, epochs=5) model.evaluate(x_test, y_test) Run quickstart Solve real-world problems with ML Explore examples of how TensorFlow is used to advance research and build AI-powered applications. TENSORFLOW.JS Catch up on the latest from the Web AI Summit Explore the latest advancements in running models client-side with speakers from Chrome, MediaPipe, Intel, Hugging Face, Microsoft, LangChain, and more. Watch now close TensorFlow GNN Analyze relational data using graph neural networks GNNs can process complex relationships between objects, making them a powerful technique for traffic forecasting, medical discovery, and more. Learn about TF GNN TensorFlow Agents Build recommendation systems with reinforcement learning Learn how Spotify uses the TensorFlow ecosystem to design an extendable offline simulator and train RL Agents to generate playlists. Read the blog What's new in TensorFlow Read the latest\n",
      "\n",
      "Stay up-to-date on the latest news and technical topics from the PyTorch Foundation. Read more PyTorch 2.6 Featuring Python 3.13 support for torch.compile, several AOTInductor enhancements, FP16 support on X86 CPUs, and more. Learn more Membership Available Become an integral part of the PyTorch Foundation, to build and shape the future of AI. Join Key Features &Capabilities See all Features Production Ready Transition seamlessly between eager and graph modes with TorchScript, and accelerate the path to production with TorchServe. Distributed Training Scalable distributed training and performance optimization in research and production is enabled by the torch.distributed backend. Robust Ecosystem A rich ecosystem of tools and libraries extends PyTorch and supports development in computer vision, NLP and more. Cloud Support PyTorch is well supported on major cloud platforms, providing frictionless development and easy scaling. Install PyTorch Select your preferences and run the install command. Stable represents the most currently tested and supported version of PyTorch. This should be suitable for many users. Preview is available if you want the latest, not fully tested and supported, builds that are generated nightly. Please ensure that you have met the prerequisites below (e.g., numpy), depending on your package manager. You can also install previous versions of PyTorch. Note that LibTorch is only available for C+. NOTE: Latest PyTorch requires Python 3.9 or later. PyTorch Build Your OS Package Language Compute Platform Run this Command: PyTorch Build Stable (1.13.0) Preview (Nightly) Your OS Linux Mac Windows Package Conda Pip LibTorch Source Language Python C+ / Java Compute Platform CUDA 11.8 CUDA 12.1 CUDA 12.4 ROCm 5.2 CPU Run this Command: conda install pytorch torchvision -c pytorch Previous versions of PyTorch Quick Start WithCloud Partners Get up and running with PyTorch quickly through popular cloud platforms and machine learning services. Amazon Web\n",
      "\n",
      "\n",
      "You ONLY have access to the following tools, and should NEVER make up tools that are not listed here:\n",
      "\n",
      "Tool Name: Search in a specific website\n",
      "Tool Arguments: {'search_query': {'description': 'Mandatory search query you want to use to search a specific website', 'type': 'str'}, 'website': {'description': 'Mandatory valid website URL you want to search on', 'type': 'str'}}\n",
      "Tool Description: A tool that can be used to semantic search a query from a specific URL content.\n",
      "\n",
      "IMPORTANT: Use the following format in your response:\n",
      "\n",
      "```\n",
      "Thought: you should always think about what to do\n",
      "Action: the action to take, only one name of [Search in a specific website], just the name, exactly as it's written.\n",
      "Action Input: the input to the action, just a simple JSON object, enclosed in curly braces, using \" to wrap keys and values.\n",
      "Observation: the result of the action\n",
      "```\n",
      "\n",
      "Once all necessary information is gathered, return the following format:\n",
      "\n",
      "```\n",
      "Thought: I now know the final answer\n",
      "Final Answer: the final answer to the original input question\n",
      "```\u001b[00m\n",
      "\u001b[93m Maximum iterations reached. Requesting final answer.\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mReporting Analyst\u001b[00m\n",
      "\u001b[95m## Final Answer:\u001b[00m \u001b[92m\n",
      "[\n",
      "    {\n",
      "        \"name\": \"H2O.ai\",\n",
      "        \"capabilities\": \"Combines predictive and generative AI capabilities, focusing on document AI and recommendation systems; strengths in natural language processing.\",\n",
      "        \"limitations\": \"Best suited for organizations familiar with AI infrastructure; may require fine-tuning for specific tasks; existing customer case studies may not cover every specific use case.\",\n",
      "        \"community_adoption\": \"Widely adopted in industries like finance and healthcare; included in Gartner Magic Quadrant for Machine Learning.\",\n",
      "        \"documentation_quality\": \"High-quality documentation and active community support.\",\n",
      "        \"task_evaluation\": \"Suitable for developing conversational systems; handles question-answering effectively through document AI features.\",\n",
      "        \"compatibility\": \"Compatible with FastAPI, though direct integration examples may be limited; LangChain can leverage H2O.ai solutions.\",\n",
      "        \"pros\": [\"Versatile for various AI applications\", \"Strong support for NLP tasks\", \"High community engagement\"],\n",
      "        \"cons\": [\"May require a steep learning curve\", \"Limits on specific PDF document handling capabilities\"],\n",
      "        \"recommendation_score\": 8.5\n",
      "    },\n",
      "    {\n",
      "        \"name\": \"LangChain\",\n",
      "        \"capabilities\": \"Framework designed for building applications powered by language models, integrating context-aware interactions.\",\n",
      "        \"limitations\": \"Still in development phase; potential backward compatibility issues as new features are added.\",\n",
      "        \"community_adoption\": \"Strong popularity with web developers; over 20 million monthly downloads, a large number of GitHub stars.\",\n",
      "        \"documentation_quality\": \"Extensive documentation and tutorial support available.\",\n",
      "        \"task_evaluation\": \"Highly suitable for RAG systems due to its focus on dynamic question answering and context management.\",\n",
      "        \"compatibility\": \"Integrates well with PostgreSQL and FastAPI; LangChain can benefit from H2O.ai's capabilities.\",\n",
      "        \"pros\": [\"Excellent for LLM applications\", \"Comprehensive documentation\", \"Active community support\"],\n",
      "        \"cons\": [\"Still maturing, which may involve some instability\", \"Complex setups for advanced features\"],\n",
      "        \"recommendation_score\": 9.0\n",
      "    },\n",
      "    {\n",
      "        \"name\": \"TensorFlow\",\n",
      "        \"capabilities\": \"Robust deep learning framework with extensive capabilities for building and training ML models.\",\n",
      "        \"limitations\": \"Can be complex to set up and may require extensive coding knowledge; less beginner-friendly.\",\n",
      "        \"community_adoption\": \"One of the most widely used frameworks, with extensive community and support resources.\",\n",
      "        \"documentation_quality\": \"Very detailed documentation and many tutorials available.\",\n",
      "        \"task_evaluation\": \"Excellent for complex conversational AI applications, but may require additional libraries for optimal RAG performance.\",\n",
      "        \"compatibility\": \"Compatible with FastAPI and can be adapted for use with LangChain.\",\n",
      "        \"pros\": [\"Highly versatile\", \"Strong support and tooling\", \"Wide industry adoption\"],\n",
      "        \"cons\": [\"Steep learning curve for newcomers\", \"Heavy resource usage for large models\"],\n",
      "        \"recommendation_score\": 8.0\n",
      "    },\n",
      "    {\n",
      "        \"name\": \"PyTorch\",\n",
      "        \"capabilities\": \"Flexible and user-friendly deep learning framework, recognized for ease of experimentation.\",\n",
      "        \"limitations\": \"Less mature in terms of deployment compared to TensorFlow; sometimes slower for production trials.\",\n",
      "        \"community_adoption\": \"Gaining rapid popularity, especially in academia and research.\",\n",
      "        \"documentation_quality\": \"Good documentation with numerous tutorials, but can sometimes lack detail in specific areas.\",\n",
      "        \"task_evaluation\": \"Strong candidate for developing conversational systems; great for research and prototyping, but deployment may require more management.\",\n",
      "        \"compatibility\": \"Works with PostgreSQL and can integrate with FastAPI; can utilize LangChain for conversational setups.\",\n",
      "        \"pros\": [\"Intuitive and easier for prototyping\", \"Strong research community\", \"Dynamic computation graph\"],\n",
      "        \"cons\": [\"Can be challenging for production deployment\", \"Resource-intense for large models\"],\n",
      "        \"recommendation_score\": 8.5\n",
      "    },\n",
      "    {\n",
      "        \"name\": \"Keras\",\n",
      "        \"capabilities\": \"High-level API for building and training deep learning models effectively.\",\n",
      "        \"limitations\": \"More limited when used independently; usually requires TensorFlow for backend support.\",\n",
      "        \"community_adoption\": \"Widely used in tandem with TensorFlow; many tutorials and community resources available.\",\n",
      "        \"documentation_quality\": \"Good documentation, though mostly centered around TensorFlow's performance.\",\n",
      "        \"task_evaluation\": \"Can help rapidly develop conversational models but doesn't provide depth on its own for complex tasks.\",\n",
      "        \"compatibility\": \"Directly uses TensorFlow; can be integrated with FastAPI and LangChain depending on model architecture.\",\n",
      "        \"pros\": [\"Rapid prototyping\", \"User-friendly interface\", \"Strong support through TensorFlow\"],\n",
      "        \"cons\": [\"Less flexible for advanced users\", \"Limited standalone capabilities\"],\n",
      "        \"recommendation_score\": 7.5\n",
      "    }\n",
      "]\u001b[00m\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages/instructor/process_response.py:222: DeprecationWarning: FUNCTIONS is deprecated and will be removed in future versions\n",
      "  if mode == Mode.FUNCTIONS:\n",
      "/Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages/instructor/function_calls.py:118: DeprecationWarning: FUNCTIONS is deprecated and will be removed in future versions\n",
      "  if mode == Mode.FUNCTIONS:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mReporting Analyst\u001b[00m\n",
      "\u001b[95m## Task:\u001b[00m \u001b[92m\n",
      "Read the analysis from the previous task's result and create a comprehensive Markdown report.\n",
      "\n",
      "The report should include:\n",
      "\n",
      "1. An introduction explaining the task ('Develop a conversational RAG system that can answer questions based on a large PDF document collection') and existing stack ('LangChain, PogreSQL, FastAPI')\n",
      "2. For each tool, create a section with:\n",
      "   - Tool name and URL as a heading\n",
      "   - Description\n",
      "   - Features (as bullet points)\n",
      "   - Pros (as bullet points)\n",
      "   - Cons (as bullet points)\n",
      "   - Integration complexity\n",
      "   - Recommendation score with justification\n",
      "3. A summary/conclusion comparing the tools and providing final recommendations\n",
      "\n",
      "Use proper Markdown formatting with headings, bullet points, and emphasis where appropriate.\n",
      "Sort tools by recommendation score (descending).\n",
      "\n",
      "Your output should be a complete, well-formatted Markdown document.\n",
      "\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mReporting Analyst\u001b[00m\n",
      "\u001b[95m## Using tool:\u001b[00m \u001b[92mSearch in a specific website\u001b[00m\n",
      "\u001b[95m## Tool Input:\u001b[00m \u001b[92m\n",
      "\"{\\\"search_query\\\": \\\"Develop a conversational RAG system that can answer questions based on a large PDF document collection\\\", \\\"website\\\": \\\"https://langchain.com/\\\"}\"\u001b[00m\n",
      "\u001b[95m## Tool Output:\u001b[00m \u001b[92m\n",
      "Relevant Content:\n",
      "H2O.ai | Convergence of The World's Best Predictive & Generative AI The World's Best Deep Research Chat Now Chat on Mobile Learn More Companies powered by H2O.ai H2Os flagship SLMs H2OVL Mississippi SVLM Series Our newest economical multimodal OCR model developed for Document AI H2OVL Mississippi-2B, based on H2O Danube2, is trained on 17.3M conversation pairs for high-res image handling. The .8B model, built on Danube3, leads OCR benchmarks with 19M pairs, outperforming all SLMs in text recognition. For Developers  1 Text Recognition H2O Danube SLM Series Our most economical small model for fast, lightweight tasks We trained H2O Danube3 models from scratch on ~100 H100 GPUs using our own curated dataset of 6T tokens. H2O Danube3-4B and .5B open-weight SLMs outperform the latest Apple OpenELM-3B and .5B instruct models. Perfect for developers who want to fine-tune their own SLMs for offline use cases. For Developers Get the Mobile App H2O Generative AI in use with customers H2O Document AI GPTs and Digital Agents in production worldwide In-Store Sales Assistant HR & IT Support Agent Fraud & Scam Detection RFP Assistant Procurement GPT Personalized Pricing Complaint Summarizer Distilled SLMs for Call Center Classification First-ever model risk management for Generative AI Generative AI is herebut can you audit it? H2O.ai brings Model Risk Management (MRM) to GenAI, combining enterprise-grade model evaluation with h2oGPTe and H2O Model Validation to make AI measurable, explainable, and compliant for banks and financial services. Automated testing, human-calibrated evaluations, and real-time risk monitoring ensure transparencyboosting compliance, risk management, and user confidence. H2O Model Validation Multi-modal Document AI with enterprise scale and security Convert Data to JSON Document AI ProcurementGPT Complaint Summarizer Convert structured data like financial tables to JSON Get answers from diagrams, flowcharts, web pages, audio files, video files and\n",
      "\n",
      "H2O.ai | Convergence of The World's Best Predictive & Generative AI The World's Best Deep Research Chat Now Chat on Mobile Learn More Companies powered by H2O.ai H2Os flagship SLMs H2OVL Mississippi SVLM Series Our newest economical multimodal OCR model developed for Document AI H2OVL Mississippi-2B, based on H2O Danube2, is trained on 17.3M conversation pairs for high-res image handling. The .8B model, built on Danube3, leads OCR benchmarks with 19M pairs, outperforming all SLMs in text recognition. For Developers  1 Text Recognition H2O Danube SLM Series Our most economical small model for fast, lightweight tasks We trained H2O Danube3 models from scratch on ~100 H100 GPUs using our own curated dataset of 6T tokens. H2O Danube3-4B and .5B open-weight SLMs outperform the latest Apple OpenELM-3B and .5B instruct models. Perfect for developers who want to fine-tune their own SLMs for offline use cases. For Developers Get the Mobile App H2O Generative AI in use with customers H2O Document AI GPTs and Digital Agents in production worldwide In-Store Sales Assistant HR & IT Support Agent Fraud & Scam Detection RFP Assistant Procurement GPT Personalized Pricing Complaint Summarizer Distilled SLMs for Call Center Classification First-ever model risk management for Generative AI Generative AI is herebut can you audit it? H2O.ai brings Model Risk Management (MRM) to GenAI, combining enterprise-grade model evaluation with h2oGPTe and H2O Model Validation to make AI measurable, explainable, and compliant for banks and financial services. Automated testing, human-calibrated evaluations, and real-time risk monitoring ensure transparencyboosting compliance, risk management, and user confidence. H2O Model Validation Multi-modal Document AI with enterprise scale and security Convert Data to JSON Document AI ProcurementGPT Complaint Summarizer Convert structured data like financial tables to JSON Get answers from diagrams, flowcharts, web pages, audio files, video files and\n",
      "\n",
      "images. Intelligent document comparison with ProcurementGPT ProcurementGPT streamlines document analysis by automatically parsing documents into sections or clauses, then identifying the most similar counterparts across documentsworking symmetrically in both directions. It further breaks down clauses into key requirements, determining where each one is fulfilled and highlighting met and unmet requirements for easy review. The system leverages the top 10 most similar requirements to assess compliance, with an overlay for quick navigation. Users can also manually select sections for comparison, ensuring flexibility when automated parsing needs refinement. Learn More Summary of complaints and key issues Provides a high-level summary of the most common complaints, the top issues faced, and recommended actions to address these concerns. Learn More Built for air-gapped, on-premises, private or managed cloud deployments. Own your data, own your prompts. Only H2O.ai provides an end-to-end GenAI platform where you can own every part of the stack. H2O.ai offers a highly flexible solution, that can run fully managed in the cloud, or in hybrid or on-premise and air-gapped environments. Fully scalable with Kubernetes. H2O.ai achieves SOC2 Type 2 +HIPAA/HITECH Cost control and flexibility Customers can decide to mix and match from a large choice of over 30 LLMs: Proprietary LLMs: GPT-4o, o1, Gemini, Claude, Mistral, etc. Reliance on third-party provider No control, no ownership Token-based pricing, unbounded cost Open source LLMs: Llama3, DeepSeek, Qwen2.5, Mixtral, Mistral, H2O Danube, H2OVL Mississippi, any Fine-Tuned OSS model. Any model from Open-Source ecosystem Hosted on your own GPU infrastructure Full control and ownership of models Fixed hardware cost, most economical choice Cost savings on H2O.ai Platform. 25x cheaper on query cost. Serve more users. Better accuracy with up to 100% cheaper on Document Processing. h2oGPTe is designed to work with any LLM of your choice\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mReporting Analyst\u001b[00m\n",
      "\u001b[95m## Final Answer:\u001b[00m \u001b[92m\n",
      "# Comprehensive Report on Developing a Conversational RAG System\n",
      "\n",
      "## Introduction\n",
      "The task at hand is to develop a **Conversational Retrieval-Augmented Generation (RAG)** system that can answer questions based on a large collection of PDF documents. This type of system leverages advances in natural language processing to retrieve relevant information and generate responses based on user queries. The existing stack for this project includes **LangChain**, **PostgreSQL**, and **FastAPI**. Below, we analyze various AI tools that can be integrated into this stack to enhance the RAG system's capabilities.\n",
      "\n",
      "## LangChain\n",
      "### [LangChain](https://langchain.com/)\n",
      "**Description**: LangChain is a versatile framework designed for constructing applications powered by language models (LLMs), enabling context-aware interactions and integration with data sources and APIs. It is particularly suited for building complex conversational systems.\n",
      "\n",
      "**Features**:\n",
      "- Streamlined development of LLM applications.\n",
      "- Context management for better user interactions.\n",
      "- Robust integration capabilities with databases and external APIs.\n",
      "\n",
      "**Pros**:\n",
      "- Excellent documentation and tutorial support.\n",
      "- Active community engagement with over 20 million monthly downloads.\n",
      "- Highly suitable for dynamic question answering.\n",
      "\n",
      "**Cons**:\n",
      "- Still maturing, which may involve some instability.\n",
      "- Advanced features may require complex setups.\n",
      "\n",
      "**Integration Complexity**: Moderate. It integrates well with PostgreSQL and FastAPI.\n",
      "\n",
      "**Recommendation Score**: **9.0**. LangChain is highly recommended for its focus on LLM applications and dynamic interaction management, making it the top choice for the conversational RAG system.\n",
      "\n",
      "## H2O.ai\n",
      "### [H2O.ai](https://www.h2o.ai/)\n",
      "**Description**: H2O.ai combines predictive and generative AI capabilities, providing tools for various tasks including document AI and recommendation systems. Its strengths in handling natural language processing make it favorable for developing conversational systems.\n",
      "\n",
      "**Features**:\n",
      "- Predictive and generative AI capabilities.\n",
      "- Document AI functions focusing on text recognition and analysis.\n",
      "- Support for multimodal applications.\n",
      "\n",
      "**Pros**:\n",
      "- Strong support for NLP tasks and high community engagement.\n",
      "- Suitable for various AI applications beyond conversational AI.\n",
      "- High-quality documentation and active support.\n",
      "\n",
      "**Cons**:\n",
      "- May require a steep learning curve for new users.\n",
      "- Limitations on specific PDF document handling capabilities.\n",
      "\n",
      "**Integration Complexity**: Moderate. Compatible with FastAPI but may require some adaptation for seamless integration.\n",
      "\n",
      "**Recommendation Score**: **8.5**. H2O.ai is recommended for its robust NLP features, enhancing the RAG system's ability to process and understand large document collections.\n",
      "\n",
      "## PyTorch\n",
      "### [PyTorch](https://pytorch.org/)\n",
      "**Description**: PyTorch is a popular deep learning framework recognized for its flexibility and user-friendly design. Developed by Facebook, it excels in research and prototyping, making it an excellent tool for developing innovative conversational technologies.\n",
      "\n",
      "**Features**:\n",
      "- Dynamic computation graph for easier experimentation.\n",
      "- Strong support for deep learning model training.\n",
      "- Extensive library support for various AI tasks.\n",
      "\n",
      "**Pros**:\n",
      "- Intuitive interface facilitating prototyping.\n",
      "- Strong research community and many tutorials available.\n",
      "- Flexibility in model design and implementation.\n",
      "\n",
      "**Cons**:\n",
      "- Challenges with production deployment compared to other frameworks.\n",
      "- Can be resource-intensive for large models.\n",
      "\n",
      "**Integration Complexity**: Moderate to High. Works well with PostgreSQL and can integrate with FastAPI through additional configurations.\n",
      "\n",
      "**Recommendation Score**: **8.5**. PyTorch is a strong candidate for supporting complex conversational AI applications, especially in a research or prototype context.\n",
      "\n",
      "## TensorFlow\n",
      "### [TensorFlow](https://www.tensorflow.org/)\n",
      "**Description**: TensorFlow, developed by Google, is an open-source library for building and training machine learning models, particularly those involving deep learning. Its extensive capabilities lend themselves well to complex conversational AI applications.\n",
      "\n",
      "**Features**:\n",
      "- Comprehensive libraries for deep learning model development.\n",
      "- TensorFlow Serving for easy deployment of models.\n",
      "- Highly customizable and scalable.\n",
      "\n",
      "**Pros**:\n",
      "- Detailed documentation with extensive community support.\n",
      "- Highly versatile for different AI applications.\n",
      "- Strong tooling support for various tasks.\n",
      "\n",
      "**Cons**:\n",
      "- Can be complex to set up, requiring extensive coding knowledge.\n",
      "- Less beginner-friendly compared to some alternatives.\n",
      "\n",
      "**Integration Complexity**: Moderate. Compatible with FastAPI and can work with LangChain.\n",
      "\n",
      "**Recommendation Score**: **8.0**. TensorFlow is a solid choice for more complex applications needing deep learning but may present a steeper learning curve.\n",
      "\n",
      "## Keras\n",
      "### [Keras](https://keras.io/)\n",
      "**Description**: Keras is a high-level API for building and training deep learning models, often used with TensorFlow. It provides a simplified interface for rapid experimentation and model development in conversational AI.\n",
      "\n",
      "**Features**:\n",
      "- Simplified high-level interface for ease of use.\n",
      "- Works seamlessly with various backends, mainly TensorFlow.\n",
      "- Rapid prototyping capabilities.\n",
      "\n",
      "**Pros**:\n",
      "- User-friendly interface appealing to beginners.\n",
      "- Strong community support through TensorFlow.\n",
      "- Encourages rapid development cycles.\n",
      "\n",
      "**Cons**:\n",
      "- Limited when used independently; typically requires TensorFlow.\n",
      "- Less flexibility for advanced users needing customized deep learning architectures.\n",
      "\n",
      "**Integration Complexity**: Moderate. Usually works well with FastAPI and can integrate with LangChain as long as TensorFlow is in use.\n",
      "\n",
      "**Recommendation Score**: **7.5**. Keras is useful for quick experimentation but may not suffice for more complex requirements.\n",
      "\n",
      "## Summary and Conclusion\n",
      "In summary, the most suitable tools for developing a conversational RAG system are **LangChain**, followed closely by **H2O.ai** and **PyTorch**. Each tool brings unique strengths to the table:\n",
      "\n",
      "- **LangChain** is the top recommendation for its dynamic capabilities and LLM focus, essential for responsive user interactions.\n",
      "- **H2O.ai** offers significant NLP capabilities valuable for processing PDF documents effectively.\n",
      "- **PyTorch** is a flexible option for research-oriented projects but may involve challenges in deploying production-level systems.\n",
      "\n",
      "Ultimately, the choice of tool will depend on specific project requirements, team expertise, and future scalability needs. Integrating these tools thoughtfully will lead to improved performance and user satisfaction in your Conversational RAG system.\u001b[00m\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages/instructor/process_response.py:222: DeprecationWarning: FUNCTIONS is deprecated and will be removed in future versions\n",
      "  if mode == Mode.FUNCTIONS:\n",
      "/Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages/instructor/function_calls.py:118: DeprecationWarning: FUNCTIONS is deprecated and will be removed in future versions\n",
      "  if mode == Mode.FUNCTIONS:\n",
      "/Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages/pydantic/main.py:1111: PydanticDeprecatedSince20: The `json` method is deprecated; use `model_dump_json` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.8/migration/\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " View run Tool_Research_1744870392 at: https://public-tracking.mlflow-e00rfd6s4w3cdtk9fx.backbone-e00rt16p71ew33dg9e.msp.eu-north1.nebius.cloud/#/experiments/7/runs/6221c1fa187e4f3a96325428f9aaf91a\n",
      " View experiment at: https://public-tracking.mlflow-e00rfd6s4w3cdtk9fx.backbone-e00rt16p71ew33dg9e.msp.eu-north1.nebius.cloud/#/experiments/7\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<div>\n",
       "  <style scoped>\n",
       "  button {\n",
       "    border: none;\n",
       "    border-radius: 4px;\n",
       "    background-color: rgb(34, 114, 180);\n",
       "    font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, \"Helvetica Neue\", Arial;\n",
       "    font-size: 13px;\n",
       "    color: white;\n",
       "    margin-top: 8px;\n",
       "    margin-bottom: 8px;\n",
       "    padding: 8px 16px;\n",
       "    cursor: pointer;\n",
       "  }\n",
       "  button:hover {\n",
       "    background-color: rgb(66, 153, 224);\n",
       "  }\n",
       "  </style>\n",
       "  <button\n",
       "    onclick=\"\n",
       "        const display = this.nextElementSibling.style.display;\n",
       "        const isCollapsed = display === 'none';\n",
       "        this.nextElementSibling.style.display = isCollapsed ? null : 'none';\n",
       "\n",
       "        const verb = isCollapsed ? 'Collapse' : 'Expand';\n",
       "        this.innerText = `${verb} MLflow Trace`;\n",
       "    \"\n",
       "  >Collapse MLflow Trace</button>\n",
       "  <iframe\n",
       "    id=\"trace-renderer\"\n",
       "    style=\"width: 100%; height: 500px; border: none; resize: vertical;\"\n",
       "    src=\"https://public-tracking.mlflow-e00rfd6s4w3cdtk9fx.backbone-e00rt16p71ew33dg9e.msp.eu-north1.nebius.cloud/static-files/lib/notebook-trace-renderer/index.html?trace_id=886c2510d3134cf5a3190de8e5d0dbde&amp;experiment_id=7&amp;version=2.21.2\"\n",
       "  />\n",
       "</div>\n"
      ],
      "text/plain": [
       "Trace(request_id=886c2510d3134cf5a3190de8e5d0dbde)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let's run it once without MLflow to see the typical verbose console output.\n",
    "\n",
    "# Turn on auto tracing by calling mlflow.crewai.autolog()\n",
    "mlflow.crewai.autolog()\n",
    "mlflow.set_experiment(\"Step 3 - Metrics\")\n",
    "\n",
    "ai_dev_crew = AIOpsResearchCrew(task_description, existing_stack)\n",
    "result = ai_dev_crew.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crew Usage Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-17T06:15:01.866365Z",
     "iopub.status.busy": "2025-04-17T06:15:01.865969Z",
     "iopub.status.idle": "2025-04-17T06:15:01.900611Z",
     "shell.execute_reply": "2025-04-17T06:15:01.900305Z",
     "shell.execute_reply.started": "2025-04-17T06:15:01.866343Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UsageMetrics(total_tokens=46689, prompt_tokens=42767, cached_prompt_tokens=23424, completion_tokens=3922, successful_requests=9)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the crew usage metrics \n",
    "\n",
    "ai_dev_crew.crew.usage_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation:** \n",
    "\n",
    "Now, every run leaves a persistent, detailed trace artifact in MLflow. No more \"What just happened?\" guesswork."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Evaluating Agent Output Quality\n",
    "\n",
    "**Problem:** How to automatically evaluate the quality of the final result without manually comparing LLMs or prompts.\n",
    "\n",
    "**MLflow Solution:**\n",
    "- Log agent configurations and outputs with MLflow parameters\n",
    "- Define specific quality criteria for assessment reports\n",
    "- Use LLM-as-Judge evaluation by having a powerful model score outputs against criteria\n",
    "- Log evaluation metrics (`accuracy_score`, `completeness_score`, etc.) to MLflow\n",
    "- Compare runs in MLflow UI to identify highest quality configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up Nebius AI Studio client to \"evaluator\" LLM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-17T06:20:05.707466Z",
     "iopub.status.busy": "2025-04-17T06:20:05.706982Z",
     "iopub.status.idle": "2025-04-17T06:20:05.754500Z",
     "shell.execute_reply": "2025-04-17T06:20:05.754180Z",
     "shell.execute_reply.started": "2025-04-17T06:20:05.707433Z"
    }
   },
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "API_KEY = os.environ.get(\"NEBIUS_API_KEY\")\n",
    "\n",
    "# Instantiate the client instance\n",
    "nebius_client = openai.OpenAI(\n",
    "    api_key=API_KEY,\n",
    "    base_url=\"https://api.studio.nebius.ai/v1/\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simplify tasks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-17T07:07:27.574706Z",
     "iopub.status.busy": "2025-04-17T07:07:27.574227Z",
     "iopub.status.idle": "2025-04-17T07:07:27.616617Z",
     "shell.execute_reply": "2025-04-17T07:07:27.616167Z",
     "shell.execute_reply.started": "2025-04-17T07:07:27.574670Z"
    }
   },
   "outputs": [],
   "source": [
    "from crewai_tools import FileReadTool\n",
    "\n",
    "class AIOpsResearchTasks:\n",
    "    def search_tools_task(self, agent, task, ai_stack):\n",
    "        \"\"\"Task to discover relevant tools for a specific task considering the existing AI stack.\"\"\"\n",
    "        return Task(\n",
    "            description=f\"\"\"\n",
    "                Find some AI tools for {task}.\n",
    "                Format however you think is best.\n",
    "            \"\"\",\n",
    "            agent=agent,\n",
    "            tools=[search_tool],\n",
    "            expected_output=\"Info about AI tools\",\n",
    "            output_file=\"output/tool_candidates.json\",\n",
    "            cache=False\n",
    "        )\n",
    "\n",
    "    def analyze_tools_task(self, agent, task, ai_stack):\n",
    "        \"\"\"Task to perform in-depth analysis of discovered tools.\"\"\"\n",
    "        return Task(\n",
    "            description=f\"\"\"\n",
    "                Check out the tools from before.\n",
    "                \n",
    "                Tell me what you think about each one.\n",
    "                Is it good? Does it work with {ai_stack}?\n",
    "                \n",
    "                Make it JSON I guess.\n",
    "            \"\"\",\n",
    "            agent=agent,\n",
    "            tools=[search_tool],\n",
    "            expected_output=\"Analysis of tools\",\n",
    "            output_file=\"output/tool_analysis.json\",\n",
    "            cache=False,\n",
    "        )\n",
    "\n",
    "    def create_report_task(self, agent, task, ai_stack):\n",
    "        \"\"\"Task to create a comprehensive report with recommendations.\"\"\"\n",
    "        return Task(\n",
    "            description=f\"\"\"\n",
    "                Make a report about the tools.\n",
    "                Use markdown formatting.\n",
    "            \"\"\",\n",
    "            agent=agent,\n",
    "            tools=[FileReadTool(file_path='output/tool_analysis.json')],\n",
    "            expected_output=\"A report\",\n",
    "            output_file=\"output/tool_recommendation_report.md\",\n",
    "            cache=False,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-17T07:48:10.106365Z",
     "iopub.status.busy": "2025-04-17T07:48:10.105852Z",
     "iopub.status.idle": "2025-04-17T07:48:10.155092Z",
     "shell.execute_reply": "2025-04-17T07:48:10.154580Z",
     "shell.execute_reply.started": "2025-04-17T07:48:10.106330Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "import os\n",
    "from textwrap import dedent\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def evaluate_report(\n",
    "    report_path: str, \n",
    "    nebius_client,\n",
    "    model: str = \"meta-llama/Llama-3.3-70B-Instruct\"\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Evaluate a technical report using Nebius AI.\n",
    "    \n",
    "    Args:\n",
    "        report_path: Path to the markdown report file\n",
    "        nebius_client: Initialized Nebius API client\n",
    "        model: Model name to use for evaluation\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary containing evaluation scores and reasoning\n",
    "    \"\"\"\n",
    "    # Read the report content\n",
    "    try:\n",
    "        with open(report_path, 'r') as f:\n",
    "            report_content = f.read()\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error reading report file: {str(e)}\")\n",
    "        return {\n",
    "            \"completeness_score\": 0,\n",
    "            \"relevance_score\": 0,\n",
    "            \"overall_quality_score\": 0,\n",
    "            \"reasoning\": f\"Error reading report: {str(e)}\"\n",
    "        }\n",
    "    \n",
    "    # Create evaluation prompt\n",
    "    evaluation_prompt = dedent(f\"\"\"\n",
    "        Evaluate the quality of the following technical report.\n",
    "        \n",
    "        Your job is to score the report on the following criteria (scale 1-10):\n",
    "        \n",
    "        - COMPLETENESS: Does the report include all required sections for each tool?\n",
    "          1-3: Missing multiple required sections\n",
    "          4-6: Has basic information but lacks detail\n",
    "          7-8: Contains most required sections with good detail\n",
    "          9-10: Complete with introduction, tool sections (name, URL, description, features, pros, cons, integration complexity, recommendation score), and conclusion\n",
    "        \n",
    "        - RELEVANCE: How directly applicable are the recommended tools to the specific task and tech stack?\n",
    "          1-3: General tools with no specific RAG functionality (like basic logging libraries)\n",
    "          4-6: Tools with potential use cases (like numpy for data processing)\n",
    "          7-8: Related tools with partial functionality (like fastai - useful but no specific RAG features)\n",
    "          9-10: Directly applicable tools (like LangGraph or CrewAI - frameworks specifically for RAG systems)\n",
    "        \n",
    "        - OVERALL QUALITY: The overall quality of the report considering all factors.\n",
    "        \n",
    "        Here is the report to evaluate:\n",
    "        \n",
    "        ---BEGIN REPORT---\n",
    "        {report_content}\n",
    "        ---END REPORT---\n",
    "        \n",
    "        Provide your evaluation as a JSON object with the following format:\n",
    "            \"completeness_score\": X,\n",
    "            \"relevance_score\": X,\n",
    "            \"overall_quality_score\": X,\n",
    "            \"reasoning\": \"Detailed explanation of your evaluation and scores...\"\n",
    "        }}\n",
    "        \n",
    "        Return ONLY the JSON object with no additional text.\n",
    "    \"\"\")\n",
    "    \n",
    "    # Call Nebius AI Studio for evaluation\n",
    "    try:\n",
    "        response = nebius_client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are an expert evaluator of technical reports.\"},\n",
    "                {\"role\": \"user\", \"content\": evaluation_prompt}\n",
    "            ],\n",
    "            temperature=0,\n",
    "            response_format={\"type\": \"json_object\"}\n",
    "        )\n",
    "        \n",
    "        # Parse response\n",
    "        result = json.loads(response.choices[0].message.content)\n",
    "        \n",
    "        # Ensure all scores are present\n",
    "        required_scores = [\"completeness_score\", \"relevance_score\", \"overall_quality_score\"]\n",
    "        for score in required_scores:\n",
    "            if score not in result:\n",
    "                result[score] = 0\n",
    "        \n",
    "        # Ensure reasoning is present\n",
    "        if \"reasoning\" not in result:\n",
    "            result[\"reasoning\"] = \"No reasoning provided.\"\n",
    "            \n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during evaluation: {str(e)}\")\n",
    "        return {\n",
    "            \"completeness_score\": 0,\n",
    "            \"relevance_score\": 0,\n",
    "            \"overall_quality_score\": 0,\n",
    "            \"reasoning\": f\"Evaluation error: {str(e)}\"\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-17T07:48:11.560586Z",
     "iopub.status.busy": "2025-04-17T07:48:11.560282Z",
     "iopub.status.idle": "2025-04-17T07:48:28.939311Z",
     "shell.execute_reply": "2025-04-17T07:48:28.938669Z",
     "shell.execute_reply.started": "2025-04-17T07:48:11.560571Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'completeness_score': 10,\n",
       " 'relevance_score': 9,\n",
       " 'overall_quality_score': 9,\n",
       " 'reasoning': 'The report is complete with an introduction, tool sections including name, URL, description, features, pros, cons, integration complexity, and recommendation score, and a conclusion. The tools recommended are directly applicable to the specific task of developing a conversational RAG system, with LangChain and H2O.ai being particularly well-suited. The report provides a detailed analysis of each tool, including their strengths and weaknesses, making it highly relevant. The overall quality of the report is excellent, with clear and concise writing, and a well-structured format. The only area for improvement could be in providing more specific examples or case studies to illustrate the effectiveness of each tool, but overall, the report is comprehensive and well-written.'}"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation_results = evaluate_report(\n",
    "                    report_path='output/tool_recommendation_report.md',\n",
    "                    nebius_client=nebius_client,\n",
    "                    model=\"meta-llama/Llama-3.3-70B-Instruct\"\n",
    "                )\n",
    "\n",
    "evaluation_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modify AIOpsResearchCrew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-17T07:48:35.995577Z",
     "iopub.status.busy": "2025-04-17T07:48:35.995161Z",
     "iopub.status.idle": "2025-04-17T07:48:36.040710Z",
     "shell.execute_reply": "2025-04-17T07:48:36.040253Z",
     "shell.execute_reply.started": "2025-04-17T07:48:35.995546Z"
    }
   },
   "outputs": [],
   "source": [
    "class AIOpsResearchCrewEvaluated:\n",
    "    def __init__(self, task, ai_stack, nebius_client=None, evaluation_model=\"meta-llama/Llama-3.3-70B-Instruct\"):\n",
    "        \"\"\"\n",
    "        Initialize the crew with the task description and existing AI stack.\n",
    "        \n",
    "        Args:\n",
    "            task (str): Description of the task requiring AI tools\n",
    "            ai_stack (str): Comma-separated list of existing tools/frameworks used\n",
    "            nebius_client: Initialized Nebius API client for evaluation\n",
    "            evaluation_model (str): Model to use for evaluation\n",
    "        \"\"\"\n",
    "        self.task = task\n",
    "        self.ai_stack = ai_stack\n",
    "        self.run_id = None\n",
    "        self.execution_log = io.StringIO()\n",
    "        self.crew = None\n",
    "        self.nebius_client = nebius_client\n",
    "        self.evaluation_model = evaluation_model\n",
    "        \n",
    "        # Ensure output directory exists\n",
    "        os.makedirs(\"output\", exist_ok=True)\n",
    "        \n",
    "        # Initialize Nebius client if not provided\n",
    "        if self.nebius_client is None and os.environ.get(\"NEBIUS_API_KEY\"):\n",
    "            self.nebius_client = openai.OpenAI(\n",
    "                api_key=os.environ.get(\"NEBIUS_API_KEY\"),\n",
    "                base_url=\"https://api.studio.nebius.ai/v1/\"\n",
    "            )\n",
    "\n",
    "    def _clean_output_files(self):\n",
    "        \"\"\"Remove all previously generated output files if they exist.\"\"\"\n",
    "        files_to_remove = [\n",
    "            \"output/tool_candidates.json\",\n",
    "            \"output/tool_analysis.json\",\n",
    "            \"output/tool_recommendation_report.md\",\n",
    "            \"output/report_evaluation.json\"\n",
    "        ]\n",
    "        \n",
    "        for file_path in files_to_remove:\n",
    "            if os.path.exists(file_path):\n",
    "                try:\n",
    "                    os.remove(file_path)\n",
    "                    logger.info(f\"Removed existing file: {file_path}\")\n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"Failed to remove file {file_path}: {str(e)}\")\n",
    "\n",
    "    def run(self, evaluate=True):\n",
    "        \"\"\"\n",
    "        Execute the research, analysis, and reporting process with MLflow tracking.\n",
    "        \n",
    "        Args:\n",
    "            evaluate (bool): Whether to evaluate the report after generation\n",
    "        \n",
    "        Returns:\n",
    "            The result from the crew execution\n",
    "        \"\"\"\n",
    "        # Clean any existing output files\n",
    "        self._clean_output_files()\n",
    "        \n",
    "        # --- Start MLflow run ---\n",
    "        with mlflow.start_run(run_name=f\"Tool_Research_{int(time.time())}\") as mlflow_run:\n",
    "            self.run_id = mlflow_run.info.run_id\n",
    "\n",
    "            # Initialize agents\n",
    "            agents = AIOpsResearchAgents()\n",
    "            researcher = agents.researcher_agent()\n",
    "            analyst = agents.analyst_agent()\n",
    "            \n",
    "            # Initialize tasks\n",
    "            tasks = AIOpsResearchTasks()\n",
    "            search_task = tasks.search_tools_task(researcher, self.task, self.ai_stack)\n",
    "            analyze_task = tasks.analyze_tools_task(analyst, self.task, self.ai_stack)\n",
    "            report_task = tasks.create_report_task(analyst, self.task, self.ai_stack)\n",
    "            \n",
    "            # Create the crew\n",
    "            self.crew = Crew(\n",
    "                agents=[researcher, analyst],\n",
    "                tasks=[search_task, analyze_task, report_task],\n",
    "                verbose=False,\n",
    "                process=Process.sequential,\n",
    "                memory=True,\n",
    "                cache=False\n",
    "            )\n",
    "            \n",
    "            # Start the crew\n",
    "            logger.info(\"Starting crew execution...\")\n",
    "            result = self.crew.kickoff()\n",
    "                        \n",
    "            # --- Log parameters ---\n",
    "            mlflow.log_param(\"task\", self.task)\n",
    "            mlflow.log_param(\"ai_stack\", self.ai_stack)\n",
    "            mlflow.log_param(\"evaluation_model\", self.evaluation_model)\n",
    "\n",
    "            # --- Log metrics ---\n",
    "            mlflow.log_metrics(json.loads(self.crew.usage_metrics.json()))\n",
    "\n",
    "            # --- Log artifacts ---\n",
    "            artifact_files = [\n",
    "                \"output/tool_candidates.json\",\n",
    "                \"output/tool_analysis.json\",\n",
    "                \"output/tool_recommendation_report.md\",\n",
    "            ]\n",
    "            for file_path in artifact_files:\n",
    "                if os.path.exists(file_path):\n",
    "                    mlflow.log_artifact(file_path)\n",
    "\n",
    "            # --- UPDATE: Set success tag ---\n",
    "            if os.path.exists(\"output/tool_recommendation_report.md\"):\n",
    "                mlflow.set_tag(\"status\", \"SUCCESS\")\n",
    "            else:\n",
    "                mlflow.set_tag(\"status\", \"FAILED\")\n",
    "            \n",
    "            # --- Set success tag ---\n",
    "            report_path = \"output/tool_recommendation_report.md\"\n",
    "            if os.path.exists(report_path):\n",
    "                mlflow.set_tag(\"status\", \"SUCCESS\")\n",
    "                \n",
    "            # --- Evaluate the report if requested and client available ---\n",
    "            if evaluate and self.nebius_client:\n",
    "                evaluation_results = evaluate_report(\n",
    "                    report_path=report_path,\n",
    "                    nebius_client=self.nebius_client,\n",
    "                    model=self.evaluation_model\n",
    "                )\n",
    "                \n",
    "                # Log evaluation results to MLflow\n",
    "                with open(\"output/report_evaluation.json\", \"w\") as f:\n",
    "                    json.dump(evaluation_results, f, indent=2)\n",
    "                mlflow.log_artifact(\"output/report_evaluation.json\")\n",
    "                \n",
    "                # Extract and log only numeric scores\n",
    "                numeric_scores = {}\n",
    "                for key, value in evaluation_results.items():\n",
    "                    if key.endswith('_score') and isinstance(value, (int, float)):\n",
    "                        numeric_scores[key] = value\n",
    "                \n",
    "                if numeric_scores:\n",
    "                    mlflow.log_metrics(numeric_scores)\n",
    "                \n",
    "                return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run with Agent evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-17T07:48:36.499341Z",
     "iopub.status.busy": "2025-04-17T07:48:36.499018Z",
     "iopub.status.idle": "2025-04-17T07:51:04.363609Z",
     "shell.execute_reply": "2025-04-17T07:51:04.363120Z",
     "shell.execute_reply.started": "2025-04-17T07:48:36.499317Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mSenior Data Researcher\u001b[00m\n",
      "\u001b[95m## Task:\u001b[00m \u001b[92m\n",
      "                Find some AI tools for Develop a conversational RAG system that can answer questions based on a large PDF document collection.\n",
      "                Format however you think is best.\n",
      "            \u001b[00m\n",
      "\u001b[91m \n",
      "\n",
      "I encountered an error while trying to use the tool. This was the error: 403 Client Error: Forbidden for url: https://towardsdatascience.com/.\n",
      " Tool Search in a specific website accepts these inputs: Tool Name: Search in a specific website\n",
      "Tool Arguments: {'search_query': {'description': 'Mandatory search query you want to use to search a specific website', 'type': 'str'}, 'website': {'description': 'Mandatory valid website URL you want to search on', 'type': 'str'}}\n",
      "Tool Description: A tool that can be used to semantic search a query from a specific URL content.\n",
      "\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mSenior Data Researcher\u001b[00m\n",
      "\u001b[95m## Thought:\u001b[00m \u001b[92mI need to find AI tools that can assist in developing a conversational Retrieval-Augmented Generation (RAG) system that utilizes a large PDF document collection.\u001b[00m\n",
      "\u001b[95m## Using tool:\u001b[00m \u001b[92mSearch in a specific website\u001b[00m\n",
      "\u001b[95m## Tool Input:\u001b[00m \u001b[92m\n",
      "\"{\\\"search_query\\\": \\\"AI tools for conversational RAG systems\\\", \\\"website\\\": \\\"https://towardsdatascience.com\\\"}\"\u001b[00m\n",
      "\u001b[95m## Tool Output:\u001b[00m \u001b[92m\n",
      "\n",
      "I encountered an error while trying to use the tool. This was the error: 403 Client Error: Forbidden for url: https://towardsdatascience.com/.\n",
      " Tool Search in a specific website accepts these inputs: Tool Name: Search in a specific website\n",
      "Tool Arguments: {'search_query': {'description': 'Mandatory search query you want to use to search a specific website', 'type': 'str'}, 'website': {'description': 'Mandatory valid website URL you want to search on', 'type': 'str'}}\n",
      "Tool Description: A tool that can be used to semantic search a query from a specific URL content..\n",
      "Moving on then. I MUST either use a tool (use one at time) OR give my best final answer not both at the same time. When responding, I must use the following format:\n",
      "\n",
      "```\n",
      "Thought: you should always think about what to do\n",
      "Action: the action to take, should be one of [Search in a specific website]\n",
      "Action Input: the input to the action, dictionary enclosed in curly braces\n",
      "Observation: the result of the action\n",
      "```\n",
      "This Thought/Action/Action Input/Result can repeat N times. Once I know the final answer, I must return the following format:\n",
      "\n",
      "```\n",
      "Thought: I now can give a great answer\n",
      "Final Answer: Your final answer must be the great and the most complete as possible, it must be outcome described\n",
      "\n",
      "```\u001b[00m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inserting batches in chromadb: 100%|| 1/1 [00:00<00:00,  2.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mSenior Data Researcher\u001b[00m\n",
      "\u001b[95m## Thought:\u001b[00m \u001b[92mThought: I need to find relevant AI tools for developing a conversational RAG system based on a large PDF document collection. I will search for tools focusing on this specific application.\u001b[00m\n",
      "\u001b[95m## Using tool:\u001b[00m \u001b[92mSearch in a specific website\u001b[00m\n",
      "\u001b[95m## Tool Input:\u001b[00m \u001b[92m\n",
      "\"{\\\"search_query\\\": \\\"AI tools for conversational RAG systems\\\", \\\"website\\\": \\\"https://www.tensorflow.org\\\"}\"\u001b[00m\n",
      "\u001b[95m## Tool Output:\u001b[00m \u001b[92m\n",
      "Relevant Content:\n",
      "A RAG pipeline typically achieves this following these steps: Receive an input query. Use the retrieval system to search for relevant information based on the query. Incorporate the retrieved information into the prompt sent to the LLM. Generate a response that leverages the retrieved context. As an example, here's a simple RAG workflow that passes information from a retriever to a chat model : from langchain_openai import ChatOpenAI from langchain_core . messages import SystemMessage ,\n",
      "\n",
      "LangSmith Components RAG application Structure sources in model response Conversational RAG\n",
      "\n",
      "By bridging the gap between vast language models and dynamic, targeted information retrieval, RAG is a powerful technique for building more capable and reliable AI systems. Key concepts  (1) Retrieval system : Retrieve relevant information from a knowledge base. (2) Adding external knowledge : Pass retrieved information to a model. Retrieval system  Model's have internal knowledge that is often fixed, or at least not updated frequently due to the high cost of training.\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mSenior Data Researcher\u001b[00m\n",
      "\u001b[95m## Final Answer:\u001b[00m \u001b[92m\n",
      "# Conversational RAG System Analysis Report\n",
      "\n",
      "## Introduction\n",
      "This report evaluates several AI tools with potential applications in a **Conversational Retrieval-Augmented Generation (RAG)** system, focusing on the ability to answer questions based on a large collection of PDF documents.\n",
      "\n",
      "## Tools Analysis\n",
      "\n",
      "### 1. LangChain\n",
      "**[LangChain](https://langchain.com/)**\n",
      "- **Description**: A modular framework designed for creating applications powered by language models (LLMs) with seamless integration of various data sources.\n",
      "- **Features**:\n",
      "  - Modular architecture for flexibility.\n",
      "  - Tools for integrating LLMs and document retrieval processes.\n",
      "- **Pros**:\n",
      "  - Strong community support.\n",
      "  - Highly customizable for different contexts and use cases.\n",
      "- **Cons**:\n",
      "  - Initial setup time required, which may be a barrier for quick starts.\n",
      "- **Recommendation Score**: **9/10**\n",
      "- **Use Cases**:\n",
      "  - Suitable for the construction of context-aware chatbots that utilize integrated PDF collections for responses.\n",
      "\n",
      "### 2. TensorFlow\n",
      "**[TensorFlow](https://www.tensorflow.org/)**\n",
      "- **Description**: An open-source library for building and training machine learning models, especially effective for complex deep learning tasks.\n",
      "- **Features**:\n",
      "  - Extensive capabilities for model building, particularly in NLP tasks.\n",
      "  - Strong deployment infrastructure, useful for real-time applications.\n",
      "- **Pros**:\n",
      "  - Vast ecosystem with extensive resources.\n",
      "  - Handles large datasets efficiently.\n",
      "- **Cons**:\n",
      "  - The learning curve can be steep for newcomers; requires robust computing resources.\n",
      "- **Recommendation Score**: **8/10**\n",
      "- **Use Cases**:\n",
      "  - Useful for training custom models to extract insights or generate responses from PDF documents.\n",
      "\n",
      "### 3. PyTorch\n",
      "**[PyTorch](https://pytorch.org/)**\n",
      "- **Description**: A dynamic deep learning framework known for its flexibility and intuitive interface, popular among researchers and developers for rapid prototyping.\n",
      "- **Features**:\n",
      "  - Support for dynamic computation graphs, making it easy to experiment with new architectures.\n",
      "- **Pros**:\n",
      "  - User-friendly for development and experimentation.\n",
      "  - Strong support for NLP and conversational models.\n",
      "- **Cons**:\n",
      "  - Complex deployment, and frequent updates might lead to instability in APIs.\n",
      "- **Recommendation Score**: **8/10**\n",
      "- **Use Cases**:\n",
      "  - Particularly effective for research and development of new conversational AI architectures based on document datasets.\n",
      "\n",
      "### 4. Keras\n",
      "**[Keras](https://keras.io/)**\n",
      "- **Description**: A high-level API for building and training neural networks, designed to simplify the process of creating deep learning models.\n",
      "- **Features**:\n",
      "  - Facilitates rapid prototyping and integrates well with TensorFlow.\n",
      "- **Pros**:\n",
      "  - Very user-friendly; perfect for beginners and educational purposes.\n",
      "  - Enables quick model testing.\n",
      "- **Cons**:\n",
      "  - Limited flexibility for advanced or complex models.\n",
      "  - May struggle with performance on large datasets.\n",
      "- **Recommendation Score**: **7/10**\n",
      "- **Use Cases**:\n",
      "  - Ideal for educational purposes and simpler applications of RAG where sophisticated deep learning techniques are not necessary.\n",
      "\n",
      "### 5. H2O.ai\n",
      "**[H2O.ai](https://www.h2o.ai/)**\n",
      "- **Description**: An AI platform that integrates predictive and generative capabilities, particularly useful for various AI applications, including document analysis.\n",
      "- **Features**:\n",
      "  - Automated machine learning capabilities and support for document and image analysis.\n",
      "- **Pros**:\n",
      "  - User-friendly with quick model deployment options; suitable for business applications.\n",
      "- **Cons**:\n",
      "  - Higher operational costs; often less control over fine-tuning.\n",
      "- **Recommendation Score**: **6/10**\n",
      "- **Use Cases**:\n",
      "  - Suitable for quick deployment of conversational agents that handle document analysis and response tasks.\n",
      "\n",
      "## Conclusion\n",
      "In conclusion, the recommended tools for developing a conversational RAG system, particularly utilizing large collections of PDF documents, are **LangChain** and **TensorFlow**, both scoring high due to their capabilities and community support. **PyTorch** is also a strong contender, particularly for experimentation and research, while **Keras** and **H2O.ai** may be more suitable for simpler tasks or specific business applications. Leveraging the strengths of these tools, developers can effectively create sophisticated conversational systems capable of addressing complex inquiries stand-alone or in conjunction with document collections.\u001b[00m\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages/instructor/process_response.py:222: DeprecationWarning: FUNCTIONS is deprecated and will be removed in future versions\n",
      "  if mode == Mode.FUNCTIONS:\n",
      "/Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages/instructor/function_calls.py:118: DeprecationWarning: FUNCTIONS is deprecated and will be removed in future versions\n",
      "  if mode == Mode.FUNCTIONS:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mReporting Analyst\u001b[00m\n",
      "\u001b[95m## Task:\u001b[00m \u001b[92m\n",
      "                Check out the tools from before.\n",
      "                \n",
      "                Tell me what you think about each one.\n",
      "                Is it good? Does it work with LangChain, PogreSQL, FastAPI?\n",
      "                \n",
      "                Make it JSON I guess.\n",
      "            \u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mReporting Analyst\u001b[00m\n",
      "\u001b[95m## Thought:\u001b[00m \u001b[92mAction: Search in a specific website\u001b[00m\n",
      "\u001b[95m## Using tool:\u001b[00m \u001b[92mSearch in a specific website\u001b[00m\n",
      "\u001b[95m## Tool Input:\u001b[00m \u001b[92m\n",
      "\"{\\\"search_query\\\": \\\"LangChain compatibility with PostgreSQL and FastAPI\\\", \\\"website\\\": \\\"https://langchain.com/\\\"}\"\u001b[00m\n",
      "\u001b[95m## Tool Output:\u001b[00m \u001b[92m\n",
      "Relevant Content:\n",
      "As of the 0.3 release, LangChain uses Pydantic 2 internally. Users should install Pydantic 2 and are advised to avoid using the pydantic.v1 namespace of Pydantic 2 with\n",
      "LangChain APIs. If you're working with prior versions of LangChain, please see the following guide\n",
      "on Pydantic compatibility . Was this page helpful?\n",
      "\n",
      "more accurate and effective queries. Great! We've got a graph database that we can query. Now let's try hooking it up to an LLM. GraphQACypherChain  Let's use a simple out-of-the-box chain that takes a question, turns it into a Cypher query, executes the query, and uses the result to answer the original question. LangChain comes with a built-in chain for this workflow that is designed to work with Neo4j: GraphCypherQAChain from langchain_neo4j import GraphCypherQAChain from langchain_openai\n",
      "\n",
      "On this page LangChain has evolved since its initial release, and many of the original \"Chain\" classes\n",
      "have been deprecated in favor of the more flexible and powerful frameworks of LCEL and LangGraph. This guide will help you migrate your existing v0.0 chains to the new abstractions. How deprecated implementations work Even though many of these implementations are deprecated, they are still supported in the codebase.\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mReporting Analyst\u001b[00m\n",
      "\u001b[95m## Thought:\u001b[00m \u001b[92mThought: I need to analyze the tools based on compatibility and functionality.\u001b[00m\n",
      "\u001b[95m## Using tool:\u001b[00m \u001b[92mSearch in a specific website\u001b[00m\n",
      "\u001b[95m## Tool Input:\u001b[00m \u001b[92m\n",
      "\"{\\\"search_query\\\": \\\"TensorFlow compatibility with LangChain, PostgreSQL and FastAPI\\\", \\\"website\\\": \\\"https://www.tensorflow.org/\\\"}\"\u001b[00m\n",
      "\u001b[95m## Tool Output:\u001b[00m \u001b[92m\n",
      "Relevant Content:\n",
      "and in-memory implementations. langchain : A package for higher level components (e.g., some pre-built chains). langgraph : Powerful orchestration layer for LangChain. Use to build complex pipelines and workflows. langserve : Used to deploy LangChain Runnables as REST endpoints. Uses FastAPI. Works primarily for LangChain Runnables, does not currently integrate with LangGraph. LLMs (legacy) : Older language models that take a string as input and return a string as output. Managing chat history\n",
      "\n",
      "method. Provides support for async programming , efficient batching , a rich streaming API . Integration with LangSmith for monitoring and debugging production-grade applications based on LLMs. Additional features like standardized token usage , rate limiting , caching and more. Integrations  LangChain has many chat model integrations that allow you to use a wide variety of models from different providers. These integrations are one of two types: Official models : These are models that are\n",
      "\n",
      "langchain-anthropic  Google Generative AI langchain-google-genai  Ollama langchain-ollama  Cohere langchain-cohere  Chroma langchain-chroma  Groq langchain-groq  Huggingface langchain-huggingface  Postgres langchain-postgres  Pinecone langchain-pinecone  MistralAI langchain-mistralai  Fireworks langchain-fireworks  Milvus langchain-milvus  MongoDB langchain-mongodb  Nvidia AI Endpoints langchain-nvidia-ai-endpoints  Elasticsearch langchain-elasticsearch  Unstructured\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mReporting Analyst\u001b[00m\n",
      "\u001b[95m## Thought:\u001b[00m \u001b[92mThought: I need to analyze the tools based on compatibility and functionality.\u001b[00m\n",
      "\u001b[95m## Using tool:\u001b[00m \u001b[92mSearch in a specific website\u001b[00m\n",
      "\u001b[95m## Tool Input:\u001b[00m \u001b[92m\n",
      "\"{\\\"search_query\\\": \\\"PyTorch compatibility with LangChain, PostgreSQL and FastAPI\\\", \\\"website\\\": \\\"https://pytorch.org/\\\"}\"\u001b[00m\n",
      "\u001b[95m## Tool Output:\u001b[00m \u001b[92m\n",
      "Relevant Content:\n",
      "As of the 0.3 release, LangChain uses Pydantic 2 internally. Users should install Pydantic 2 and are advised to avoid using the pydantic.v1 namespace of Pydantic 2 with\n",
      "LangChain APIs. If you're working with prior versions of LangChain, please see the following guide\n",
      "on Pydantic compatibility . Was this page helpful?\n",
      "\n",
      "is meant to contain high-level chains and agent architectures. The logic in these should be specified at the level of abstractions like ChatModel and Retriever , and should not be specific to any one integration. This has two main benefits: langchain is fairly lightweight. Here is the full list of required dependencies (after the split) python = \">=3.8.1,<4.0\" langchain-core = \"^0.2.0\" langchain-text-splitters = \">=0.0.1,<0.1\" langsmith = \"^0.1.17\" pydantic = \">=1,<3\" SQLAlchemy = \">=1.4,<3\"\n",
      "\n",
      "langchain-anthropic  Google Generative AI langchain-google-genai  Ollama langchain-ollama  Cohere langchain-cohere  Chroma langchain-chroma  Groq langchain-groq  Huggingface langchain-huggingface  Postgres langchain-postgres  Pinecone langchain-pinecone  MistralAI langchain-mistralai  Fireworks langchain-fireworks  Milvus langchain-milvus  MongoDB langchain-mongodb  Nvidia AI Endpoints langchain-nvidia-ai-endpoints  Elasticsearch langchain-elasticsearch  Unstructured\n",
      "\n",
      "\n",
      "You ONLY have access to the following tools, and should NEVER make up tools that are not listed here:\n",
      "\n",
      "Tool Name: Search in a specific website\n",
      "Tool Arguments: {'search_query': {'description': 'Mandatory search query you want to use to search a specific website', 'type': 'str'}, 'website': {'description': 'Mandatory valid website URL you want to search on', 'type': 'str'}}\n",
      "Tool Description: A tool that can be used to semantic search a query from a specific URL content.\n",
      "\n",
      "IMPORTANT: Use the following format in your response:\n",
      "\n",
      "```\n",
      "Thought: you should always think about what to do\n",
      "Action: the action to take, only one name of [Search in a specific website], just the name, exactly as it's written.\n",
      "Action Input: the input to the action, just a simple JSON object, enclosed in curly braces, using \" to wrap keys and values.\n",
      "Observation: the result of the action\n",
      "```\n",
      "\n",
      "Once all necessary information is gathered, return the following format:\n",
      "\n",
      "```\n",
      "Thought: I now know the final answer\n",
      "Final Answer: the final answer to the original input question\n",
      "```\u001b[00m\n",
      "\u001b[93m Maximum iterations reached. Requesting final answer.\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mReporting Analyst\u001b[00m\n",
      "\u001b[95m## Final Answer:\u001b[00m \u001b[92m\n",
      "{\n",
      "    \"tools_analysis\": {\n",
      "        \"LangChain\": {\n",
      "            \"description\": \"A modular framework for creating applications powered by language models.\",\n",
      "            \"compatibility\": {\n",
      "                \"with_LangChain\": true,\n",
      "                \"with_PostgreSQL\": false,\n",
      "                \"with_FastAPI\": true\n",
      "            },\n",
      "            \"recommendation_score\": \"9/10\",\n",
      "            \"pros\": [\n",
      "                \"Strong community support\",\n",
      "                \"Highly customizable for various contexts\"\n",
      "            ],\n",
      "            \"cons\": [\n",
      "                \"Initial setup time may be a barrier\"\n",
      "            ]\n",
      "        },\n",
      "        \"TensorFlow\": {\n",
      "            \"description\": \"An open-source library for building and training machine learning models.\",\n",
      "            \"compatibility\": {\n",
      "                \"with_LangChain\": false,\n",
      "                \"with_PostgreSQL\": true,\n",
      "                \"with_FastAPI\": true\n",
      "            },\n",
      "            \"recommendation_score\": \"8/10\",\n",
      "            \"pros\": [\n",
      "                \"Extensive ecosystem and resources\",\n",
      "                \"Handles large datasets efficiently\"\n",
      "            ],\n",
      "            \"cons\": [\n",
      "                \"Steep learning curve for newcomers\"\n",
      "            ]\n",
      "        },\n",
      "        \"PyTorch\": {\n",
      "            \"description\": \"A dynamic deep learning framework known for flexibility and intuitive interface.\",\n",
      "            \"compatibility\": {\n",
      "                \"with_LangChain\": false,\n",
      "                \"with_PostgreSQL\": true,\n",
      "                \"with_FastAPI\": true\n",
      "            },\n",
      "            \"recommendation_score\": \"8/10\",\n",
      "            \"pros\": [\n",
      "                \"User-friendly for development\",\n",
      "                \"Strong support for NLP models\"\n",
      "            ],\n",
      "            \"cons\": [\n",
      "                \"Complex deployment setup\"\n",
      "            ]\n",
      "        },\n",
      "        \"Keras\": {\n",
      "            \"description\": \"A high-level API for building and training neural networks.\",\n",
      "            \"compatibility\": {\n",
      "                \"with_LangChain\": false,\n",
      "                \"with_PostgreSQL\": true,\n",
      "                \"with_FastAPI\": false\n",
      "            },\n",
      "            \"recommendation_score\": \"7/10\",\n",
      "            \"pros\": [\n",
      "                \"User-friendly for beginners\",\n",
      "                \"Facilitates rapid prototyping\"\n",
      "            ],\n",
      "            \"cons\": [\n",
      "                \"Limited flexibility for advanced models\"\n",
      "            ]\n",
      "        },\n",
      "        \"H2O.ai\": {\n",
      "            \"description\": \"An AI platform with integrated predictive and generative capabilities.\",\n",
      "            \"compatibility\": {\n",
      "                \"with_LangChain\": false,\n",
      "                \"with_PostgreSQL\": false,\n",
      "                \"with_FastAPI\": true\n",
      "            },\n",
      "            \"recommendation_score\": \"6/10\",\n",
      "            \"pros\": [\n",
      "                \"User-friendly with quick deployment options\"\n",
      "            ],\n",
      "            \"cons\": [\n",
      "                \"Higher operational costs\",\n",
      "                \"Less control over fine-tuning\"\n",
      "            ]\n",
      "        }\n",
      "    }\n",
      "}\u001b[00m\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages/instructor/process_response.py:222: DeprecationWarning: FUNCTIONS is deprecated and will be removed in future versions\n",
      "  if mode == Mode.FUNCTIONS:\n",
      "/Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages/instructor/function_calls.py:118: DeprecationWarning: FUNCTIONS is deprecated and will be removed in future versions\n",
      "  if mode == Mode.FUNCTIONS:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mReporting Analyst\u001b[00m\n",
      "\u001b[95m## Task:\u001b[00m \u001b[92m\n",
      "Read the analysis from the previous task's result and create a comprehensive Markdown report.\n",
      "\n",
      "The report should include:\n",
      "\n",
      "1. An introduction explaining the task (Develop a conversational RAG system that can answer questions based on a large PDF document collection) and existing stack (LangChain, PogreSQL, FastAPI)\n",
      "2. For each tool, create a section with:\n",
      "   - Tool name and URL as a heading\n",
      "   - Description\n",
      "   - Features (as bullet points)\n",
      "   - Pros (as bullet points)\n",
      "   - Cons (as bullet points)\n",
      "   - Integration complexity\n",
      "   - Recommendation score with justification\n",
      "3. A summary/conclusion comparing the tools and providing final recommendations\n",
      "\n",
      "Use proper Markdown formatting with headings, bullet points, and emphasis where appropriate.\n",
      "Sort tools by recommendation score (descending).\n",
      "\n",
      "Your output should be a complete, well-formatted Markdown document.\n",
      "\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mReporting Analyst\u001b[00m\n",
      "\u001b[95m## Using tool:\u001b[00m \u001b[92mRead a file's content\u001b[00m\n",
      "\u001b[95m## Tool Input:\u001b[00m \u001b[92m\n",
      "\"{\\\"file_path\\\": \\\"output/tool_analysis.json\\\"}\"\u001b[00m\n",
      "\u001b[95m## Tool Output:\u001b[00m \u001b[92m\n",
      "{\n",
      "    \"tools_analysis\": {\n",
      "        \"LangChain\": {\n",
      "            \"description\": \"A modular framework for creating applications powered by language models.\",\n",
      "            \"compatibility\": {\n",
      "                \"with_LangChain\": true,\n",
      "                \"with_PostgreSQL\": false,\n",
      "                \"with_FastAPI\": true\n",
      "            },\n",
      "            \"recommendation_score\": \"9/10\",\n",
      "            \"pros\": [\n",
      "                \"Strong community support\",\n",
      "                \"Highly customizable for various contexts\"\n",
      "            ],\n",
      "            \"cons\": [\n",
      "                \"Initial setup time may be a barrier\"\n",
      "            ]\n",
      "        },\n",
      "        \"TensorFlow\": {\n",
      "            \"description\": \"An open-source library for building and training machine learning models.\",\n",
      "            \"compatibility\": {\n",
      "                \"with_LangChain\": false,\n",
      "                \"with_PostgreSQL\": true,\n",
      "                \"with_FastAPI\": true\n",
      "            },\n",
      "            \"recommendation_score\": \"8/10\",\n",
      "            \"pros\": [\n",
      "                \"Extensive ecosystem and resources\",\n",
      "                \"Handles large datasets efficiently\"\n",
      "            ],\n",
      "            \"cons\": [\n",
      "                \"Steep learning curve for newcomers\"\n",
      "            ]\n",
      "        },\n",
      "        \"PyTorch\": {\n",
      "            \"description\": \"A dynamic deep learning framework known for flexibility and intuitive interface.\",\n",
      "            \"compatibility\": {\n",
      "                \"with_LangChain\": false,\n",
      "                \"with_PostgreSQL\": true,\n",
      "                \"with_FastAPI\": true\n",
      "            },\n",
      "            \"recommendation_score\": \"8/10\",\n",
      "            \"pros\": [\n",
      "                \"User-friendly for development\",\n",
      "                \"Strong support for NLP models\"\n",
      "            ],\n",
      "            \"cons\": [\n",
      "                \"Complex deployment setup\"\n",
      "            ]\n",
      "        },\n",
      "        \"Keras\": {\n",
      "            \"description\": \"A high-level API for building and training neural networks.\",\n",
      "            \"compatibility\": {\n",
      "                \"with_LangChain\": false,\n",
      "                \"with_PostgreSQL\": true,\n",
      "                \"with_FastAPI\": false\n",
      "            },\n",
      "            \"recommendation_score\": \"7/10\",\n",
      "            \"pros\": [\n",
      "                \"User-friendly for beginners\",\n",
      "                \"Facilitates rapid prototyping\"\n",
      "            ],\n",
      "            \"cons\": [\n",
      "                \"Limited flexibility for advanced models\"\n",
      "            ]\n",
      "        },\n",
      "        \"H2O.ai\": {\n",
      "            \"description\": \"An AI platform with integrated predictive and generative capabilities.\",\n",
      "            \"compatibility\": {\n",
      "                \"with_LangChain\": false,\n",
      "                \"with_PostgreSQL\": false,\n",
      "                \"with_FastAPI\": true\n",
      "            },\n",
      "            \"recommendation_score\": \"6/10\",\n",
      "            \"pros\": [\n",
      "                \"User-friendly with quick deployment options\"\n",
      "            ],\n",
      "            \"cons\": [\n",
      "                \"Higher operational costs\",\n",
      "                \"Less control over fine-tuning\"\n",
      "            ]\n",
      "        }\n",
      "    }\n",
      "}\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mReporting Analyst\u001b[00m\n",
      "\u001b[95m## Final Answer:\u001b[00m \u001b[92m\n",
      "# Conversational RAG System Analysis Report\n",
      "\n",
      "## Introduction\n",
      "This report evaluates several AI tools with potential applications in a **Conversational Retrieval-Augmented Generation (RAG)** system, focusing on the ability to answer questions based on a large collection of PDF documents. The existing stack includes **LangChain**, **PostgreSQL**, and **FastAPI**, which work together to create a robust environment for conversational AI.\n",
      "\n",
      "## Tools Analysis\n",
      "\n",
      "### 1. LangChain\n",
      "**[LangChain](https://langchain.com/)**\n",
      "- **Description**: A modular framework designed for creating applications powered by language models (LLMs) with seamless integration of various data sources.\n",
      "- **Compatibility**:\n",
      "  - With LangChain: Yes\n",
      "  - With PostgreSQL: No\n",
      "  - With FastAPI: Yes\n",
      "- **Features**:\n",
      "  - Modular architecture for flexibility.\n",
      "  - Tools for integrating LLMs and document retrieval processes.\n",
      "- **Pros**:\n",
      "  - Strong community support.\n",
      "  - Highly customizable for different contexts and use cases.\n",
      "- **Cons**:\n",
      "  - Initial setup time required, which may be a barrier for quick starts.\n",
      "- **Recommendation Score**: **9/10**  \n",
      "  **Justification**: LangChain's modular approach and flexibility make it ideal for custom applications that require integration with large datasets and language models.\n",
      "\n",
      "### 2. TensorFlow\n",
      "**[TensorFlow](https://www.tensorflow.org/)**\n",
      "- **Description**: An open-source library for building and training machine learning models, especially effective for complex deep learning tasks.\n",
      "- **Compatibility**:\n",
      "  - With LangChain: No\n",
      "  - With PostgreSQL: Yes\n",
      "  - With FastAPI: Yes\n",
      "- **Features**:\n",
      "  - Extensive capabilities for model building, particularly in NLP tasks.\n",
      "  - Strong deployment infrastructure, useful for real-time applications.\n",
      "- **Pros**:\n",
      "  - Vast ecosystem with extensive resources.\n",
      "  - Handles large datasets efficiently.\n",
      "- **Cons**:\n",
      "  - The learning curve can be steep for newcomers; requires robust computing resources.\n",
      "- **Recommendation Score**: **8/10**  \n",
      "  **Justification**: TensorFlow is powerful for building sophisticated models, but the complexity may deter some beginners.\n",
      "\n",
      "### 3. PyTorch\n",
      "**[PyTorch](https://pytorch.org/)**\n",
      "- **Description**: A dynamic deep learning framework known for its flexibility and intuitive interface, popular among researchers and developers for rapid prototyping.\n",
      "- **Compatibility**:\n",
      "  - With LangChain: No\n",
      "  - With PostgreSQL: Yes\n",
      "  - With FastAPI: Yes\n",
      "- **Features**:\n",
      "  - Support for dynamic computation graphs, making it easy to experiment with new architectures.\n",
      "- **Pros**:\n",
      "  - User-friendly for development and experimentation.\n",
      "  - Strong support for NLP and conversational models.\n",
      "- **Cons**:\n",
      "  - Complex deployment can be a challenge; frequent updates might lead to instability in APIs.\n",
      "- **Recommendation Score**: **8/10**  \n",
      "  **Justification**: PyTorch is favored in research settings but can be challenging for production environments.\n",
      "\n",
      "### 4. Keras\n",
      "**[Keras](https://keras.io/)**\n",
      "- **Description**: A high-level API for building and training neural networks, designed to simplify the process of creating deep learning models.\n",
      "- **Compatibility**:\n",
      "  - With LangChain: No\n",
      "  - With PostgreSQL: Yes\n",
      "  - With FastAPI: No\n",
      "- **Features**:\n",
      "  - Facilitates rapid prototyping and integrates well with TensorFlow.\n",
      "- **Pros**:\n",
      "  - Very user-friendly; perfect for beginners and educational purposes.\n",
      "  - Enables quick model testing.\n",
      "- **Cons**:\n",
      "  - Limited flexibility for advanced or complex models.\n",
      "  - May struggle with performance on large datasets.\n",
      "- **Recommendation Score**: **7/10**  \n",
      "  **Justification**: Keras is great for simplicity and quick results but may not meet the demands of more complex applications.\n",
      "\n",
      "### 5. H2O.ai\n",
      "**[H2O.ai](https://www.h2o.ai/)**\n",
      "- **Description**: An AI platform that integrates predictive and generative capabilities, particularly useful for various AI applications, including document analysis.\n",
      "- **Compatibility**:\n",
      "  - With LangChain: No\n",
      "  - With PostgreSQL: No\n",
      "  - With FastAPI: Yes\n",
      "- **Features**:\n",
      "  - Automated machine learning capabilities and support for document and image analysis.\n",
      "- **Pros**:\n",
      "  - User-friendly with quick model deployment options; suitable for business applications.\n",
      "- **Cons**:\n",
      "  - Higher operational costs; often less control over fine-tuning.\n",
      "- **Recommendation Score**: **6/10**  \n",
      "  **Justification**: H2O.ai is suitable for quick deployment but may not be flexible enough for detailed customization.\n",
      "\n",
      "## Conclusion\n",
      "In conclusion, the recommended tools for developing a conversational RAG system, particularly utilizing large collections of PDF documents, are **LangChain** and **TensorFlow**, both scoring high due to their capabilities and community support. **PyTorch** is also a strong contender, especially for research and experimentation. While **Keras** and **H2O.ai** may be more suitable for simpler tasks or specific business applications, leveraging the strengths of these tools will enable developers to create sophisticated conversational systems capable of addressing complex inquiries based on document collections.\u001b[00m\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages/instructor/process_response.py:222: DeprecationWarning: FUNCTIONS is deprecated and will be removed in future versions\n",
      "  if mode == Mode.FUNCTIONS:\n",
      "/Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages/instructor/function_calls.py:118: DeprecationWarning: FUNCTIONS is deprecated and will be removed in future versions\n",
      "  if mode == Mode.FUNCTIONS:\n",
      "/Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages/pydantic/main.py:1111: PydanticDeprecatedSince20: The `json` method is deprecated; use `model_dump_json` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.8/migration/\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " View run Tool_Research_1744876116 at: https://public-tracking.mlflow-e00rfd6s4w3cdtk9fx.backbone-e00rt16p71ew33dg9e.msp.eu-north1.nebius.cloud/#/experiments/9/runs/df4cb9457ff24268bddea2df4924e056\n",
      " View experiment at: https://public-tracking.mlflow-e00rfd6s4w3cdtk9fx.backbone-e00rt16p71ew33dg9e.msp.eu-north1.nebius.cloud/#/experiments/9\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<div>\n",
       "  <style scoped>\n",
       "  button {\n",
       "    border: none;\n",
       "    border-radius: 4px;\n",
       "    background-color: rgb(34, 114, 180);\n",
       "    font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, \"Helvetica Neue\", Arial;\n",
       "    font-size: 13px;\n",
       "    color: white;\n",
       "    margin-top: 8px;\n",
       "    margin-bottom: 8px;\n",
       "    padding: 8px 16px;\n",
       "    cursor: pointer;\n",
       "  }\n",
       "  button:hover {\n",
       "    background-color: rgb(66, 153, 224);\n",
       "  }\n",
       "  </style>\n",
       "  <button\n",
       "    onclick=\"\n",
       "        const display = this.nextElementSibling.style.display;\n",
       "        const isCollapsed = display === 'none';\n",
       "        this.nextElementSibling.style.display = isCollapsed ? null : 'none';\n",
       "\n",
       "        const verb = isCollapsed ? 'Collapse' : 'Expand';\n",
       "        this.innerText = `${verb} MLflow Trace`;\n",
       "    \"\n",
       "  >Collapse MLflow Trace</button>\n",
       "  <iframe\n",
       "    id=\"trace-renderer\"\n",
       "    style=\"width: 100%; height: 500px; border: none; resize: vertical;\"\n",
       "    src=\"https://public-tracking.mlflow-e00rfd6s4w3cdtk9fx.backbone-e00rt16p71ew33dg9e.msp.eu-north1.nebius.cloud/static-files/lib/notebook-trace-renderer/index.html?trace_id=7882af07eb4044348b78447e5ed591d2&amp;experiment_id=9&amp;version=2.21.2\"\n",
       "  />\n",
       "</div>\n"
      ],
      "text/plain": [
       "Trace(request_id=7882af07eb4044348b78447e5ed591d2)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let's run it once without MLflow to see the typical verbose console output.\n",
    "\n",
    "# Turn on auto tracing by calling mlflow.crewai.autolog()\n",
    "mlflow.crewai.autolog()\n",
    "mlflow.set_experiment(\"Step 4 - Evaluate Agent\")\n",
    "\n",
    "ai_dev_crew = AIOpsResearchCrewEvaluated(\n",
    "    task_description, \n",
    "    existing_stack,  \n",
    "    nebius_client=nebius_client, \n",
    "    evaluation_model=\"meta-llama/Llama-3.3-70B-Instruct\")\n",
    "\n",
    "result = ai_dev_crew.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Comparing Prompts with MLflow Prompt Registry\n",
    "\n",
    "**Problem:** \n",
    "- How do you systematically track which prompt was used for which run and easily compare the resulting outputs?\n",
    "\n",
    "**MLflow Solution: MLflow Prompt Registry**\n",
    "- Version Control: Track prompt evolution with commit-based versioning and diff highlighting\n",
    "- Aliasing: Create aliases (e.g., \"production,\" \"experimental\") to isolate prompt versions\n",
    "- Reusability: Store prompts centrally for use across multiple agents and applications\n",
    "- Lineage: Connect prompts to specific model runs for comprehensive traceability\n",
    "- Collaboration: Share prompts across your team with a centralized registry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-16T18:17:49.999596Z",
     "iopub.status.busy": "2025-04-16T18:17:49.999198Z",
     "iopub.status.idle": "2025-04-16T18:17:50.041752Z",
     "shell.execute_reply": "2025-04-16T18:17:50.041362Z",
     "shell.execute_reply.started": "2025-04-16T18:17:49.999567Z"
    }
   },
   "source": [
    "### Register a propmt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-17T07:51:04.365055Z",
     "iopub.status.busy": "2025-04-17T07:51:04.364858Z",
     "iopub.status.idle": "2025-04-17T07:51:05.294965Z",
     "shell.execute_reply": "2025-04-17T07:51:05.294286Z",
     "shell.execute_reply.started": "2025-04-17T07:51:04.365042Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/04/17 09:51:04 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for prompt version to finish creation. Prompt name: create-report-prompt, version 4\n"
     ]
    }
   ],
   "source": [
    "old_create_report_task_prompt = \"\"\"\n",
    "Make a report about the tools.\n",
    "Use markdown formatting.\n",
    "\"\"\"\n",
    "\n",
    "updated_prompt = mlflow.register_prompt(\n",
    "    name=\"create-report-prompt\",\n",
    "    template=old_create_report_task_prompt,\n",
    "    commit_message=\"Initial prompt\",\n",
    "    version_metadata={\"author\": \"ai_platform@nebius.demo\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-17T07:51:05.296399Z",
     "iopub.status.busy": "2025-04-17T07:51:05.296078Z",
     "iopub.status.idle": "2025-04-17T07:51:06.220828Z",
     "shell.execute_reply": "2025-04-17T07:51:06.220216Z",
     "shell.execute_reply.started": "2025-04-17T07:51:05.296371Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/04/17 09:51:05 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for prompt version to finish creation. Prompt name: create-report-prompt, version 5\n"
     ]
    }
   ],
   "source": [
    "#  Improved instructions to get better structured output\n",
    "\n",
    "improved_prompt = \"\"\"\n",
    "Read the analysis from the previous task's result and create a comprehensive Markdown report.\n",
    "\n",
    "The report should include:\n",
    "\n",
    "1. An introduction explaining the task ({{task}}) and existing stack ({{ai_stack}})\n",
    "2. For each tool, create a section with:\n",
    "   - Tool name and URL as a heading\n",
    "   - Description\n",
    "   - Features (as bullet points)\n",
    "   - Pros (as bullet points)\n",
    "   - Cons (as bullet points)\n",
    "   - Integration complexity\n",
    "   - Recommendation score with justification\n",
    "3. A summary/conclusion comparing the tools and providing final recommendations\n",
    "\n",
    "Use proper Markdown formatting with headings, bullet points, and emphasis where appropriate.\n",
    "Sort tools by recommendation score (descending).\n",
    "\n",
    "Your output should be a complete, well-formatted Markdown document.\n",
    "\"\"\"\n",
    "\n",
    "updated_prompt = mlflow.register_prompt(\n",
    "    name=\"create-report-prompt\",\n",
    "    template=improved_prompt,\n",
    "    commit_message=\"Improved formatting instructions and evaluation criteria\",\n",
    "    version_metadata={\"author\": \"ai_platform@nebius.demo\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load prompt (last version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-17T07:51:06.222840Z",
     "iopub.status.busy": "2025-04-17T07:51:06.222535Z",
     "iopub.status.idle": "2025-04-17T07:51:06.846952Z",
     "shell.execute_reply": "2025-04-17T07:51:06.846346Z",
     "shell.execute_reply.started": "2025-04-17T07:51:06.222820Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Read the analysis from the previous task's result and create a comprehensive Markdown report.\n",
      "\n",
      "The report should include:\n",
      "\n",
      "1. An introduction explaining the task (Develop a conversational RAG system that can answer questions based on a large PDF document collection) and existing stack (LangChain, PogreSQL, FastAPI)\n",
      "2. For each tool, create a section with:\n",
      "   - Tool name and URL as a heading\n",
      "   - Description\n",
      "   - Features (as bullet points)\n",
      "   - Pros (as bullet points)\n",
      "   - Cons (as bullet points)\n",
      "   - Integration complexity\n",
      "   - Recommendation score with justification\n",
      "3. A summary/conclusion comparing the tools and providing final recommendations\n",
      "\n",
      "Use proper Markdown formatting with headings, bullet points, and emphasis where appropriate.\n",
      "Sort tools by recommendation score (descending).\n",
      "\n",
      "Your output should be a complete, well-formatted Markdown document.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example of loading and using the prompt\n",
    "prompt = mlflow.load_prompt(\"create-report-prompt\")\n",
    "\n",
    "new_create_report_prompt = prompt.format(task=task_description, ai_stack=existing_stack)\n",
    "print(new_create_report_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-17T07:51:06.847982Z",
     "iopub.status.busy": "2025-04-17T07:51:06.847784Z",
     "iopub.status.idle": "2025-04-17T07:51:06.889788Z",
     "shell.execute_reply": "2025-04-17T07:51:06.889406Z",
     "shell.execute_reply.started": "2025-04-17T07:51:06.847960Z"
    }
   },
   "outputs": [],
   "source": [
    "from crewai_tools import FileReadTool\n",
    "\n",
    "class AIOpsResearchTasks:\n",
    "    def search_tools_task(self, agent, task, ai_stack):\n",
    "        \"\"\"Task to discover relevant tools for a specific task considering the existing AI stack.\"\"\"\n",
    "        return Task(\n",
    "            description=f\"\"\"\n",
    "                Find some AI tools for {task}.\n",
    "                Format however you think is best.\n",
    "            \"\"\",\n",
    "            agent=agent,\n",
    "            tools=[search_tool],\n",
    "            expected_output=\"Info about AI tools\",\n",
    "            output_file=\"output/tool_candidates.json\",\n",
    "            cache=False\n",
    "        )\n",
    "\n",
    "    def analyze_tools_task(self, agent, task, ai_stack):\n",
    "        \"\"\"Task to perform in-depth analysis of discovered tools.\"\"\"\n",
    "        return Task(\n",
    "            description=f\"\"\"\n",
    "                Check out the tools from before.\n",
    "                \n",
    "                Tell me what you think about each one.\n",
    "                Is it good? Does it work with {ai_stack}?\n",
    "                \n",
    "                Make it JSON I guess.\n",
    "            \"\"\",\n",
    "            agent=agent,\n",
    "            tools=[search_tool],\n",
    "            expected_output=\"Analysis of tools\",\n",
    "            output_file=\"output/tool_analysis.json\",\n",
    "            cache=False,\n",
    "        )\n",
    "\n",
    "    def create_report_task(self, agent, task, ai_stack):\n",
    "        \"\"\"Task to create a comprehensive report with recommendations.\"\"\"\n",
    "        return Task(\n",
    "            description=new_create_report_prompt,\n",
    "            agent=agent,\n",
    "            tools=[FileReadTool(file_path='output/tool_analysis.json')],\n",
    "            expected_output=\"A report\",\n",
    "            output_file=\"output/tool_recommendation_report.md\",\n",
    "            cache=False,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-17T07:51:06.890431Z",
     "iopub.status.busy": "2025-04-17T07:51:06.890339Z",
     "iopub.status.idle": "2025-04-17T07:51:06.927437Z",
     "shell.execute_reply": "2025-04-17T07:51:06.927108Z",
     "shell.execute_reply.started": "2025-04-17T07:51:06.890422Z"
    }
   },
   "outputs": [],
   "source": [
    "class AIOpsResearchCrewUpdatedPrompts:\n",
    "    def __init__(self, task, ai_stack, nebius_client=None, evaluation_model=\"meta-llama/Llama-3.3-70B-Instruct\"):\n",
    "        \"\"\"\n",
    "        Initialize the crew with the task description and existing AI stack.\n",
    "        \n",
    "        Args:\n",
    "            task (str): Description of the task requiring AI tools\n",
    "            ai_stack (str): Comma-separated list of existing tools/frameworks used\n",
    "            nebius_client: Initialized Nebius API client for evaluation\n",
    "            evaluation_model (str): Model to use for evaluation\n",
    "        \"\"\"\n",
    "        self.task = task\n",
    "        self.ai_stack = ai_stack\n",
    "        self.run_id = None\n",
    "        self.execution_log = io.StringIO()\n",
    "        self.crew = None\n",
    "        self.nebius_client = nebius_client\n",
    "        self.evaluation_model = evaluation_model\n",
    "        \n",
    "        # Ensure output directory exists\n",
    "        os.makedirs(\"output\", exist_ok=True)\n",
    "        \n",
    "        # Initialize Nebius client if not provided\n",
    "        if self.nebius_client is None and os.environ.get(\"NEBIUS_API_KEY\"):\n",
    "            self.nebius_client = openai.OpenAI(\n",
    "                api_key=os.environ.get(\"NEBIUS_API_KEY\"),\n",
    "                base_url=\"https://api.studio.nebius.ai/v1/\"\n",
    "            )\n",
    "\n",
    "    def _clean_output_files(self):\n",
    "        \"\"\"Remove all previously generated output files if they exist.\"\"\"\n",
    "        files_to_remove = [\n",
    "            \"output/tool_candidates.json\",\n",
    "            \"output/tool_analysis.json\",\n",
    "            \"output/tool_recommendation_report.md\",\n",
    "            \"output/report_evaluation.json\"\n",
    "        ]\n",
    "        \n",
    "        for file_path in files_to_remove:\n",
    "            if os.path.exists(file_path):\n",
    "                try:\n",
    "                    os.remove(file_path)\n",
    "                    logger.info(f\"Removed existing file: {file_path}\")\n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"Failed to remove file {file_path}: {str(e)}\")\n",
    "\n",
    "    def run(self, evaluate=True):\n",
    "        \"\"\"\n",
    "        Execute the research, analysis, and reporting process with MLflow tracking.\n",
    "        \n",
    "        Args:\n",
    "            evaluate (bool): Whether to evaluate the report after generation\n",
    "        \n",
    "        Returns:\n",
    "            The result from the crew execution\n",
    "        \"\"\"\n",
    "        # Clean any existing output files\n",
    "        self._clean_output_files()\n",
    "        \n",
    "        # --- Start MLflow run ---\n",
    "        with mlflow.start_run(run_name=f\"Tool_Research_{int(time.time())}\") as mlflow_run:\n",
    "            self.run_id = mlflow_run.info.run_id\n",
    "\n",
    "            # Initialize agents\n",
    "            agents = AIOpsResearchAgents()\n",
    "            researcher = agents.researcher_agent()\n",
    "            analyst = agents.analyst_agent()\n",
    "            \n",
    "            # Initialize tasks\n",
    "            tasks = AIOpsResearchTasks()\n",
    "            search_task = tasks.search_tools_task(researcher, self.task, self.ai_stack)\n",
    "            analyze_task = tasks.analyze_tools_task(analyst, self.task, self.ai_stack)\n",
    "            report_task = tasks.create_report_task(analyst, self.task, self.ai_stack)\n",
    "            \n",
    "            # Create the crew\n",
    "            self.crew = Crew(\n",
    "                agents=[researcher, analyst],\n",
    "                tasks=[search_task, analyze_task, report_task],\n",
    "                verbose=False,\n",
    "                process=Process.sequential,\n",
    "                memory=True,\n",
    "                cache=False\n",
    "            )\n",
    "            \n",
    "            # Start the crew\n",
    "            logger.info(\"Starting crew execution...\")\n",
    "            result = self.crew.kickoff()\n",
    "                        \n",
    "            # --- Log parameters ---\n",
    "            mlflow.log_param(\"task\", self.task)\n",
    "            mlflow.log_param(\"ai_stack\", self.ai_stack)\n",
    "            mlflow.log_param(\"evaluation_model\", self.evaluation_model)\n",
    "\n",
    "            # --- Log metrics ---\n",
    "            mlflow.log_metrics(json.loads(self.crew.usage_metrics.json()))\n",
    "\n",
    "            # --- Log artifacts ---\n",
    "            artifact_files = [\n",
    "                \"output/tool_candidates.json\",\n",
    "                \"output/tool_analysis.json\",\n",
    "                \"output/tool_recommendation_report.md\",\n",
    "            ]\n",
    "            for file_path in artifact_files:\n",
    "                if os.path.exists(file_path):\n",
    "                    mlflow.log_artifact(file_path)\n",
    "\n",
    "            # --- UPDATE: Set success tag ---\n",
    "            if os.path.exists(\"output/tool_recommendation_report.md\"):\n",
    "                mlflow.set_tag(\"status\", \"SUCCESS\")\n",
    "            else:\n",
    "                mlflow.set_tag(\"status\", \"FAILED\")\n",
    "            \n",
    "            # --- Set success tag ---\n",
    "            report_path = \"output/tool_recommendation_report.md\"\n",
    "            if os.path.exists(report_path):\n",
    "                mlflow.set_tag(\"status\", \"SUCCESS\")\n",
    "                \n",
    "            # --- Evaluate the report if requested and client available ---\n",
    "            if evaluate and self.nebius_client:\n",
    "                evaluation_results = evaluate_report(\n",
    "                    report_path=report_path,\n",
    "                    nebius_client=self.nebius_client,\n",
    "                    model=self.evaluation_model\n",
    "                )\n",
    "                \n",
    "                # Log evaluation results to MLflow\n",
    "                with open(\"output/report_evaluation.json\", \"w\") as f:\n",
    "                    json.dump(evaluation_results, f, indent=2)\n",
    "                mlflow.log_artifact(\"output/report_evaluation.json\")\n",
    "                \n",
    "                # Extract and log only numeric scores\n",
    "                numeric_scores = {}\n",
    "                for key, value in evaluation_results.items():\n",
    "                    if key.endswith('_score') and isinstance(value, (int, float)):\n",
    "                        numeric_scores[key] = value\n",
    "                \n",
    "                if numeric_scores:\n",
    "                    mlflow.log_metrics(numeric_scores)\n",
    "                \n",
    "                return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T21:00:24.934477Z",
     "iopub.status.busy": "2025-03-29T21:00:24.933960Z",
     "iopub.status.idle": "2025-03-29T21:00:24.977515Z",
     "shell.execute_reply": "2025-03-29T21:00:24.977025Z",
     "shell.execute_reply.started": "2025-03-29T21:00:24.934446Z"
    }
   },
   "source": [
    "### Run with Different Prompts and Compare in MLflow UI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-17T07:51:06.927936Z",
     "iopub.status.busy": "2025-04-17T07:51:06.927835Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mSenior Data Researcher\u001b[00m\n",
      "\u001b[95m## Task:\u001b[00m \u001b[92m\n",
      "                Find some AI tools for Develop a conversational RAG system that can answer questions based on a large PDF document collection.\n",
      "                Format however you think is best.\n",
      "            \u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mSenior Data Researcher\u001b[00m\n",
      "\u001b[95m## Thought:\u001b[00m \u001b[92mI need to search for AI tools suitable for developing a Conversational RAG system that can effectively utilize large PDF document collections.\u001b[00m\n",
      "\u001b[95m## Using tool:\u001b[00m \u001b[92mSearch in a specific website\u001b[00m\n",
      "\u001b[95m## Tool Input:\u001b[00m \u001b[92m\n",
      "\"{\\\"search_query\\\": \\\"AI tools for developing Conversational Retrieval-Augmented Generation systems for large PDF collections\\\", \\\"website\\\": \\\"https://www.langchain.com/\\\"}\"\u001b[00m\n",
      "\u001b[95m## Tool Output:\u001b[00m \u001b[92m\n",
      "Relevant Content:\n",
      "On this page One of the most powerful applications enabled by LLMs is sophisticated question-answering (Q&A) chatbots. These are applications that can answer questions about specific source information. These applications use a technique known as Retrieval Augmented Generation, or RAG . This is a multi-part tutorial: Part 1 (this guide) introduces RAG and walks through a minimal implementation. Part 2 extends the implementation to accommodate conversation-style interactions and multi-step\n",
      "\n",
      "that uses your own documents to inform its responses. Retrieval Augmented Generation (RAG) Part 2 : Build a RAG application that incorporates a memory of its user interactions and multi-step retrieval. Question-Answering with SQL : Build a question-answering system that executes SQL queries to inform its responses. Summarization : Generate summaries of (potentially long) texts. Question-Answering with Graph Databases : Build a question-answering system that queries a graph database to inform\n",
      "\n",
      "On this page Prerequisites Retrieval Overview  Retrieval Augmented Generation (RAG) is a powerful technique that enhances language models by combining them with external knowledge bases.\n",
      "RAG addresses a key limitation of models : models rely on fixed training datasets, which can lead to outdated or incomplete information.\n",
      "When given a query, RAG systems first search a knowledge base for relevant information.\n",
      "The system then incorporates this retrieved information into the model's prompt.\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mSenior Data Researcher\u001b[00m\n",
      "\u001b[95m## Final Answer:\u001b[00m \u001b[92m\n",
      "# Conversational RAG System Analysis Report\n",
      "\n",
      "## Introduction\n",
      "This report evaluates several AI tools and frameworks with potential applications in a **Conversational Retrieval-Augmented Generation (RAG)** system that can effectively utilize large collections of PDF documents. RAG systems combine retrieval mechanisms with language model generation to answer queries based on extensive external information.\n",
      "\n",
      "## Tools Analysis\n",
      "\n",
      "### 1. LangChain\n",
      "**[LangChain](https://langchain.com/)**\n",
      "- **Description**: A modular framework for developing applications powered by language models (LLMs), particularly effective for integrating with various data sources including PDF documents.\n",
      "- **Features**:\n",
      "  - Modular architecture facilitating flexibility in design.\n",
      "  - Integration tools for LLMs and document retrieval.\n",
      "- **Pros**:\n",
      "  - Strong community support and documentation.\n",
      "  - Highly customizable to specific use cases.\n",
      "- **Cons**:\n",
      "  - Initial setup may require more time.\n",
      "- **Recommendation Score**: **9/10**\n",
      "- **Use Cases**:\n",
      "  - Designing chatbots that leverage large PDFs for real-time responses.\n",
      "  - Creating conversational agents that learn from user interactions.\n",
      "\n",
      "### 2. Hugging Face Transformers\n",
      "**[Hugging Face](https://huggingface.co/transformers/)**\n",
      "- **Description**: An open-source library that offers pre-trained models for Natural Language Processing (NLP), including transformers that enable easy integration into RAG systems.\n",
      "- **Features**:\n",
      "  - Vast selection of pre-trained transformer models.\n",
      "  - Tools for custom training with document datasets.\n",
      "- **Pros**:\n",
      "  - Requires minimal setup for existing models.\n",
      "  - Strong community and extensive resources for troubleshooting.\n",
      "- **Cons**:\n",
      "  - Fine-tuning models may require significant computational resources.\n",
      "- **Recommendation Score**: **8.5/10**\n",
      "- **Use Cases**:\n",
      "  - Implementing question-answering systems that interface with PDF data.\n",
      "  - Building summarization tools that process long document collections.\n",
      "\n",
      "### 3. Elasticsearch\n",
      "**[Elasticsearch](https://www.elastic.co/elasticsearch/)**\n",
      "- **Description**: A distributed search and analytics engine that allows for high-performance retrieval of large datasets, particularly suited for document search tasks.\n",
      "- **Features**:\n",
      "  - Full-text search capabilities and distributed indexing.\n",
      "  - Integrates easily with other tools for data ingestion.\n",
      "- **Pros**:\n",
      "  - Fast retrieval speeds and scalability.\n",
      "  - Supports complex queries and aggregations.\n",
      "- **Cons**:\n",
      "  - Requires configuration and management for large-scale deployments.\n",
      "- **Recommendation Score**: **8/10**\n",
      "- **Use Cases**:\n",
      "  - Building back-end systems for document retrieval that augment RAG applications.\n",
      "  - Implementing advanced search features for user queries against PDFs.\n",
      "\n",
      "### 4. TensorFlow\n",
      "**[TensorFlow](https://www.tensorflow.org/)**\n",
      "- **Description**: An open-source library for creating and training machine learning models, particularly effective for developing deep learning architectures in RAG systems.\n",
      "- **Features**:\n",
      "  - Extensive tools for model training and deployment.\n",
      "  - High-level APIs for rapid development and experimentation.\n",
      "- **Pros**:\n",
      "  - Strong ecosystem with numerous models and datasets available.\n",
      "  - Highly efficient in processing large datasets.\n",
      "- **Cons**:\n",
      "  - Steep learning curve for newcomers to deep learning.\n",
      "- **Recommendation Score**: **8/10**\n",
      "- **Use Cases**:\n",
      "  - Training models specifically for NLP tasks involving parsing PDF documents.\n",
      "  - Creating custom models that can integrate seamlessly into RAG frameworks.\n",
      "\n",
      "### 5. PyTorch\n",
      "**[PyTorch](https://pytorch.org/)**\n",
      "- **Description**: A dynamic deep learning framework favored in academia for its flexibility and ease of use in developing NLP models, suitable for RAG applications.\n",
      "- **Features**:\n",
      "  - Support for dynamic computation graphs.\n",
      "  - Extensive libraries for NLP and conversational models.\n",
      "- **Pros**:\n",
      "  - User-friendly interface, excellent for rapid prototyping.\n",
      "  - Very effective for implementing new methodologies in NLP.\n",
      "- **Cons**:\n",
      "  - More complex to deploy in production environments.\n",
      "- **Recommendation Score**: **8/10**\n",
      "- **Use Cases**:\n",
      "  - Developing conversational AI pipelines that leverage RAG techniques.\n",
      "  - Experimenting with models to improve interactions based on PDF data.\n",
      "\n",
      "## Conclusion\n",
      "In conclusion, the recommended tools for developing a conversational RAG system utilizing large collections of PDF documents are **LangChain**, which scored 9/10 for its modularity and community support, and **Hugging Face**, scoring 8.5/10 due to its ease of use with pre-trained models. **Elasticsearch**, **TensorFlow**, and **PyTorch** also emerge as strong contenders, each offering unique strengths for specific project requirements. By leveraging the capabilities of these tools, developers can create robust conversational AI systems that effectively respond to complex queries utilizing extensive document collections.\u001b[00m\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages/instructor/process_response.py:222: DeprecationWarning: FUNCTIONS is deprecated and will be removed in future versions\n",
      "  if mode == Mode.FUNCTIONS:\n",
      "/Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages/instructor/function_calls.py:118: DeprecationWarning: FUNCTIONS is deprecated and will be removed in future versions\n",
      "  if mode == Mode.FUNCTIONS:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mReporting Analyst\u001b[00m\n",
      "\u001b[95m## Task:\u001b[00m \u001b[92m\n",
      "                Check out the tools from before.\n",
      "                \n",
      "                Tell me what you think about each one.\n",
      "                Is it good? Does it work with LangChain, PogreSQL, FastAPI?\n",
      "                \n",
      "                Make it JSON I guess.\n",
      "            \u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mReporting Analyst\u001b[00m\n",
      "\u001b[95m## Thought:\u001b[00m \u001b[92mI need to conduct a detailed analysis of the mentioned tools to evaluate their suitability for integration with LangChain, PostgreSQL, and FastAPI. Since I lack direct access to information concerning certain tools, such as their compatibility with PostgreSQL and FastAPI, I will first search for relevant information to enhance my analysis.\u001b[00m\n",
      "\u001b[95m## Using tool:\u001b[00m \u001b[92mSearch in a specific website\u001b[00m\n",
      "\u001b[95m## Tool Input:\u001b[00m \u001b[92m\n",
      "\"{\\\"search_query\\\": \\\"LangChain compatibility with PostgreSQL and FastAPI\\\", \\\"website\\\": \\\"https://langchain.com/\\\"}\"\u001b[00m\n",
      "\u001b[95m## Tool Output:\u001b[00m \u001b[92m\n",
      "Relevant Content:\n",
      "As of the 0.3 release, LangChain uses Pydantic 2 internally. Users should install Pydantic 2 and are advised to avoid using the pydantic.v1 namespace of Pydantic 2 with\n",
      "LangChain APIs. If you're working with prior versions of LangChain, please see the following guide\n",
      "on Pydantic compatibility . Was this page helpful?\n",
      "\n",
      "more accurate and effective queries. Great! We've got a graph database that we can query. Now let's try hooking it up to an LLM. GraphQACypherChain  Let's use a simple out-of-the-box chain that takes a question, turns it into a Cypher query, executes the query, and uses the result to answer the original question. LangChain comes with a built-in chain for this workflow that is designed to work with Neo4j: GraphCypherQAChain from langchain_neo4j import GraphCypherQAChain from langchain_openai\n",
      "\n",
      "On this page LangChain has evolved since its initial release, and many of the original \"Chain\" classes\n",
      "have been deprecated in favor of the more flexible and powerful frameworks of LCEL and LangGraph. This guide will help you migrate your existing v0.0 chains to the new abstractions. How deprecated implementations work Even though many of these implementations are deprecated, they are still supported in the codebase.\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mReporting Analyst\u001b[00m\n",
      "\u001b[95m## Thought:\u001b[00m \u001b[92mThought: Now that I have some information on LangChain, I need to evaluate additional tools like PostgreSQL and FastAPI for their compatibility with the previously mentioned AI tools. I will continue gathering relevant data for a comprehensive analysis.\u001b[00m\n",
      "\u001b[95m## Using tool:\u001b[00m \u001b[92mSearch in a specific website\u001b[00m\n",
      "\u001b[95m## Tool Input:\u001b[00m \u001b[92m\n",
      "\"{\\\"search_query\\\": \\\"PostgreSQL compatibility with LangChain and FastAPI\\\", \\\"website\\\": \\\"https://langchain.com/\\\"}\"\u001b[00m\n",
      "\u001b[95m## Tool Output:\u001b[00m \u001b[92m\n",
      "Relevant Content:\n",
      "As of the 0.3 release, LangChain uses Pydantic 2 internally. Users should install Pydantic 2 and are advised to avoid using the pydantic.v1 namespace of Pydantic 2 with\n",
      "LangChain APIs. If you're working with prior versions of LangChain, please see the following guide\n",
      "on Pydantic compatibility . Was this page helpful?\n",
      "\n",
      "and in-memory implementations. langchain : A package for higher level components (e.g., some pre-built chains). langgraph : Powerful orchestration layer for LangChain. Use to build complex pipelines and workflows. langserve : Used to deploy LangChain Runnables as REST endpoints. Uses FastAPI. Works primarily for LangChain Runnables, does not currently integrate with LangGraph. LLMs (legacy) : Older language models that take a string as input and return a string as output. Managing chat history\n",
      "\n",
      "more accurate and effective queries. Great! We've got a graph database that we can query. Now let's try hooking it up to an LLM. GraphQACypherChain  Let's use a simple out-of-the-box chain that takes a question, turns it into a Cypher query, executes the query, and uses the result to answer the original question. LangChain comes with a built-in chain for this workflow that is designed to work with Neo4j: GraphCypherQAChain from langchain_neo4j import GraphCypherQAChain from langchain_openai\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mReporting Analyst\u001b[00m\n",
      "\u001b[95m## Thought:\u001b[00m \u001b[92mThought: I am still obtaining relevant data regarding the tools mentioned. It appears that there are details confirming LangChain's compatibility with FastAPI, particularly through its component langserve, which utilizes FastAPI. However, further investigation into PostgreSQL's compatibility is still necessary. I will conduct another search specifically targeting PostgreSQL's role with LangChain and FastAPI.\u001b[00m\n",
      "\u001b[95m## Using tool:\u001b[00m \u001b[92mSearch in a specific website\u001b[00m\n",
      "\u001b[95m## Tool Input:\u001b[00m \u001b[92m\n",
      "\"{\\\"search_query\\\": \\\"PostgreSQL integration with LangChain and FastAPI\\\", \\\"website\\\": \\\"https://langchain.com/\\\"}\"\u001b[00m\n",
      "\u001b[95m## Tool Output:\u001b[00m \u001b[92m\n",
      "Relevant Content:\n",
      "method. Provides support for async programming , efficient batching , a rich streaming API . Integration with LangSmith for monitoring and debugging production-grade applications based on LLMs. Additional features like standardized token usage , rate limiting , caching and more. Integrations  LangChain has many chat model integrations that allow you to use a wide variety of models from different providers. These integrations are one of two types: Official models : These are models that are\n",
      "\n",
      "and in-memory implementations. langchain : A package for higher level components (e.g., some pre-built chains). langgraph : Powerful orchestration layer for LangChain. Use to build complex pipelines and workflows. langserve : Used to deploy LangChain Runnables as REST endpoints. Uses FastAPI. Works primarily for LangChain Runnables, does not currently integrate with LangGraph. LLMs (legacy) : Older language models that take a string as input and return a string as output. Managing chat history\n",
      "\n",
      "As of the 0.3 release, LangChain uses Pydantic 2 internally. Users should install Pydantic 2 and are advised to avoid using the pydantic.v1 namespace of Pydantic 2 with\n",
      "LangChain APIs. If you're working with prior versions of LangChain, please see the following guide\n",
      "on Pydantic compatibility . Was this page helpful?\n",
      "\n",
      "\n",
      "You ONLY have access to the following tools, and should NEVER make up tools that are not listed here:\n",
      "\n",
      "Tool Name: Search in a specific website\n",
      "Tool Arguments: {'search_query': {'description': 'Mandatory search query you want to use to search a specific website', 'type': 'str'}, 'website': {'description': 'Mandatory valid website URL you want to search on', 'type': 'str'}}\n",
      "Tool Description: A tool that can be used to semantic search a query from a specific URL content.\n",
      "\n",
      "IMPORTANT: Use the following format in your response:\n",
      "\n",
      "```\n",
      "Thought: you should always think about what to do\n",
      "Action: the action to take, only one name of [Search in a specific website], just the name, exactly as it's written.\n",
      "Action Input: the input to the action, just a simple JSON object, enclosed in curly braces, using \" to wrap keys and values.\n",
      "Observation: the result of the action\n",
      "```\n",
      "\n",
      "Once all necessary information is gathered, return the following format:\n",
      "\n",
      "```\n",
      "Thought: I now know the final answer\n",
      "Final Answer: the final answer to the original input question\n",
      "```\u001b[00m\n",
      "\u001b[93m Maximum iterations reached. Requesting final answer.\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mReporting Analyst\u001b[00m\n",
      "\u001b[95m## Final Answer:\u001b[00m \u001b[92m\n",
      "{\n",
      "  \"LangChain\": {\n",
      "    \"Description\": \"A modular framework for developing applications powered by language models (LLMs), particularly effective for integrating with various data sources including PDF documents.\",\n",
      "    \"Compatibility\": {\n",
      "      \"PostgreSQL\": \"Can be integrated for backend data storage and retrieval.\",\n",
      "      \"FastAPI\": \"Compatible through the langserve component, allowing deployment of LangChain Runnables as REST endpoints.\"\n",
      "    },\n",
      "    \"Pros\": [\n",
      "      \"Strong community support and documentation.\",\n",
      "      \"Highly customizable to specific use cases.\"\n",
      "    ],\n",
      "    \"Cons\": \"Initial setup may require more time.\",\n",
      "    \"Recommendation Score\": \"9/10\"\n",
      "  },\n",
      "  \"Hugging Face Transformers\": {\n",
      "    \"Description\": \"An open-source library that offers pre-trained models for Natural Language Processing (NLP), including transformers that enable easy integration into RAG systems.\",\n",
      "    \"Compatibility\": \"Can work with PostgreSQL for data storage; FastAPI can be used for serving models.\",\n",
      "    \"Pros\": [\n",
      "      \"Requires minimal setup for existing models.\",\n",
      "      \"Strong community and extensive resources for troubleshooting.\"\n",
      "    ],\n",
      "    \"Cons\": \"Fine-tuning models may require significant computational resources.\",\n",
      "    \"Recommendation Score\": \"8.5/10\"\n",
      "  },\n",
      "  \"Elasticsearch\": {\n",
      "    \"Description\": \"A distributed search and analytics engine that allows for high-performance retrieval of large datasets, particularly suited for document search tasks.\",\n",
      "    \"Compatibility\": \"Potential integration with PostgreSQL for data synchronization, but primarily used for indexing.\",\n",
      "    \"Pros\": [\n",
      "      \"Fast retrieval speeds and scalability.\",\n",
      "      \"Supports complex queries and aggregations.\"\n",
      "    ],\n",
      "    \"Cons\": \"Requires configuration and management for large-scale deployments.\",\n",
      "    \"Recommendation Score\": \"8/10\"\n",
      "  },\n",
      "  \"TensorFlow\": {\n",
      "    \"Description\": \"An open-source library for creating and training machine learning models, particularly effective for developing deep learning architectures in RAG systems.\",\n",
      "    \"Compatibility\": \"Can interact with PostgreSQL for data storage; FastAPI can facilitate model serving.\",\n",
      "    \"Pros\": [\n",
      "      \"Strong ecosystem with numerous models and datasets available.\",\n",
      "      \"Highly efficient in processing large datasets.\"\n",
      "    ],\n",
      "    \"Cons\": \"Steep learning curve for newcomers to deep learning.\",\n",
      "    \"Recommendation Score\": \"8/10\"\n",
      "  },\n",
      "  \"PyTorch\": {\n",
      "    \"Description\": \"A dynamic deep learning framework favored in academia for its flexibility and ease of use in developing NLP models, suitable for RAG applications.\",\n",
      "    \"Compatibility\": \"Can utilize PostgreSQL for data management and integrate with FastAPI for serving models.\",\n",
      "    \"Pros\": [\n",
      "      \"User-friendly interface, excellent for rapid prototyping.\",\n",
      "      \"Very effective for implementing new methodologies in NLP.\"\n",
      "    ],\n",
      "    \"Cons\": \"More complex to deploy in production environments.\",\n",
      "    \"Recommendation Score\": \"8/10\"\n",
      "  }\n",
      "}\u001b[00m\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages/instructor/process_response.py:222: DeprecationWarning: FUNCTIONS is deprecated and will be removed in future versions\n",
      "  if mode == Mode.FUNCTIONS:\n",
      "/Users/mikhailrozhkov/.pyenv/versions/3.11.6/lib/python3.11/site-packages/instructor/function_calls.py:118: DeprecationWarning: FUNCTIONS is deprecated and will be removed in future versions\n",
      "  if mode == Mode.FUNCTIONS:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mReporting Analyst\u001b[00m\n",
      "\u001b[95m## Task:\u001b[00m \u001b[92m\n",
      "Read the analysis from the previous task's result and create a comprehensive Markdown report.\n",
      "\n",
      "The report should include:\n",
      "\n",
      "1. An introduction explaining the task (Develop a conversational RAG system that can answer questions based on a large PDF document collection) and existing stack (LangChain, PogreSQL, FastAPI)\n",
      "2. For each tool, create a section with:\n",
      "   - Tool name and URL as a heading\n",
      "   - Description\n",
      "   - Features (as bullet points)\n",
      "   - Pros (as bullet points)\n",
      "   - Cons (as bullet points)\n",
      "   - Integration complexity\n",
      "   - Recommendation score with justification\n",
      "3. A summary/conclusion comparing the tools and providing final recommendations\n",
      "\n",
      "Use proper Markdown formatting with headings, bullet points, and emphasis where appropriate.\n",
      "Sort tools by recommendation score (descending).\n",
      "\n",
      "Your output should be a complete, well-formatted Markdown document.\n",
      "\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mReporting Analyst\u001b[00m\n",
      "\u001b[95m## Using tool:\u001b[00m \u001b[92mRead a file's content\u001b[00m\n",
      "\u001b[95m## Tool Input:\u001b[00m \u001b[92m\n",
      "\"{\\\"file_path\\\": \\\"output/tool_analysis.json\\\"}\"\u001b[00m\n",
      "\u001b[95m## Tool Output:\u001b[00m \u001b[92m\n",
      "{\n",
      "  \"LangChain\": {\n",
      "    \"Description\": \"A modular framework for developing applications powered by language models (LLMs), particularly effective for integrating with various data sources including PDF documents.\",\n",
      "    \"Compatibility\": {\n",
      "      \"PostgreSQL\": \"Can be integrated for backend data storage and retrieval.\",\n",
      "      \"FastAPI\": \"Compatible through the langserve component, allowing deployment of LangChain Runnables as REST endpoints.\"\n",
      "    },\n",
      "    \"Pros\": [\n",
      "      \"Strong community support and documentation.\",\n",
      "      \"Highly customizable to specific use cases.\"\n",
      "    ],\n",
      "    \"Cons\": \"Initial setup may require more time.\",\n",
      "    \"Recommendation Score\": \"9/10\"\n",
      "  },\n",
      "  \"Hugging Face Transformers\": {\n",
      "    \"Description\": \"An open-source library that offers pre-trained models for Natural Language Processing (NLP), including transformers that enable easy integration into RAG systems.\",\n",
      "    \"Compatibility\": \"Can work with PostgreSQL for data storage; FastAPI can be used for serving models.\",\n",
      "    \"Pros\": [\n",
      "      \"Requires minimal setup for existing models.\",\n",
      "      \"Strong community and extensive resources for troubleshooting.\"\n",
      "    ],\n",
      "    \"Cons\": \"Fine-tuning models may require significant computational resources.\",\n",
      "    \"Recommendation Score\": \"8.5/10\"\n",
      "  },\n",
      "  \"Elasticsearch\": {\n",
      "    \"Description\": \"A distributed search and analytics engine that allows for high-performance retrieval of large datasets, particularly suited for document search tasks.\",\n",
      "    \"Compatibility\": \"Potential integration with PostgreSQL for data synchronization, but primarily used for indexing.\",\n",
      "    \"Pros\": [\n",
      "      \"Fast retrieval speeds and scalability.\",\n",
      "      \"Supports complex queries and aggregations.\"\n",
      "    ],\n",
      "    \"Cons\": \"Requires configuration and management for large-scale deployments.\",\n",
      "    \"Recommendation Score\": \"8/10\"\n",
      "  },\n",
      "  \"TensorFlow\": {\n",
      "    \"Description\": \"An open-source library for creating and training machine learning models, particularly effective for developing deep learning architectures in RAG systems.\",\n",
      "    \"Compatibility\": \"Can interact with PostgreSQL for data storage; FastAPI can facilitate model serving.\",\n",
      "    \"Pros\": [\n",
      "      \"Strong ecosystem with numerous models and datasets available.\",\n",
      "      \"Highly efficient in processing large datasets.\"\n",
      "    ],\n",
      "    \"Cons\": \"Steep learning curve for newcomers to deep learning.\",\n",
      "    \"Recommendation Score\": \"8/10\"\n",
      "  },\n",
      "  \"PyTorch\": {\n",
      "    \"Description\": \"A dynamic deep learning framework favored in academia for its flexibility and ease of use in developing NLP models, suitable for RAG applications.\",\n",
      "    \"Compatibility\": \"Can utilize PostgreSQL for data management and integrate with FastAPI for serving models.\",\n",
      "    \"Pros\": [\n",
      "      \"User-friendly interface, excellent for rapid prototyping.\",\n",
      "      \"Very effective for implementing new methodologies in NLP.\"\n",
      "    ],\n",
      "    \"Cons\": \"More complex to deploy in production environments.\",\n",
      "    \"Recommendation Score\": \"8/10\"\n",
      "  }\n",
      "}\u001b[00m\n"
     ]
    }
   ],
   "source": [
    "# Let's run it once without MLflow to see the typical verbose console output.\n",
    "\n",
    "# Turn on auto tracing by calling mlflow.crewai.autolog()\n",
    "mlflow.crewai.autolog()\n",
    "mlflow.set_experiment(\"Step 5 - Comparing prompts\")\n",
    "\n",
    "ai_dev_crew = AIOpsResearchCrewUpdatedPrompts(\n",
    "    task_description, \n",
    "    existing_stack,\n",
    "    nebius_client=nebius_client, \n",
    "    evaluation_model=\"meta-llama/Llama-3.3-70B-Instruct\")\n",
    "\n",
    "result = ai_dev_crew.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation:** MLflow makes A/B testing prompts or configurations straightforward. You have a clear record of what changed and can easily compare the outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Conclusion\n",
    "\n",
    "Developing robust and reliable LLM agents requires moving beyond simple script execution. By integrating **Managed MLflow**, we gain crucial capabilities:\n",
    "\n",
    "*   **Traceability:** Understand exactly what happened during an agent run via logged artifacts (`execution_log.txt`, `error.log`).\n",
    "*   **Reproducibility:** Capture configurations as parameters, ensuring you know precisely what setup produced a given result.\n",
    "*   **Comparison:** Systematically evaluate the impact of changes (prompts, LLMs, agent logic) using the MLflow UI's compare feature for parameters, metrics, and artifacts.\n",
    "*   **Debugging:** Quickly identify and analyze failed runs using status tags and dedicated error logs.\n",
    "*   **Performance Monitoring:** Track execution time and other metrics to identify bottlenecks and regressions.\n",
    "*   **Quality & Cost Evaluation:** Provide a central repository to store and analyze quality scores (human or LLM-judged) and resource usage (tokens, cost) for holistic optimization.\n",
    "\n",
    "MLflow transforms agent development into a more transparent, measurable, and efficient engineering process, enabling faster iteration and more reliable outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlflow-nebius",
   "language": "python",
   "name": "mlflow-nebius"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
