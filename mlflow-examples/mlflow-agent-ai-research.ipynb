{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: Simplifying Agent Development with MLflow & CrewAI\n",
    "\n",
    "**Goal:** This tutorial demonstrates how Managed MLflow transforms the development of multi-agent systems (using CrewAI as an example) from a potentially opaque and difficult-to-debug process into a structured, observable, and iterative engineering workflow.\n",
    "\n",
    "**Scenario:** We will use the \"AI Research\" agentic systen. This system researches new AI/ML tools. While useful, developing such agents presents common challenges:\n",
    "\n",
    "*   **Black Box Execution:** What *exactly* did the agent do step-by-step? Why did it choose *that* tool?\n",
    "*   **Comparing Changes:** How do we reliably compare results if we tweak prompts or agent configurations?\n",
    "*   **Debugging Failures:** Why did the agent fail? What was it trying to do right before the error?\n",
    "*   **Performance:** Is the agent getting slower? Where are the bottlenecks?\n",
    "*   **Quality Evaluation:** Is changing the LLM or prompts actually improving the *quality* of the output?\n",
    "*   **Cost/Speed Optimization:** How can we track and reduce token usage or latency?\n",
    "\n",
    "We will tackle some problems by incrementally integrating MLflow tracking features into our CrewAI development process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "\n",
    "1. Launch your instance of the Managed Service for MLflow with [the MLflow quickstart](https://docs.nebius.com/mlflow/quickstart).\n",
    "2. Set up your API key to connect to Nebius AI Studio with [the AI Studio quickstart](https://docs.nebius.com/studio/inference/quickstart).\n",
    "\n",
    "> **Note:** Launching an MLflow cluster for the first time may take 15-30 minutes to be fully provisioned and ready to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install mlflow==2.21.2 python-dotenv openai crewai==0.114.0 crewai-tools duckduckgo-search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Secrets and Environment Variables\n",
    "\n",
    "Set the following environment variables where this notebook is running, so that the code in the following cells can connect to both Nebius Managed Service for MLflow and Nebius AI Studio. \n",
    "\n",
    "MLflow:<br>\n",
    "`MLFLOW_TRACKING_SERVER_CERT_PATH`<br>\n",
    "`MLFLOW_TRACKING_URI`<br>\n",
    "`MLFLOW_TRACKING_USERNAME`<br>\n",
    "`MLFLOW_TRACKING_PASSWORD`<br>\n",
    "\n",
    "AI Studio:<br>\n",
    "`NEBIUS_API_KEY`\n",
    "\n",
    "To set the environment variables, run the following cell. You may choose to set them interactively or by loading from a `.env` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Add the parent directory to Python path\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "\n",
    "from env_setup import setup_env_from_file, setup_env_interactive, verify_env_setup\n",
    "\n",
    "# Option 1: Interactive setup\n",
    "# setup_env_interactive()\n",
    "\n",
    "# Option 2: Load from .env file\n",
    "setup_env_from_file('../.env')\n",
    "\n",
    "# Verify the setup\n",
    "verify_env_setup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check connection to MLflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import mlflow \n",
    "\n",
    "# List experiments in MLflow\n",
    "mlflow.search_experiments()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Quick implementation of an AI Agent with CrewAI\n",
    "\n",
    "Problem: How do we track what happens during agent execution when logs scroll by too quickly?\n",
    "\n",
    "Solution:\n",
    "\n",
    "- CrewAI provides basic logging but lacks structured traceability\n",
    "- Console output is ephemeral and hard to analyze\n",
    "- No built-in token usage or cost tracking\n",
    "- Difficult to debug errors without visibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import time\n",
    "import json\n",
    "import traceback\n",
    "import contextlib\n",
    "from textwrap import dedent\n",
    "from crewai import Agent, Crew, Task, Process\n",
    "from crewai import __version__ as crewai_version\n",
    "from crewai_tools import WebsiteSearchTool\n",
    "from langchain_openai import ChatOpenAI\n",
    "import mlflow\n",
    "\n",
    "print(f\"MLflow Version: {mlflow.__version__}\")\n",
    "print(f\"CrewAI Version: {crewai_version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To search across any discovered websites\n",
    "\n",
    "search_tool = WebsiteSearchTool()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define an Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResearchAgents:\n",
    "    def researcher_agent(self):\n",
    "        \"\"\"Agent responsible for discovering relevant AI tools for specific tasks.\"\"\"\n",
    "        return Agent(\n",
    "            role=\"Senior Data Researcher\",\n",
    "            goal=\"Discover and evaluate relevant AI tools (libraries, frameworks) for specific tasks\",\n",
    "            backstory=\"An expert in AI development and MLOps\",\n",
    "            tools=[search_tool],\n",
    "            verbose=True,\n",
    "            max_iter=3\n",
    "        )\n",
    "\n",
    "    def analyst_agent(self):\n",
    "        \"\"\"Agent responsible for analyzing tools and creating detailed reports.\"\"\"\n",
    "        return Agent(\n",
    "            role=\"Reporting Analyst\",\n",
    "            goal=\"Create detailed reports based on data analysis and research findings\",\n",
    "            backstory=\"\"\"You are a technical expert specializing in AI technologies evaluation. \n",
    "    You have a deep understanding of AI libraries and frameworks, and can \n",
    "    quickly assess their technical merits\"\"\",\n",
    "            tools=[search_tool],\n",
    "            verbose=True,\n",
    "            max_iter=3 # Maximum iterations before the agent must provide its best answer. Default is 20.\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AIOpsResearchTasks:\n",
    "    def search_tools_task(self, agent, task, ai_stack):\n",
    "        \"\"\"Task to discover relevant tools for a specific task considering the existing AI stack.\"\"\"\n",
    "        return Task(\n",
    "            description=dedent(f\"\"\"\n",
    "                You are a research agent tasked with finding AI tools for: {{task}}.\n",
    "                Consider compatibility with: {{ai_stack}}.\n",
    "                \n",
    "                For each tool, identify:\n",
    "                - Name and URL\n",
    "                - Primary use case\n",
    "                - Brief description (2-3 sentences)\n",
    "                \n",
    "                Format your response as a JSON list of objects.\n",
    "            \"\"\"),\n",
    "            agent=agent,\n",
    "            expected_output=\"A JSON list containing details of 3-5 relevant AI tools with their names, URLs, use cases, and descriptions.\",\n",
    "            output_file=\"output/tool_candidates.json\"\n",
    "        )\n",
    "\n",
    "    def analyze_tools_task(self, agent, task, ai_stack):\n",
    "        \"\"\"Task to perform in-depth analysis of discovered tools.\"\"\"\n",
    "        return Task(\n",
    "            description=dedent(f\"\"\"\n",
    "                Read the tool list from the previous task's result and perform a detailed analysis of each tool.\n",
    "                \n",
    "                For each tool:\n",
    "                1. Research its capabilities, limitations, community adoption, documentation quality\n",
    "                2. Evaluate how well it addresses the specified task: '{task}'\n",
    "                3. Consider its compatibility with '{ai_stack}'\n",
    "                4. Identify pros and cons\n",
    "                \n",
    "                Your output should be a JSON list of these detailed analysis objects.\n",
    "            \"\"\"),\n",
    "            agent=agent,\n",
    "            expected_output=\"A JSON list containing detailed analysis of each tool with comprehensive information about features, pros, cons, and recommendation scores.\",\n",
    "            output_file=\"output/tool_analysis.json\",\n",
    "        )\n",
    "\n",
    "    def create_report_task(self, agent, task, ai_stack):\n",
    "        \"\"\"Task to create a comprehensive report with recommendations.\"\"\"\n",
    "        return Task(\n",
    "            description=dedent(f\"\"\"\n",
    "                Read the analysis from the previous task's result and create a comprehensive Markdown report.\n",
    "                \n",
    "                The report should include:\n",
    "                \n",
    "                1. An introduction explaining the task ('{task}') and existing stack ('{ai_stack}')\n",
    "                2. For each tool, create a section with:\n",
    "                   - Tool name and URL as a heading\n",
    "                   - Description\n",
    "                   - Features (as bullet points)\n",
    "                   - Pros (as bullet points)\n",
    "                   - Cons (as bullet points)\n",
    "                   - Integration complexity\n",
    "                   - Recommendation score with justification\n",
    "                3. A summary/conclusion comparing the tools and providing final recommendations\n",
    "                \n",
    "                Use proper Markdown formatting with headings, bullet points, and emphasis where appropriate.\n",
    "                Sort tools by recommendation score (descending).\n",
    "                \n",
    "                Your output should be a complete, well-formatted Markdown document.\n",
    "            \"\"\"),\n",
    "            agent=agent,\n",
    "            expected_output=\"A comprehensive Markdown report analyzing each tool with recommendations, properly formatted with headings, bullet points, and clear sections.\",\n",
    "            output_file=\"output/tool_recommendation_report.md\",\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T20:48:17.410964Z",
     "iopub.status.busy": "2025-03-29T20:48:17.410280Z",
     "iopub.status.idle": "2025-03-29T20:48:17.452481Z",
     "shell.execute_reply": "2025-03-29T20:48:17.452093Z",
     "shell.execute_reply.started": "2025-03-29T20:48:17.410925Z"
    }
   },
   "source": [
    "### Design a Crew \n",
    "\n",
    "A crew in crewAI is a collaborative group of agents working together to complete tasks. Crews define:\n",
    "\n",
    "- Task execution strategy\n",
    "- Agent collaboration methods\n",
    "- Overall workflow coordination\n",
    "- Communication patterns between agents\n",
    "- Task delegation and sequencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AIOpsResearchCrew:\n",
    "    def __init__(self, task, ai_stack):\n",
    "        \"\"\"\n",
    "        Initialize the crew with the task description and existing AI stack.\n",
    "        \n",
    "        Args:\n",
    "            task (str): Description of the task requiring AI tools\n",
    "            ai_stack (str): Comma-separated list of existing tools/frameworks used\n",
    "        \"\"\"\n",
    "        self.task = task\n",
    "        self.ai_stack = ai_stack\n",
    "        \n",
    "        # Ensure output directory exists\n",
    "        os.makedirs(\"output\", exist_ok=True)\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"Execute the research, analysis, and reporting process.\"\"\"\n",
    "        # Initialize agents\n",
    "        agents = AIOpsResearchAgents()\n",
    "        researcher = agents.researcher_agent()\n",
    "        analyst = agents.analyst_agent()\n",
    "\n",
    "        # Initialize tasks\n",
    "        tasks = AIOpsResearchTasks()\n",
    "        search_task = tasks.search_tools_task(researcher, self.task, self.ai_stack)\n",
    "        analyze_task = tasks.analyze_tools_task(analyst, self.task, self.ai_stack)\n",
    "        report_task = tasks.create_report_task(analyst, self.task, self.ai_stack)\n",
    "        \n",
    "        # Create the crew\n",
    "        crew = Crew(\n",
    "            agents=[researcher, analyst],\n",
    "            tasks=[search_task, analyze_task, report_task],\n",
    "            verbose=True,\n",
    "            process=Process.sequential,\n",
    "            memory=True\n",
    "        )\n",
    "        \n",
    "        result = crew.kickoff()\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Run (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let's run it once without MLflow to see the typical verbose console output.\n",
    "\n",
    "task_description = \"Develop a conversational RAG system that can answer questions based on a large PDF document collection\"\n",
    "existing_stack = \"LangChain, PogreSQL, FastAPI\"\n",
    "\n",
    "ai_dev_crew = AIOpsResearchCrew(task_description, existing_stack)\n",
    "result = ai_dev_crew.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"\\n=== Final Result (Base Run) ===\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Aautolog Agent execution and LLM calls with MLflow\n",
    "\n",
    "Problem: How can we reliably capture exactly what the agent did, including reasoning steps and tool usage, for later inspection?\n",
    "\n",
    "\n",
    "Solution:\n",
    "\n",
    "- Enable structured logging with `mlflow....autolog()`\n",
    "- Record agent steps, decisions, and tools used\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Turn on auto tracing by calling mlflow.crewai.autolog()\n",
    "mlflow.crewai.autolog()\n",
    "mlflow.set_experiment(\"Step 2 - Autolog\")\n",
    "\n",
    "ai_dev_crew = AIOpsResearchCrew(task_description, existing_stack)\n",
    "result = ai_dev_crew.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Tracking metrics and artifacts\n",
    "\n",
    "Problem: How do we measure and compare agent performance across multiple runs?\n",
    "\n",
    "Solution:\n",
    "\n",
    "- Track execution time, token counts, and costs with `mlflow.log_metric()`\n",
    "- Store output artifacts and conversation transcripts with `mlflow.log_artifact()`\n",
    "- Add searchable metadata with `mlflow.set_tag()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modify Crew Class for MLflow Run & Log Capture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import logging\n",
    "import io\n",
    "import sys\n",
    "import contextlib\n",
    "\n",
    "\n",
    "class AIOpsResearchCrew:\n",
    "    def __init__(self, task, ai_stack):\n",
    "        \"\"\"\n",
    "        Initialize the crew with the task description and existing AI stack.\n",
    "        \n",
    "        Args:\n",
    "            task (str): Description of the task requiring AI tools\n",
    "            ai_stack (str): Comma-separated list of existing tools/frameworks used\n",
    "        \"\"\"\n",
    "        self.task = task\n",
    "        self.ai_stack = ai_stack\n",
    "        self.run_id = None\n",
    "        self.crew = None\n",
    "        \n",
    "        # Ensure output directory exists\n",
    "        os.makedirs(\"output\", exist_ok=True)\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"Execute the research, analysis, and reporting process with MLflow tracking.\"\"\"\n",
    "        \n",
    "        # --- UPDATE: Start MLflow run ---\n",
    "        with mlflow.start_run(run_name=f\"Tool_Research_{int(time.time())}\") as mlflow_run:\n",
    "            self.run_id = mlflow_run.info.run_id\n",
    "            \n",
    "\n",
    "            logger.info(\"Initializing agents...\")\n",
    "            agents = AIOpsResearchAgents()\n",
    "            researcher = agents.researcher_agent()\n",
    "            analyst = agents.analyst_agent()\n",
    "            \n",
    "            # Initialize tasks\n",
    "            tasks = AIOpsResearchTasks()\n",
    "            search_task = tasks.search_tools_task(researcher, self.task, self.ai_stack)\n",
    "            analyze_task = tasks.analyze_tools_task(analyst, self.task, self.ai_stack)\n",
    "            report_task = tasks.create_report_task(analyst, self.task, self.ai_stack)\n",
    "            \n",
    "            # Create the crew\n",
    "            self.crew = Crew(\n",
    "                agents=[researcher, analyst],\n",
    "                tasks=[search_task, analyze_task, report_task],\n",
    "                verbose=False,\n",
    "                process=Process.sequential,\n",
    "                memory=True\n",
    "            )\n",
    "            \n",
    "            # Start the crew\n",
    "            result = self.crew.kickoff()\n",
    "\n",
    "            \n",
    "            # --- UPDATE: Log parameters ---\n",
    "            mlflow.log_param(\"task\", self.task)\n",
    "            mlflow.log_param(\"ai_stack\", self.ai_stack)\n",
    "            \n",
    "            # --- UPDATE: Log metrics ---\n",
    "            mlflow.log_metrics(json.loads(self.crew.usage_metrics.json()))\n",
    "\n",
    "            # --- UPDATE: Log artifacts ---\n",
    "            artifact_files = [\n",
    "                \"output/tool_candidates.json\",\n",
    "                \"output/tool_analysis.json\",\n",
    "                \"output/tool_recommendation_report.md\",\n",
    "            ]\n",
    "            for file_path in artifact_files:\n",
    "                if os.path.exists(\"output/tool_candidates.json\"):\n",
    "                    mlflow.log_artifact(file_path)\n",
    "\n",
    "            # --- UPDATE: Set success tag ---\n",
    "            if os.path.exists(\"output/tool_recommendation_report.md\"):\n",
    "                mlflow.set_tag(\"status\", \"SUCCESS\")\n",
    "            else:\n",
    "                mlflow.set_tag(\"status\", \"FAILED\")\n",
    "            \n",
    "            return result\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T20:51:48.263584Z",
     "iopub.status.busy": "2025-03-29T20:51:48.263097Z",
     "iopub.status.idle": "2025-03-29T20:51:48.311052Z",
     "shell.execute_reply": "2025-03-29T20:51:48.309975Z",
     "shell.execute_reply.started": "2025-03-29T20:51:48.263555Z"
    }
   },
   "source": [
    "### Run and Observe in MLflow UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let's run it once without MLflow to see the typical verbose console output.\n",
    "\n",
    "# Turn on auto tracing by calling mlflow.crewai.autolog()\n",
    "mlflow.crewai.autolog()\n",
    "mlflow.set_experiment(\"Step 3 - Metrics\")\n",
    "\n",
    "ai_dev_crew = AIOpsResearchCrew(task_description, existing_stack)\n",
    "result = ai_dev_crew.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crew Usage Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the crew usage metrics \n",
    "\n",
    "ai_dev_crew.crew.usage_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Evaluating Agent Output Quality\n",
    "\n",
    "**Problem:** How do we automatically evaluate output quality without manual comparison?\n",
    "\n",
    "**MLflow Solution:**\n",
    "- Define specific quality criteria for assessment\n",
    "- Use LLM-as-Judge evaluation for scoring\n",
    "- Log evaluation metrics to MLflow\n",
    "- Compare configurations in MLflow UI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up Nebius AI Studio client to \"evaluator\" LLM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "API_KEY = os.environ.get(\"NEBIUS_API_KEY\")\n",
    "\n",
    "# Instantiate the client instance\n",
    "nebius_client = openai.OpenAI(\n",
    "    api_key=API_KEY,\n",
    "    base_url=\"https://api.studio.nebius.ai/v1/\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simplify tasks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from crewai_tools import FileReadTool\n",
    "\n",
    "class AIOpsResearchTasks:\n",
    "    def search_tools_task(self, agent, task, ai_stack):\n",
    "        \"\"\"Task to discover relevant tools for a specific task considering the existing AI stack.\"\"\"\n",
    "        return Task(\n",
    "            description=f\"\"\"\n",
    "                Find some AI tools for {task}.\n",
    "                Format however you think is best.\n",
    "            \"\"\",\n",
    "            agent=agent,\n",
    "            tools=[search_tool],\n",
    "            expected_output=\"Info about AI tools\",\n",
    "            output_file=\"output/tool_candidates.json\",\n",
    "            cache=False\n",
    "        )\n",
    "\n",
    "    def analyze_tools_task(self, agent, task, ai_stack):\n",
    "        \"\"\"Task to perform in-depth analysis of discovered tools.\"\"\"\n",
    "        return Task(\n",
    "            description=f\"\"\"\n",
    "                Check out the tools from before.\n",
    "                \n",
    "                Tell me what you think about each one.\n",
    "                Is it good? Does it work with {ai_stack}?\n",
    "                \n",
    "                Make it JSON I guess.\n",
    "            \"\"\",\n",
    "            agent=agent,\n",
    "            tools=[search_tool],\n",
    "            expected_output=\"Analysis of tools\",\n",
    "            output_file=\"output/tool_analysis.json\",\n",
    "            cache=False,\n",
    "        )\n",
    "\n",
    "    def create_report_task(self, agent, task, ai_stack):\n",
    "        \"\"\"Task to create a comprehensive report with recommendations.\"\"\"\n",
    "        return Task(\n",
    "            description=f\"\"\"\n",
    "                Make a report about the tools.\n",
    "                Use markdown formatting.\n",
    "            \"\"\",\n",
    "            agent=agent,\n",
    "            tools=[FileReadTool(file_path='output/tool_analysis.json')],\n",
    "            expected_output=\"A report\",\n",
    "            output_file=\"output/tool_recommendation_report.md\",\n",
    "            cache=False,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "import os\n",
    "from textwrap import dedent\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def evaluate_report(\n",
    "    report_path: str, \n",
    "    nebius_client,\n",
    "    model: str = \"meta-llama/Llama-3.3-70B-Instruct\"\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Evaluate a technical report using Nebius AI.\n",
    "    \n",
    "    Args:\n",
    "        report_path: Path to the markdown report file\n",
    "        nebius_client: Initialized Nebius API client\n",
    "        model: Model name to use for evaluation\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary containing evaluation scores and reasoning\n",
    "    \"\"\"\n",
    "    # Read the report content\n",
    "    try:\n",
    "        with open(report_path, 'r') as f:\n",
    "            report_content = f.read()\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error reading report file: {str(e)}\")\n",
    "        return {\n",
    "            \"completeness_score\": 0,\n",
    "            \"relevance_score\": 0,\n",
    "            \"overall_quality_score\": 0,\n",
    "            \"reasoning\": f\"Error reading report: {str(e)}\"\n",
    "        }\n",
    "    \n",
    "    # Create evaluation prompt\n",
    "    evaluation_prompt = dedent(f\"\"\"\n",
    "        Evaluate the quality of the following technical report.\n",
    "        \n",
    "        Your job is to score the report on the following criteria (scale 1-10):\n",
    "        \n",
    "        - COMPLETENESS: Does the report include all required sections for each tool?\n",
    "          1-3: Missing multiple required sections\n",
    "          4-6: Has basic information but lacks detail\n",
    "          7-8: Contains most required sections with good detail\n",
    "          9-10: Complete with introduction, tool sections (name, URL, description, features, pros, cons, integration complexity, recommendation score), and conclusion\n",
    "        \n",
    "        - RELEVANCE: How directly applicable are the recommended tools to the specific task and tech stack?\n",
    "          1-3: General tools with no specific RAG functionality (like basic logging libraries)\n",
    "          4-6: Tools with potential use cases (like numpy for data processing)\n",
    "          7-8: Related tools with partial functionality (like fastai - useful but no specific RAG features)\n",
    "          9-10: Directly applicable tools (like LangGraph or CrewAI - frameworks specifically for RAG systems)\n",
    "        \n",
    "        - OVERALL QUALITY: The overall quality of the report considering all factors.\n",
    "        \n",
    "        Here is the report to evaluate:\n",
    "        \n",
    "        ---BEGIN REPORT---\n",
    "        {report_content}\n",
    "        ---END REPORT---\n",
    "        \n",
    "        Provide your evaluation as a JSON object with the following format:\n",
    "            \"completeness_score\": X,\n",
    "            \"relevance_score\": X,\n",
    "            \"overall_quality_score\": X,\n",
    "            \"reasoning\": \"Detailed explanation of your evaluation and scores...\"\n",
    "        }}\n",
    "        \n",
    "        Return ONLY the JSON object with no additional text.\n",
    "    \"\"\")\n",
    "    \n",
    "    # Call Nebius AI Studio for evaluation\n",
    "    try:\n",
    "        response = nebius_client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are an expert evaluator of technical reports.\"},\n",
    "                {\"role\": \"user\", \"content\": evaluation_prompt}\n",
    "            ],\n",
    "            temperature=0,\n",
    "            response_format={\"type\": \"json_object\"}\n",
    "        )\n",
    "        \n",
    "        # Parse response\n",
    "        result = json.loads(response.choices[0].message.content)\n",
    "        \n",
    "        # Ensure all scores are present\n",
    "        required_scores = [\"completeness_score\", \"relevance_score\", \"overall_quality_score\"]\n",
    "        for score in required_scores:\n",
    "            if score not in result:\n",
    "                result[score] = 0\n",
    "        \n",
    "        # Ensure reasoning is present\n",
    "        if \"reasoning\" not in result:\n",
    "            result[\"reasoning\"] = \"No reasoning provided.\"\n",
    "            \n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during evaluation: {str(e)}\")\n",
    "        return {\n",
    "            \"completeness_score\": 0,\n",
    "            \"relevance_score\": 0,\n",
    "            \"overall_quality_score\": 0,\n",
    "            \"reasoning\": f\"Evaluation error: {str(e)}\"\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_results = evaluate_report(\n",
    "                    report_path='output/tool_recommendation_report.md',\n",
    "                    nebius_client=nebius_client,\n",
    "                    model=\"meta-llama/Llama-3.3-70B-Instruct\"\n",
    "                )\n",
    "\n",
    "evaluation_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modify AIOpsResearchCrew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AIOpsResearchCrewEvaluated:\n",
    "    def __init__(self, task, ai_stack, nebius_client=None, evaluation_model=\"meta-llama/Llama-3.3-70B-Instruct\"):\n",
    "        \"\"\"\n",
    "        Initialize the crew with the task description and existing AI stack.\n",
    "        \n",
    "        Args:\n",
    "            task (str): Description of the task requiring AI tools\n",
    "            ai_stack (str): Comma-separated list of existing tools/frameworks used\n",
    "            nebius_client: Initialized Nebius API client for evaluation\n",
    "            evaluation_model (str): Model to use for evaluation\n",
    "        \"\"\"\n",
    "        self.task = task\n",
    "        self.ai_stack = ai_stack\n",
    "        self.run_id = None\n",
    "        self.execution_log = io.StringIO()\n",
    "        self.crew = None\n",
    "        self.nebius_client = nebius_client\n",
    "        self.evaluation_model = evaluation_model\n",
    "        \n",
    "        # Ensure output directory exists\n",
    "        os.makedirs(\"output\", exist_ok=True)\n",
    "        \n",
    "        # Initialize Nebius client if not provided\n",
    "        if self.nebius_client is None and os.environ.get(\"NEBIUS_API_KEY\"):\n",
    "            self.nebius_client = openai.OpenAI(\n",
    "                api_key=os.environ.get(\"NEBIUS_API_KEY\"),\n",
    "                base_url=\"https://api.studio.nebius.ai/v1/\"\n",
    "            )\n",
    "\n",
    "    def _clean_output_files(self):\n",
    "        \"\"\"Remove all previously generated output files if they exist.\"\"\"\n",
    "        files_to_remove = [\n",
    "            \"output/tool_candidates.json\",\n",
    "            \"output/tool_analysis.json\",\n",
    "            \"output/tool_recommendation_report.md\",\n",
    "            \"output/report_evaluation.json\"\n",
    "        ]\n",
    "        \n",
    "        for file_path in files_to_remove:\n",
    "            if os.path.exists(file_path):\n",
    "                try:\n",
    "                    os.remove(file_path)\n",
    "                    logger.info(f\"Removed existing file: {file_path}\")\n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"Failed to remove file {file_path}: {str(e)}\")\n",
    "\n",
    "    def run(self, evaluate=True):\n",
    "        \"\"\"\n",
    "        Execute the research, analysis, and reporting process with MLflow tracking.\n",
    "        \n",
    "        Args:\n",
    "            evaluate (bool): Whether to evaluate the report after generation\n",
    "        \n",
    "        Returns:\n",
    "            The result from the crew execution\n",
    "        \"\"\"\n",
    "        # Clean any existing output files\n",
    "        self._clean_output_files()\n",
    "        \n",
    "        # --- Start MLflow run ---\n",
    "        with mlflow.start_run(run_name=f\"Tool_Research_{int(time.time())}\") as mlflow_run:\n",
    "            self.run_id = mlflow_run.info.run_id\n",
    "\n",
    "            # Initialize agents\n",
    "            agents = AIOpsResearchAgents()\n",
    "            researcher = agents.researcher_agent()\n",
    "            analyst = agents.analyst_agent()\n",
    "            \n",
    "            # Initialize tasks\n",
    "            tasks = AIOpsResearchTasks()\n",
    "            search_task = tasks.search_tools_task(researcher, self.task, self.ai_stack)\n",
    "            analyze_task = tasks.analyze_tools_task(analyst, self.task, self.ai_stack)\n",
    "            report_task = tasks.create_report_task(analyst, self.task, self.ai_stack)\n",
    "            \n",
    "            # Create the crew\n",
    "            self.crew = Crew(\n",
    "                agents=[researcher, analyst],\n",
    "                tasks=[search_task, analyze_task, report_task],\n",
    "                verbose=False,\n",
    "                process=Process.sequential,\n",
    "                memory=True,\n",
    "                cache=False\n",
    "            )\n",
    "            \n",
    "            # Start the crew\n",
    "            logger.info(\"Starting crew execution...\")\n",
    "            result = self.crew.kickoff()\n",
    "                        \n",
    "            # --- Log parameters ---\n",
    "            mlflow.log_param(\"task\", self.task)\n",
    "            mlflow.log_param(\"ai_stack\", self.ai_stack)\n",
    "            mlflow.log_param(\"evaluation_model\", self.evaluation_model)\n",
    "\n",
    "            # --- Log metrics ---\n",
    "            mlflow.log_metrics(json.loads(self.crew.usage_metrics.json()))\n",
    "\n",
    "            # --- Log artifacts ---\n",
    "            artifact_files = [\n",
    "                \"output/tool_candidates.json\",\n",
    "                \"output/tool_analysis.json\",\n",
    "                \"output/tool_recommendation_report.md\",\n",
    "            ]\n",
    "            for file_path in artifact_files:\n",
    "                if os.path.exists(file_path):\n",
    "                    mlflow.log_artifact(file_path)\n",
    "\n",
    "            # --- UPDATE: Set success tag ---\n",
    "            if os.path.exists(\"output/tool_recommendation_report.md\"):\n",
    "                mlflow.set_tag(\"status\", \"SUCCESS\")\n",
    "            else:\n",
    "                mlflow.set_tag(\"status\", \"FAILED\")\n",
    "            \n",
    "            # --- Set success tag ---\n",
    "            report_path = \"output/tool_recommendation_report.md\"\n",
    "            if os.path.exists(report_path):\n",
    "                mlflow.set_tag(\"status\", \"SUCCESS\")\n",
    "                \n",
    "            # --- Evaluate the report if requested and client available ---\n",
    "            if evaluate and self.nebius_client:\n",
    "                evaluation_results = evaluate_report(\n",
    "                    report_path=report_path,\n",
    "                    nebius_client=self.nebius_client,\n",
    "                    model=self.evaluation_model\n",
    "                )\n",
    "                \n",
    "                # Log evaluation results to MLflow\n",
    "                with open(\"output/report_evaluation.json\", \"w\") as f:\n",
    "                    json.dump(evaluation_results, f, indent=2)\n",
    "                mlflow.log_artifact(\"output/report_evaluation.json\")\n",
    "                \n",
    "                # Extract and log only numeric scores\n",
    "                numeric_scores = {}\n",
    "                for key, value in evaluation_results.items():\n",
    "                    if key.endswith('_score') and isinstance(value, (int, float)):\n",
    "                        numeric_scores[key] = value\n",
    "                \n",
    "                if numeric_scores:\n",
    "                    mlflow.log_metrics(numeric_scores)\n",
    "                \n",
    "                return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run with Agent evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let's run it once without MLflow to see the typical verbose console output.\n",
    "\n",
    "# Turn on auto tracing by calling mlflow.crewai.autolog()\n",
    "mlflow.crewai.autolog()\n",
    "mlflow.set_experiment(\"Step 4 - Evaluate Agent\")\n",
    "\n",
    "ai_dev_crew = AIOpsResearchCrewEvaluated(\n",
    "    task_description, \n",
    "    existing_stack,  \n",
    "    nebius_client=nebius_client, \n",
    "    evaluation_model=\"meta-llama/Llama-3.3-70B-Instruct\")\n",
    "\n",
    "result = ai_dev_crew.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Comparing Prompts with MLflow Prompt Registry\n",
    "\n",
    "**Problem:** How do you track which prompt was used for which run and compare outputs?\n",
    "\n",
    "**MLflow Solution: MLflow Prompt Registry**\n",
    "- Version Control: Track prompt evolution with commit-based versioning and diff highlighting\n",
    "- Aliasing: Create aliases (e.g., \"production,\" \"experimental\") to isolate prompt versions\n",
    "- Reusability: Store prompts centrally for use across multiple agents and applications\n",
    "- Lineage: Connect prompts to specific model runs for comprehensive traceability\n",
    "- Collaboration: Share prompts across your team with a centralized registry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-16T18:17:49.999596Z",
     "iopub.status.busy": "2025-04-16T18:17:49.999198Z",
     "iopub.status.idle": "2025-04-16T18:17:50.041752Z",
     "shell.execute_reply": "2025-04-16T18:17:50.041362Z",
     "shell.execute_reply.started": "2025-04-16T18:17:49.999567Z"
    }
   },
   "source": [
    "### Register a propmt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_create_report_task_prompt = \"\"\"\n",
    "Make a report about the tools.\n",
    "Use markdown formatting.\n",
    "\"\"\"\n",
    "\n",
    "updated_prompt = mlflow.register_prompt(\n",
    "    name=\"create-report-prompt\",\n",
    "    template=old_create_report_task_prompt,\n",
    "    commit_message=\"Initial prompt\",\n",
    "    version_metadata={\"author\": \"ai_platform@nebius.demo\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Improved instructions to get better structured output\n",
    "\n",
    "improved_prompt = \"\"\"\n",
    "Read the analysis from the previous task's result and create a comprehensive Markdown report.\n",
    "\n",
    "The report should include:\n",
    "\n",
    "1. An introduction explaining the task ({{task}}) and existing stack ({{ai_stack}})\n",
    "2. For each tool, create a section with:\n",
    "   - Tool name and URL as a heading\n",
    "   - Description\n",
    "   - Features (as bullet points)\n",
    "   - Pros (as bullet points)\n",
    "   - Cons (as bullet points)\n",
    "   - Integration complexity\n",
    "   - Recommendation score with justification\n",
    "3. A summary/conclusion comparing the tools and providing final recommendations\n",
    "\n",
    "Use proper Markdown formatting with headings, bullet points, and emphasis where appropriate.\n",
    "Sort tools by recommendation score (descending).\n",
    "\n",
    "Your output should be a complete, well-formatted Markdown document.\n",
    "\"\"\"\n",
    "\n",
    "updated_prompt = mlflow.register_prompt(\n",
    "    name=\"create-report-prompt\",\n",
    "    template=improved_prompt,\n",
    "    commit_message=\"Improved formatting instructions and evaluation criteria\",\n",
    "    version_metadata={\"author\": \"ai_platform@nebius.demo\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load prompt (last version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of loading and using the prompt\n",
    "prompt = mlflow.load_prompt(\"create-report-prompt\")\n",
    "\n",
    "new_create_report_prompt = prompt.format(task=task_description, ai_stack=existing_stack)\n",
    "print(new_create_report_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from crewai_tools import FileReadTool\n",
    "\n",
    "class AIOpsResearchTasks:\n",
    "    def search_tools_task(self, agent, task, ai_stack):\n",
    "        \"\"\"Task to discover relevant tools for a specific task considering the existing AI stack.\"\"\"\n",
    "        return Task(\n",
    "            description=f\"\"\"\n",
    "                Find some AI tools for {task}.\n",
    "                Format however you think is best.\n",
    "            \"\"\",\n",
    "            agent=agent,\n",
    "            tools=[search_tool],\n",
    "            expected_output=\"Info about AI tools\",\n",
    "            output_file=\"output/tool_candidates.json\",\n",
    "            cache=False\n",
    "        )\n",
    "\n",
    "    def analyze_tools_task(self, agent, task, ai_stack):\n",
    "        \"\"\"Task to perform in-depth analysis of discovered tools.\"\"\"\n",
    "        return Task(\n",
    "            description=f\"\"\"\n",
    "                Check out the tools from before.\n",
    "                \n",
    "                Tell me what you think about each one.\n",
    "                Is it good? Does it work with {ai_stack}?\n",
    "                \n",
    "                Make it JSON I guess.\n",
    "            \"\"\",\n",
    "            agent=agent,\n",
    "            tools=[search_tool],\n",
    "            expected_output=\"Analysis of tools\",\n",
    "            output_file=\"output/tool_analysis.json\",\n",
    "            cache=False,\n",
    "        )\n",
    "\n",
    "    def create_report_task(self, agent, task, ai_stack):\n",
    "        \"\"\"Task to create a comprehensive report with recommendations.\"\"\"\n",
    "        return Task(\n",
    "            description=new_create_report_prompt,\n",
    "            agent=agent,\n",
    "            tools=[FileReadTool(file_path='output/tool_analysis.json')],\n",
    "            expected_output=\"A report\",\n",
    "            output_file=\"output/tool_recommendation_report.md\",\n",
    "            cache=False,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AIOpsResearchCrewUpdatedPrompts:\n",
    "    def __init__(self, task, ai_stack, nebius_client=None, evaluation_model=\"meta-llama/Llama-3.3-70B-Instruct\"):\n",
    "        \"\"\"\n",
    "        Initialize the crew with the task description and existing AI stack.\n",
    "        \n",
    "        Args:\n",
    "            task (str): Description of the task requiring AI tools\n",
    "            ai_stack (str): Comma-separated list of existing tools/frameworks used\n",
    "            nebius_client: Initialized Nebius API client for evaluation\n",
    "            evaluation_model (str): Model to use for evaluation\n",
    "        \"\"\"\n",
    "        self.task = task\n",
    "        self.ai_stack = ai_stack\n",
    "        self.run_id = None\n",
    "        self.execution_log = io.StringIO()\n",
    "        self.crew = None\n",
    "        self.nebius_client = nebius_client\n",
    "        self.evaluation_model = evaluation_model\n",
    "        \n",
    "        # Ensure output directory exists\n",
    "        os.makedirs(\"output\", exist_ok=True)\n",
    "        \n",
    "        # Initialize Nebius client if not provided\n",
    "        if self.nebius_client is None and os.environ.get(\"NEBIUS_API_KEY\"):\n",
    "            self.nebius_client = openai.OpenAI(\n",
    "                api_key=os.environ.get(\"NEBIUS_API_KEY\"),\n",
    "                base_url=\"https://api.studio.nebius.ai/v1/\"\n",
    "            )\n",
    "\n",
    "    def _clean_output_files(self):\n",
    "        \"\"\"Remove all previously generated output files if they exist.\"\"\"\n",
    "        files_to_remove = [\n",
    "            \"output/tool_candidates.json\",\n",
    "            \"output/tool_analysis.json\",\n",
    "            \"output/tool_recommendation_report.md\",\n",
    "            \"output/report_evaluation.json\"\n",
    "        ]\n",
    "        \n",
    "        for file_path in files_to_remove:\n",
    "            if os.path.exists(file_path):\n",
    "                try:\n",
    "                    os.remove(file_path)\n",
    "                    logger.info(f\"Removed existing file: {file_path}\")\n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"Failed to remove file {file_path}: {str(e)}\")\n",
    "\n",
    "    def run(self, evaluate=True):\n",
    "        \"\"\"\n",
    "        Execute the research, analysis, and reporting process with MLflow tracking.\n",
    "        \n",
    "        Args:\n",
    "            evaluate (bool): Whether to evaluate the report after generation\n",
    "        \n",
    "        Returns:\n",
    "            The result from the crew execution\n",
    "        \"\"\"\n",
    "        # Clean any existing output files\n",
    "        self._clean_output_files()\n",
    "        \n",
    "        # --- Start MLflow run ---\n",
    "        with mlflow.start_run(run_name=f\"Tool_Research_{int(time.time())}\") as mlflow_run:\n",
    "            self.run_id = mlflow_run.info.run_id\n",
    "\n",
    "            # Initialize agents\n",
    "            agents = AIOpsResearchAgents()\n",
    "            researcher = agents.researcher_agent()\n",
    "            analyst = agents.analyst_agent()\n",
    "            \n",
    "            # Initialize tasks\n",
    "            tasks = AIOpsResearchTasks()\n",
    "            search_task = tasks.search_tools_task(researcher, self.task, self.ai_stack)\n",
    "            analyze_task = tasks.analyze_tools_task(analyst, self.task, self.ai_stack)\n",
    "            report_task = tasks.create_report_task(analyst, self.task, self.ai_stack)\n",
    "            \n",
    "            # Create the crew\n",
    "            self.crew = Crew(\n",
    "                agents=[researcher, analyst],\n",
    "                tasks=[search_task, analyze_task, report_task],\n",
    "                verbose=False,\n",
    "                process=Process.sequential,\n",
    "                memory=True,\n",
    "                cache=False\n",
    "            )\n",
    "            \n",
    "            # Start the crew\n",
    "            logger.info(\"Starting crew execution...\")\n",
    "            result = self.crew.kickoff()\n",
    "                        \n",
    "            # --- Log parameters ---\n",
    "            mlflow.log_param(\"task\", self.task)\n",
    "            mlflow.log_param(\"ai_stack\", self.ai_stack)\n",
    "            mlflow.log_param(\"evaluation_model\", self.evaluation_model)\n",
    "\n",
    "            # --- Log metrics ---\n",
    "            mlflow.log_metrics(json.loads(self.crew.usage_metrics.json()))\n",
    "\n",
    "            # --- Log artifacts ---\n",
    "            artifact_files = [\n",
    "                \"output/tool_candidates.json\",\n",
    "                \"output/tool_analysis.json\",\n",
    "                \"output/tool_recommendation_report.md\",\n",
    "            ]\n",
    "            for file_path in artifact_files:\n",
    "                if os.path.exists(file_path):\n",
    "                    mlflow.log_artifact(file_path)\n",
    "\n",
    "            # --- UPDATE: Set success tag ---\n",
    "            if os.path.exists(\"output/tool_recommendation_report.md\"):\n",
    "                mlflow.set_tag(\"status\", \"SUCCESS\")\n",
    "            else:\n",
    "                mlflow.set_tag(\"status\", \"FAILED\")\n",
    "            \n",
    "            # --- Set success tag ---\n",
    "            report_path = \"output/tool_recommendation_report.md\"\n",
    "            if os.path.exists(report_path):\n",
    "                mlflow.set_tag(\"status\", \"SUCCESS\")\n",
    "                \n",
    "            # --- Evaluate the report if requested and client available ---\n",
    "            if evaluate and self.nebius_client:\n",
    "                evaluation_results = evaluate_report(\n",
    "                    report_path=report_path,\n",
    "                    nebius_client=self.nebius_client,\n",
    "                    model=self.evaluation_model\n",
    "                )\n",
    "                \n",
    "                # Log evaluation results to MLflow\n",
    "                with open(\"output/report_evaluation.json\", \"w\") as f:\n",
    "                    json.dump(evaluation_results, f, indent=2)\n",
    "                mlflow.log_artifact(\"output/report_evaluation.json\")\n",
    "                \n",
    "                # Extract and log only numeric scores\n",
    "                numeric_scores = {}\n",
    "                for key, value in evaluation_results.items():\n",
    "                    if key.endswith('_score') and isinstance(value, (int, float)):\n",
    "                        numeric_scores[key] = value\n",
    "                \n",
    "                if numeric_scores:\n",
    "                    mlflow.log_metrics(numeric_scores)\n",
    "                \n",
    "                return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T21:00:24.934477Z",
     "iopub.status.busy": "2025-03-29T21:00:24.933960Z",
     "iopub.status.idle": "2025-03-29T21:00:24.977515Z",
     "shell.execute_reply": "2025-03-29T21:00:24.977025Z",
     "shell.execute_reply.started": "2025-03-29T21:00:24.934446Z"
    }
   },
   "source": [
    "### Run with Different Prompts and Compare in MLflow UI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let's run it once without MLflow to see the typical verbose console output.\n",
    "\n",
    "# Turn on auto tracing by calling mlflow.crewai.autolog()\n",
    "mlflow.crewai.autolog()\n",
    "mlflow.set_experiment(\"Step 5 - Comparing prompts\")\n",
    "\n",
    "ai_dev_crew = AIOpsResearchCrewUpdatedPrompts(\n",
    "    task_description, \n",
    "    existing_stack,\n",
    "    nebius_client=nebius_client, \n",
    "    evaluation_model=\"meta-llama/Llama-3.3-70B-Instruct\")\n",
    "\n",
    "result = ai_dev_crew.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Conclusion\n",
    "\n",
    "Developing robust and reliable LLM agents requires moving beyond simple script execution. By integrating **Managed MLflow**, we gain crucial capabilities:\n",
    "\n",
    "*   **Traceability:** Understand exactly what happened during an agent run via logged artifacts (`execution_log.txt`, `error.log`).\n",
    "*   **Reproducibility:** Capture configurations as parameters, ensuring you know precisely what setup produced a given result.\n",
    "*   **Comparison:** Systematically evaluate the impact of changes (prompts, LLMs, agent logic) using the MLflow UI's compare feature for parameters, metrics, and artifacts.\n",
    "*   **Debugging:** Quickly identify and analyze failed runs using status tags and dedicated error logs.\n",
    "*   **Performance Monitoring:** Track execution time and other metrics to identify bottlenecks and regressions.\n",
    "*   **Quality & Cost Evaluation:** Provide a central repository to store and analyze quality scores (human or LLM-judged) and resource usage (tokens, cost) for holistic optimization.\n",
    "\n",
    "MLflow transforms agent development into a more transparent, measurable, and efficient engineering process, enabling faster iteration and more reliable outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlflow-nebius",
   "language": "python",
   "name": "mlflow-nebius"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
