{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nebius Managed Service for MLflow Demo\n",
    "\n",
    "This demo shows you how key use cases of using the Nebius Managed Service for MLflow for AI development."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "\n",
    "1. Launch your instance of the Managed Service for MLflow with [the MLflow quickstart](https://docs.nebius.com/mlflow/quickstart).\n",
    "2. Set up your API key to connect to Nebius AI Studio with [the AI Studio quickstart](https://docs.nebius.com/studio/inference/quickstart).\n",
    "\n",
    "> **Note:** Launching an MLflow cluster may take 30-60 minutes to be fully provisioned and ready to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install mlflow==2.20.2 python-dotenv openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Secrets and Environment Variables\n",
    "\n",
    "You will need to set the following environment variables where this notebook is running, so that the code in the following cells can connect to both Nebius Managed Service for MLflow and Nebius AI Studio. \n",
    "\n",
    "MLflow:<br>\n",
    "`MLFLOW_TRACKING_SERVER_CERT_PATH`<br>\n",
    "`MLFLOW_TRACKING_URI`<br>\n",
    "`MLFLOW_TRACKING_USERNAME`<br>\n",
    "`MLFLOW_TRACKING_PASSWORD`<br>\n",
    "\n",
    "AI Studio:<br>\n",
    "`NEBIUS_API_KEY`\n",
    "\n",
    "### Environment Setup\n",
    "\n",
    "To set the environment variables, run the following cell. You may choose to set them interactively or by loading from a `.env` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Add the parent directory to Python path\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "\n",
    "from env_setup import setup_env_from_file, setup_env_interactive, verify_env_setup\n",
    "\n",
    "# Option 1: Interactive setup\n",
    "# setup_env_interactive()\n",
    "\n",
    "# Option 2: Load from .env file\n",
    "setup_env_from_file('../.env')\n",
    "\n",
    "# Verify the setup\n",
    "verify_env_setup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check connection to MLflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow \n",
    "\n",
    "# List experiments in MLflow\n",
    "mlflow.search_experiments()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up Nebius AI Studio client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "API_KEY = os.environ.get(\"NEBIUS_API_KEY\")\n",
    "\n",
    "# Instantiate the client instance\n",
    "nebius_client = openai.OpenAI(api_key=API_KEY,\n",
    "                              base_url=\"https://api.studio.nebius.ai/v1/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 1: MLflow Tracking Quickstart\n",
    "\n",
    "https://mlflow.org/docs/latest/getting-started/intro-quickstart/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from mlflow.models import infer_signature\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "\n",
    "# Load the Iris dataset\n",
    "X, y = datasets.load_iris(return_X_y=True)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Define the model hyperparameters\n",
    "params = {\n",
    "    \"solver\": \"lbfgs\",\n",
    "    \"max_iter\": 1000,\n",
    "    \"multi_class\": \"auto\",\n",
    "    \"random_state\": 8888,\n",
    "}\n",
    "\n",
    "# Train the model\n",
    "lr = LogisticRegression(**params)\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = lr.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log the model and its metadata to MLflow\n",
    "\n",
    "- Initiate an MLflow run context to start a new run that we will log the model and metadata to.\n",
    "- Log model parameters and performance metrics.\n",
    "- Tag the run for easy retrieval.\n",
    "- Register the model in the MLflow Model Registry while logging (saving) the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a new MLflow Experiment\n",
    "mlflow.set_experiment(\"MLflow Quickstart\")\n",
    "\n",
    "# Start an MLflow run\n",
    "with mlflow.start_run():\n",
    "    # Log the hyperparameters\n",
    "    mlflow.log_params(params)\n",
    "\n",
    "    # Log the loss metric\n",
    "    mlflow.log_metric(\"accuracy\", accuracy)\n",
    "\n",
    "    # Set a tag that we can use to remind ourselves what this run was for\n",
    "    mlflow.set_tag(\"Training Info\", \"Basic LR model for iris data\")\n",
    "\n",
    "    # Infer the model signature\n",
    "    signature = infer_signature(X_train, lr.predict(X_train))\n",
    "\n",
    "    # Log the model\n",
    "    model_info = mlflow.sklearn.log_model(\n",
    "        sk_model=lr,\n",
    "        artifact_path=\"iris_model\",\n",
    "        signature=signature,\n",
    "        input_example=X_train,\n",
    "        registered_model_name=\"tracking-quickstart\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_info.model_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the model as a Python Function (pyfunc) and use it for inference\n",
    "\n",
    "- Loading the model using MLflow's pyfunc flavor.\n",
    "- Running Predict on new data using the loaded model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model back for predictions as a generic Python Function model\n",
    "loaded_model = mlflow.pyfunc.load_model(model_info.model_uri)\n",
    "\n",
    "loaded_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions \n",
    "\n",
    "predictions = loaded_model.predict(X_test)\n",
    "\n",
    "iris_feature_names = datasets.load_iris().feature_names\n",
    "\n",
    "result = pd.DataFrame(X_test, columns=iris_feature_names)\n",
    "result[\"actual_class\"] = y_test\n",
    "result[\"predicted_class\"] = predictions\n",
    "\n",
    "result[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 2: MLflow Tracing for LLM Observability\n",
    "\n",
    "Traces enhances LLM observability in your Generative AI (GenAI) applications by capturing detailed information about the execution details. \n",
    "\n",
    "https://mlflow.org/docs/latest/tracing/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic Tracing of LLM calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import openai\n",
    "\n",
    "tracing_experiment =mlflow.set_experiment(\"MLflow Tracing\")\n",
    "\n",
    "# Enable MLflow automatic tracing for OpenAI with one line of code!\n",
    "mlflow.openai.autolog()\n",
    "\n",
    "\n",
    "# Time to call the LLM -- tracing is done automatically\n",
    "nebius_client.chat.completions.create(\n",
    "    model=\"meta-llama/Llama-3.3-70B-Instruct\",\n",
    "    temperature=0.95,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a chatbot.\"},\n",
    "        {\"role\": \"user\", \"content\": \"What is the weather like today?\"},\n",
    "    ],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual Tracing\n",
    "\n",
    "1. Instrument a function with @mlflow.trace decorator.\n",
    "2. Instrument any block of code using mlflow.start_span context manager.\n",
    "3. Grouping or annotating traces using a tag.\n",
    "4. Disabling trace globally.\n",
    "\n",
    "https://mlflow.org/docs/latest/tracing/#manual-tracing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from mlflow.entities import SpanType\n",
    "import openai\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use MLflow tracing to trace the execution of a function\n",
    "\n",
    "@mlflow.trace(span_type=\"func\", attributes={\"key\": \"value\"})\n",
    "def add_1(x):\n",
    "    return x + 1\n",
    "\n",
    "\n",
    "@mlflow.trace(span_type=\"func\", attributes={\"key\": \"value\"})\n",
    "def minus_1(x):\n",
    "    return x - 1\n",
    "\n",
    "\n",
    "@mlflow.trace(name=\"Trace Test\")\n",
    "def trace_test(x):\n",
    "    step1 = add_1(x)\n",
    "    return minus_1(step1)\n",
    "\n",
    "\n",
    "trace_test(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Integrate tracing into your LLM workflow\n",
    "\n",
    "mlflow.openai.autolog()\n",
    "\n",
    "@mlflow.trace(span_type=SpanType.CHAIN)\n",
    "def run(question):\n",
    "    messages = build_messages(question)\n",
    "    # MLflow automatically generates a span for OpenAI invocation\n",
    "    response = nebius_client.chat.completions.create(\n",
    "        # model=\"gpt-4o-mini\",\n",
    "        model=\"meta-llama/Llama-3.3-70B-Instruct\",\n",
    "        max_tokens=100,\n",
    "        messages=messages,\n",
    "    )\n",
    "    return parse_response(response)\n",
    "\n",
    "\n",
    "@mlflow.trace\n",
    "def build_messages(question):\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful chatbot.\"},\n",
    "        {\"role\": \"user\", \"content\": question},\n",
    "    ]\n",
    "\n",
    "@mlflow.trace\n",
    "def parse_response(response):\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "\n",
    "run(\"What is MLflow?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Catch exceptions in your traces\n",
    "\n",
    "\n",
    "# Define methods to be traced\n",
    "@mlflow.trace(span_type=SpanType.TOOL, attributes={\"time\": \"morning\"})\n",
    "def morning_greeting(name: str):\n",
    "    time.sleep(1)\n",
    "    mlflow.update_current_trace(tags={\"person\": name})\n",
    "    return f\"Good morning {name}.\"\n",
    "\n",
    "\n",
    "@mlflow.trace(span_type=SpanType.TOOL, attributes={\"time\": \"evening\"})\n",
    "def evening_greeting(name: str):\n",
    "    time.sleep(1)\n",
    "    mlflow.update_current_trace(tags={\"person\": name})\n",
    "    return f\"Good evening {name}.\"\n",
    "\n",
    "\n",
    "@mlflow.trace(span_type=SpanType.TOOL)\n",
    "def goodbye():\n",
    "    raise Exception(\"Cannot say goodbye\")\n",
    "\n",
    "\n",
    "# Execute the methods within different experiments\n",
    "morning_experiment = mlflow.set_experiment(\"Morning Experiment\")\n",
    "morning_greeting(\"Tom\")\n",
    "morning_greeting(\"Mike\")\n",
    "\n",
    "# Get the timestamp in milliseconds\n",
    "morning_time = int(time.time() * 1000)\n",
    "\n",
    "evening_experiment = mlflow.set_experiment(\"Evening Experiment\")\n",
    "experiment_ids = [morning_experiment.experiment_id, evening_experiment.experiment_id]\n",
    "evening_greeting(\"Mary\")\n",
    "goodbye()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the timestamp in milliseconds\n",
    "one_hour_ago = int(time.time() - 3600) * 1000  # 3600 seconds = 1 hour in milliseconds\n",
    "\n",
    "one_hour_ago"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search and analyze traces\n",
    "\n",
    "mlflow.search_traces(\n",
    "    tracing_experiment.experiment_id, \n",
    "    filter_string=f\"timestamp_ms < {one_hour_ago}\",\n",
    ")[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 3: Automatic Tracking of AI Agent Behavior\n",
    "\n",
    "- Tracing LangGraph with MLflow https://mlflow.org/docs/latest/tracing/integrations/langgraph \n",
    "- Example: Code generation with RAG and self-correction with https://langchain-ai.github.io/langgraph/tutorials/code_assistant/langgraph_code_assistant/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain_openai langchain langgraph langchain_core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tracing LangGraph "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "import mlflow\n",
    "\n",
    "from langchain_core.messages import AIMessage, ToolCall\n",
    "from langchain_core.outputs import ChatGeneration, ChatResult\n",
    "from langchain_core.tools import tool\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "# Enabling tracing for LangGraph (LangChain)\n",
    "mlflow.langchain.autolog()\n",
    "\n",
    "# Optional: Set a tracking URI and an experiment\n",
    "# mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "mlflow.set_experiment(\"LangGraph\")\n",
    "\n",
    "\n",
    "@tool\n",
    "def get_weather(city: Literal[\"nyc\", \"sf\"]):\n",
    "    \"\"\"Use this to get weather information.\"\"\"\n",
    "    if city == \"nyc\":\n",
    "        return \"It might be cloudy in nyc\"\n",
    "    elif city == \"sf\":\n",
    "        return \"It's always sunny in sf\"\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "tools = [get_weather]\n",
    "graph = create_react_agent(llm, tools)\n",
    "\n",
    "# Invoke the graph\n",
    "result = graph.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in tokyo?\"}]}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Track Code Generation with RAG and self-correction\n",
    "\n",
    "- Original example: https://langchain-ai.github.io/langgraph/tutorials/code_assistant/langgraph_code_assistant/ \n",
    "- MLflow integration: https://mlflow.org/docs/latest/tracing/integrations/langgraph#adding-spans-within-a-node-or-a-tool \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -U langchain_community langchain-openai langchain-anthropic langchain langgraph bs4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load LangChain Expression Language (LCEL) docs as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as Soup\n",
    "from langchain_community.document_loaders.recursive_url_loader import RecursiveUrlLoader\n",
    "\n",
    "# LCEL docs\n",
    "url = \"https://python.langchain.com/docs/concepts/lcel/\"\n",
    "loader = RecursiveUrlLoader(\n",
    "    url=url, max_depth=20, extractor=lambda x: Soup(x, \"html.parser\").text\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "# Sort the list based on the URLs and get the text\n",
    "d_sorted = sorted(docs, key=lambda x: x.metadata[\"source\"])\n",
    "d_reversed = list(reversed(d_sorted))\n",
    "concatenated_content = \"\\n\\n\\n --- \\n\\n\\n\".join(\n",
    "    [doc.page_content for doc in d_reversed]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a code_gen_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Create a code_gen_chain with OpenAI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "### OpenAI\n",
    "\n",
    "# Grader prompt\n",
    "code_gen_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"You are a coding assistant with expertise in LCEL, LangChain expression language. \\n \n",
    "    Here is a full set of LCEL documentation:  \\n ------- \\n  {context} \\n ------- \\n Answer the user \n",
    "    question based on the above provided documentation. Ensure any code you provide can be executed \\n \n",
    "    with all required imports and variables defined. Structure your answer with a description of the code solution. \\n\n",
    "    Then list the imports. And finally list the functioning code block. Here is the user question:\"\"\",\n",
    "        ),\n",
    "        (\"placeholder\", \"{messages}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# Data model\n",
    "class code(BaseModel):\n",
    "    \"\"\"Schema for code solutions to questions about LCEL.\"\"\"\n",
    "\n",
    "    prefix: str = Field(description=\"Description of the problem and approach\")\n",
    "    imports: str = Field(description=\"Code block import statements\")\n",
    "    code: str = Field(description=\"Code block not including import statements\")\n",
    "\n",
    "\n",
    "expt_llm = \"gpt-4o-mini\"\n",
    "llm = ChatOpenAI(temperature=0, model=expt_llm)\n",
    "code_gen_chain_oai = code_gen_prompt | llm.with_structured_output(code)\n",
    "question = \"How do I build a RAG chain in LCEL?\"\n",
    "solution = code_gen_chain_oai.invoke(\n",
    "    {\"context\": concatenated_content, \"messages\": [(\"user\", question)]}\n",
    ")\n",
    "solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Create a code_gen_chain with Anthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "### Anthropic\n",
    "\n",
    "# Prompt to enforce tool use\n",
    "code_gen_prompt_claude = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"<instructions> You are a coding assistant with expertise in LCEL, LangChain expression language. \\n \n",
    "    Here is the LCEL documentation:  \\n ------- \\n  {context} \\n ------- \\n Answer the user  question based on the \\n \n",
    "    above provided documentation. Ensure any code you provide can be executed with all required imports and variables \\n\n",
    "    defined. Structure your answer: 1) a prefix describing the code solution, 2) the imports, 3) the functioning code block. \\n\n",
    "    Invoke the code tool to structure the output correctly. </instructions> \\n Here is the user question:\"\"\",\n",
    "        ),\n",
    "        (\"placeholder\", \"{messages}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# LLM\n",
    "expt_llm = \"claude-3-opus-20240229\"\n",
    "llm = ChatAnthropic(\n",
    "    model=expt_llm,\n",
    "    default_headers={\"anthropic-beta\": \"tools-2024-04-04\"},\n",
    ")\n",
    "\n",
    "structured_llm_claude = llm.with_structured_output(code, include_raw=True)\n",
    "\n",
    "\n",
    "# Optional: Check for errors in case tool use is flaky\n",
    "def check_claude_output(tool_output):\n",
    "    \"\"\"Check for parse error or failure to call the tool\"\"\"\n",
    "\n",
    "    # Error with parsing\n",
    "    if tool_output[\"parsing_error\"]:\n",
    "        # Report back output and parsing errors\n",
    "        print(\"Parsing error!\")\n",
    "        raw_output = str(tool_output[\"raw\"].content)\n",
    "        error = tool_output[\"parsing_error\"]\n",
    "        raise ValueError(\n",
    "            f\"Error parsing your output! Be sure to invoke the tool. Output: {raw_output}. \\n Parse error: {error}\"\n",
    "        )\n",
    "\n",
    "    # Tool was not invoked\n",
    "    elif not tool_output[\"parsed\"]:\n",
    "        print(\"Failed to invoke tool!\")\n",
    "        raise ValueError(\n",
    "            \"You did not use the provided tool! Be sure to invoke the tool to structure the output.\"\n",
    "        )\n",
    "    return tool_output\n",
    "\n",
    "\n",
    "# Chain with output check\n",
    "code_chain_claude_raw = (\n",
    "    code_gen_prompt_claude | structured_llm_claude | check_claude_output\n",
    ")\n",
    "\n",
    "\n",
    "def insert_errors(inputs):\n",
    "    \"\"\"Insert errors for tool parsing in the messages\"\"\"\n",
    "\n",
    "    # Get errors\n",
    "    error = inputs[\"error\"]\n",
    "    messages = inputs[\"messages\"]\n",
    "    messages += [\n",
    "        (\n",
    "            \"assistant\",\n",
    "            f\"Retry. You are required to fix the parsing errors: {error} \\n\\n You must invoke the provided tool.\",\n",
    "        )\n",
    "    ]\n",
    "    return {\n",
    "        \"messages\": messages,\n",
    "        \"context\": inputs[\"context\"],\n",
    "    }\n",
    "\n",
    "\n",
    "# This will be run as a fallback chain\n",
    "fallback_chain = insert_errors | code_chain_claude_raw\n",
    "N = 3  # Max re-tries\n",
    "code_gen_chain_re_try = code_chain_claude_raw.with_fallbacks(\n",
    "    fallbacks=[fallback_chain] * N, exception_key=\"error\"\n",
    ")\n",
    "\n",
    "\n",
    "def parse_output(solution):\n",
    "    \"\"\"When we add 'include_raw=True' to structured output,\n",
    "    it will return a dict w 'raw', 'parsed', 'parsing_error'.\"\"\"\n",
    "\n",
    "    return solution[\"parsed\"]\n",
    "\n",
    "\n",
    "# Optional: With re-try to correct for failure to invoke tool\n",
    "code_gen_chain = code_gen_chain_re_try | parse_output\n",
    "\n",
    "# No re-try\n",
    "code_gen_chain = code_gen_prompt_claude | structured_llm_claude | parse_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "question = \"How do I build a RAG chain in LCEL?\"\n",
    "solution = code_gen_chain.invoke(\n",
    "    {\"context\": concatenated_content, \"messages\": [(\"user\", question)]}\n",
    ")\n",
    "solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of our graph.\n",
    "\n",
    "    Attributes:\n",
    "        error : Binary flag for control flow to indicate whether test error was tripped\n",
    "        messages : With user question, error messages, reasoning\n",
    "        generation : Code solution\n",
    "        iterations : Number of tries\n",
    "    \"\"\"\n",
    "\n",
    "    error: str\n",
    "    messages: List\n",
    "    generation: str\n",
    "    iterations: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Parameter\n",
    "\n",
    "# Max tries\n",
    "max_iterations = 3\n",
    "# Reflect\n",
    "# flag = 'reflect'\n",
    "flag = \"do not reflect\"\n",
    "\n",
    "### Nodes\n",
    "\n",
    "\n",
    "def generate(state: GraphState):\n",
    "    \"\"\"\n",
    "    Generate a code solution\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, generation\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---GENERATING CODE SOLUTION---\")\n",
    "\n",
    "    # State\n",
    "    messages = state[\"messages\"]\n",
    "    iterations = state[\"iterations\"]\n",
    "    error = state[\"error\"]\n",
    "\n",
    "    # We have been routed back to generation with an error\n",
    "    if error == \"yes\":\n",
    "        messages += [\n",
    "            (\n",
    "                \"user\",\n",
    "                \"Now, try again. Invoke the code tool to structure the output with a prefix, imports, and code block:\",\n",
    "            )\n",
    "        ]\n",
    "\n",
    "    # Solution\n",
    "    code_solution = code_gen_chain.invoke(\n",
    "        {\"context\": concatenated_content, \"messages\": messages}\n",
    "    )\n",
    "    messages += [\n",
    "        (\n",
    "            \"assistant\",\n",
    "            f\"{code_solution.prefix} \\n Imports: {code_solution.imports} \\n Code: {code_solution.code}\",\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    # Increment\n",
    "    iterations = iterations + 1\n",
    "    return {\"generation\": code_solution, \"messages\": messages, \"iterations\": iterations}\n",
    "\n",
    "\n",
    "\n",
    "def reflect(state: GraphState):\n",
    "    \"\"\"\n",
    "    Reflect on errors\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, generation\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---GENERATING CODE SOLUTION---\")\n",
    "\n",
    "    # State\n",
    "    messages = state[\"messages\"]\n",
    "    iterations = state[\"iterations\"]\n",
    "    code_solution = state[\"generation\"]\n",
    "\n",
    "    # Prompt reflection\n",
    "\n",
    "    # Add reflection\n",
    "    reflections = code_gen_chain.invoke(\n",
    "        {\"context\": concatenated_content, \"messages\": messages}\n",
    "    )\n",
    "    messages += [(\"assistant\", f\"Here are reflections on the error: {reflections}\")]\n",
    "    return {\"generation\": code_solution, \"messages\": messages, \"iterations\": iterations}\n",
    "\n",
    "\n",
    "### Edges\n",
    "\n",
    "\n",
    "def decide_to_finish(state: GraphState):\n",
    "    \"\"\"\n",
    "    Determines whether to finish.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Next node to call\n",
    "    \"\"\"\n",
    "    error = state[\"error\"]\n",
    "    iterations = state[\"iterations\"]\n",
    "\n",
    "    if error == \"no\" or iterations == max_iterations:\n",
    "        print(\"---DECISION: FINISH---\")\n",
    "        return \"end\"\n",
    "    else:\n",
    "        print(\"---DECISION: RE-TRY SOLUTION---\")\n",
    "        if flag == \"reflect\":\n",
    "            return \"reflect\"\n",
    "        else:\n",
    "            return \"generate\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Code check with integrated tracing with MLflow\n",
    "\n",
    "def code_check(state: GraphState):\n",
    "    # State\n",
    "    messages = state[\"messages\"]\n",
    "    code_solution = state[\"generation\"]\n",
    "    iterations = state[\"iterations\"]\n",
    "\n",
    "    # Get solution components\n",
    "    imports = code_solution.imports\n",
    "    code = code_solution.code\n",
    "\n",
    "    # Check imports\n",
    "    try:\n",
    "        # Create a child span manually with mlflow.start_span() API\n",
    "        with mlflow.start_span(name=\"import_check\", span_type=SpanType.TOOL) as span:\n",
    "            span.set_inputs(imports)\n",
    "            exec(imports)\n",
    "            span.set_outputs(\"ok\")\n",
    "    except Exception as e:\n",
    "        error_message = [(\"user\", f\"Your solution failed the import test: {e}\")]\n",
    "        messages += error_message\n",
    "        return {\n",
    "            \"generation\": code_solution,\n",
    "            \"messages\": messages,\n",
    "            \"iterations\": iterations,\n",
    "            \"error\": \"yes\",\n",
    "        }\n",
    "\n",
    "    # Check execution\n",
    "    try:\n",
    "        code = imports + \"\\n\" + code\n",
    "        with mlflow.start_span(name=\"execution_check\", span_type=SpanType.TOOL) as span:\n",
    "            span.set_inputs(code)\n",
    "            exec(code)\n",
    "            span.set_outputs(\"ok\")\n",
    "    except Exception as e:\n",
    "        error_message = [(\"user\", f\"Your solution failed the code execution test: {e}\")]\n",
    "        messages += error_message\n",
    "        return {\n",
    "            \"generation\": code_solution,\n",
    "            \"messages\": messages,\n",
    "            \"iterations\": iterations,\n",
    "            \"error\": \"yes\",\n",
    "        }\n",
    "\n",
    "    # No errors\n",
    "    return {\n",
    "        \"generation\": code_solution,\n",
    "        \"messages\": messages,\n",
    "        \"iterations\": iterations,\n",
    "        \"error\": \"no\",\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create workflow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END, StateGraph, START\n",
    "\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# Define the nodes\n",
    "workflow.add_node(\"generate\", generate)  # generation solution\n",
    "workflow.add_node(\"check_code\", code_check)  # check code\n",
    "workflow.add_node(\"reflect\", reflect)  # reflect\n",
    "\n",
    "# Build graph\n",
    "workflow.add_edge(START, \"generate\")\n",
    "workflow.add_edge(\"generate\", \"check_code\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"check_code\",\n",
    "    decide_to_finish,\n",
    "    {\n",
    "        \"end\": END,\n",
    "        \"reflect\": \"reflect\",\n",
    "        \"generate\": \"generate\",\n",
    "    },\n",
    ")\n",
    "workflow.add_edge(\"reflect\", \"generate\")\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"How can I directly pass a string to a runnable and use it to construct the input needed for my prompt?\"\n",
    "solution = app.invoke({\"messages\": [(\"user\", question)], \"iterations\": 0, \"error\": \"\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tracing CrewAI Agents\n",
    "\n",
    "- Example https://mlflow.org/docs/latest/tracing/integrations/crewai "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install crewai crewai_tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "# Turn on auto tracing by calling mlflow.crewai.autolog()\n",
    "mlflow.crewai.autolog()\n",
    "\n",
    "mlflow.set_experiment(\"CrewAI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from crewai import Agent, Crew, Task\n",
    "from crewai.knowledge.source.string_knowledge_source import StringKnowledgeSource\n",
    "from crewai_tools import SerperDevTool, WebsiteSearchTool\n",
    "\n",
    "from textwrap import dedent\n",
    "\n",
    "content = \"Users name is John. He is 30 years old and lives in San Francisco.\"\n",
    "string_source = StringKnowledgeSource(\n",
    "    content=content, metadata={\"preference\": \"personal\"}\n",
    ")\n",
    "\n",
    "search_tool = WebsiteSearchTool()\n",
    "\n",
    "class TripAgents:\n",
    "    def city_selection_agent(self):\n",
    "        return Agent(\n",
    "            role=\"City Selection Expert\",\n",
    "            goal=\"Select the best city based on weather, season, and prices\",\n",
    "            backstory=\"An expert in analyzing travel data to pick ideal destinations\",\n",
    "            tools=[\n",
    "                search_tool,\n",
    "            ],\n",
    "            verbose=True,\n",
    "        )\n",
    "\n",
    "    def local_expert(self):\n",
    "        return Agent(\n",
    "            role=\"Local Expert at this city\",\n",
    "            goal=\"Provide the BEST insights about the selected city\",\n",
    "            backstory=\"\"\"A knowledgeable local guide with extensive information\n",
    "        about the city, it's attractions and customs\"\"\",\n",
    "            tools=[search_tool],\n",
    "            verbose=True,\n",
    "        )\n",
    "\n",
    "class TripTasks:\n",
    "    def identify_task(self, agent, origin, cities, interests, range):\n",
    "        return Task(\n",
    "            description=dedent(\n",
    "                f\"\"\"\n",
    "                Analyze and select the best city for the trip based\n",
    "                on specific criteria such as weather patterns, seasonal\n",
    "                events, and travel costs. This task involves comparing\n",
    "                multiple cities, considering factors like current weather\n",
    "                conditions, upcoming cultural or seasonal events, and\n",
    "                overall travel expenses.\n",
    "                Your final answer must be a detailed\n",
    "                report on the chosen city, and everything you found out\n",
    "                about it, including the actual flight costs, weather\n",
    "                forecast and attractions.\n",
    "\n",
    "                Traveling from: {origin}\n",
    "                City Options: {cities}\n",
    "                Trip Date: {range}\n",
    "                Traveler Interests: {interests}\n",
    "            \"\"\"\n",
    "            ),\n",
    "            agent=agent,\n",
    "            expected_output=\"Detailed report on the chosen city including flight costs, weather forecast, and attractions\",\n",
    "        )\n",
    "\n",
    "    def gather_task(self, agent, origin, interests, range):\n",
    "        return Task(\n",
    "            description=dedent(\n",
    "                f\"\"\"\n",
    "                As a local expert on this city you must compile an\n",
    "                in-depth guide for someone traveling there and wanting\n",
    "                to have THE BEST trip ever!\n",
    "                Gather information about key attractions, local customs,\n",
    "                special events, and daily activity recommendations.\n",
    "                Find the best spots to go to, the kind of place only a\n",
    "                local would know.\n",
    "                This guide should provide a thorough overview of what\n",
    "                the city has to offer, including hidden gems, cultural\n",
    "                hotspots, must-visit landmarks, weather forecasts, and\n",
    "                high level costs.\n",
    "                The final answer must be a comprehensive city guide,\n",
    "                rich in cultural insights and practical tips,\n",
    "                tailored to enhance the travel experience.\n",
    "\n",
    "                Trip Date: {range}\n",
    "                Traveling from: {origin}\n",
    "                Traveler Interests: {interests}\n",
    "            \"\"\"\n",
    "            ),\n",
    "            agent=agent,\n",
    "            expected_output=\"Comprehensive city guide including hidden gems, cultural hotspots, and practical travel tips\",\n",
    "        )\n",
    "\n",
    "\n",
    "class TripCrew:\n",
    "    def __init__(self, origin, cities, date_range, interests):\n",
    "        self.cities = cities\n",
    "        self.origin = origin\n",
    "        self.interests = interests\n",
    "        self.date_range = date_range\n",
    "\n",
    "    def run(self):\n",
    "        agents = TripAgents()\n",
    "        tasks = TripTasks()\n",
    "\n",
    "        city_selector_agent = agents.city_selection_agent()\n",
    "        local_expert_agent = agents.local_expert()\n",
    "\n",
    "        identify_task = tasks.identify_task(\n",
    "            city_selector_agent,\n",
    "            self.origin,\n",
    "            self.cities,\n",
    "            self.interests,\n",
    "            self.date_range,\n",
    "        )\n",
    "        gather_task = tasks.gather_task(\n",
    "            local_expert_agent, self.origin, self.interests, self.date_range\n",
    "        )\n",
    "\n",
    "        crew = Crew(\n",
    "            agents=[city_selector_agent, local_expert_agent],\n",
    "            tasks=[identify_task, gather_task],\n",
    "            verbose=True,\n",
    "            memory=True,\n",
    "            knowledge={\n",
    "                \"sources\": [string_source],\n",
    "                \"metadata\": {\"preference\": \"personal\"},\n",
    "                \"collection_name\":\"knowledge\",\n",
    "            },\n",
    "        )\n",
    "\n",
    "        result = crew.kickoff()\n",
    "        return result\n",
    "\n",
    "\n",
    "trip_crew = TripCrew(\"California\", \"Tokyo\", \"Dec 12 - Dec 20\", \"sports\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the crew\n",
    "\n",
    "result = trip_crew.run()\n",
    "result.dict()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlflow-nebius",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
