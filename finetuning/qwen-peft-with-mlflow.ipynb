{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning Open-Source LLM using LoRA with MLflow and PEFT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c9d96afb-a47e-4f15-a208-92ce50ae6c73",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Overview\n",
    "\n",
    "Many powerful open-source LLMs have emerged and are easily accessible. However, they are not designed to be deployed to your production environment out-of-the-box; instead, you have to **fine-tune** them for your specific tasks, such as a chatbot, content generation, etc. One challenge, though, is that training LLMs is usually very expensive. Even if your dataset for fine-tuning is small, the backpropagation step needs to compute gradients for billions of parameters. For example, fully fine-tuning a 7B model requires 112GB of VRAM, i.e. at least two 80GB H100 GPUs. Fortunately, there are many research efforts on how to reduce the cost of LLM fine-tuning.\n",
    "\n",
    "In this tutorial, we will demonstrate how to build a Python coding aassitant by fine-tuning the Qwen2.5 7B model.\n",
    "\n",
    "### What You Will Learn\n",
    "1. Hands-on learning of the typical LLM fine-tuning process.\n",
    "2. Understand how to use **LoRA** and **PEFT** to overcome the GPU memory limitation for fine-tuning.\n",
    "3. Manage the model training cycle using **MLflow** to log the model artifacts, hyperparameters, metrics, and prompts.\n",
    "4. How to save prompt template and inference parameters (e.g. max_token_length) in MLflow to simplify prediction interface.\n",
    "\n",
    "### Key Actors\n",
    "In this tutorial, you will learn about the techniques and methods behind efficient LLM fine-tuning by actually running the code. There are more detailed explanations for each cell below, but let's start with a brief preview of a few main important libraries/methods used in this tutorial.\n",
    "\n",
    "* [Qwen/Qwen2.5-7B](https://huggingface.co/Qwen/Qwen2.5-7B) model is a pretrained text-generation model with 7 billion parameters, developed by [Qwen](https://github.com/QwenLM). Qwen2.5 is the latest series of Qwen large language models.\n",
    "* [LoRA](https://huggingface.co/docs/diffusers/en/training/lora) is a novel method that allows us to fine-tune large foundational models with limited GPU resources. It reduces the number of trainable parameters by learning pairs of rank-decomposition matrices.\n",
    "* [PEFT](https://huggingface.co/docs/peft/en/index) is a library developed by HuggingFaceü§ó, that enables developers to easily integrate various optimization methods with pretrained models available on the HuggingFace Hub. With PEFT, you can apply LoRA to the pretrained model with a few lines of configurations and run fine-tuning just like the normal Transformers model training.\n",
    "* [MLflow](https://mlflow.org/) manages an exploding number of configurations, assets, and metrics during the LLM training on your behalf. MLflow is natively integrated with Transformers and PEFT, and plays a crucial role in organizing the fine-tuning cycle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Set up\n",
    "\n",
    "### Hardware Requirement\n",
    "This notebook has been tested on a single NVIDIA H100 GPU with 80GB of VRAM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0f59cf87-864b-4fba-a587-0edcb85c4b2b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Install Python Libraries\n",
    "\n",
    "This tutorial utilizes the following Python libraries:\n",
    "\n",
    "* [mlflow](https://pypi.org/project/mlflow/) - for tracking parameters, metrics, and saving trained models.\n",
    "* [transformers](https://pypi.org/project/transformers/) - for defining the model, tokenizer, and trainer.\n",
    "* [peft](https://pypi.org/project/peft/) - for creating a LoRA adapter on top of the Transformer model.\n",
    "* [trl](https://pypi.org/project/trl/) -  for post-training foundation models using advanced techniques like Supervised Fine-Tuning (SFT), Proximal Policy Optimization (PPO), and Direct Preference Optimization (DPO).\n",
    "* [accelerate](https://pypi.org/project/accelerate/) - a dependency required by bitsandbytes.\n",
    "* [datasets](https://pypi.org/project/datasets/) - for loading the training dataset from the HuggingFace hub.\n",
    "\n",
    "**Note**: Restarting the Python kernel may be necessary after installing these dependencies.\n",
    "\n",
    "The notebook has been tested with `mlflow==2.15.1`, `transformers==4.47.0`, `peft==0.13.2`, `accelerate==1.2.0`, `trl==0.12.1`  and `datasets==3.2.0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fcdac661-657b-4bd8-8fa2-f399485c5f4f",
     "showTitle": false,
     "title": ""
    },
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%pip install mlflow==2.15.1\n",
    "%pip install transformers peft accelerate trl datasets -q -U"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to provide the necessary environment variables to use managed MLFlow from `Nebius`:\n",
    "``` bash\n",
    "# the following vars can be accessed from your managed MLFlow deployement\n",
    "MLFLOW_TRACKING_SERVER_CERT_PATH=path/to/tracking/server/certificate\n",
    "MLFLOW_TRACKING_URI=tracking/server/uri\n",
    "MLFLOW_TRACKING_USERNAME=username\n",
    "MLFLOW_TRACKING_PASSWORD=password\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fbb9b043-b0bb-490c-9869-78274b50e2c1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 2. Dataset Preparation\n",
    "\n",
    "### Load Dataset from HuggingFace Hub\n",
    "\n",
    "We will use the `iamtarun/python_code_instructions_18k_alpaca` dataset from the [Hugging Face Hub](https://huggingface.co/datasets/iamtarun/python_code_instructions_18k_alpaca) for this tutorial. This dataset comprises 18.6k pairs of natural language instructions, inputs and their corresponding outputs (Python code), making it usefult to teaching model coding in Python. The dataset includes 4 columns:\n",
    "\n",
    "* `instruction`: A natural language instruction (problem definition) which is supposed to be solved via Python code.\n",
    "* `input`: An optional input to the previous instruction which has to be accepted by the resulting Python program.\n",
    "* `output`: The solution to the initial problem (working Python program).\n",
    "* `prompt`: Prepared prompt for training in `alpaca` format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cyril-k/.cache/pypoetry/virtualenvs/llms-with-mlflow-9f2i9-bN-py3.10/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style> .dataframe th, .dataframe tbody td { text-align: left; padding-right: 30px; } </style> <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instruction</th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "      <th>prompt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Create a function to calculate the sum of a sequence of integers.</td>\n",
       "      <td>[1, 2, 3, 4, 5]</td>\n",
       "      <td># Python code<br>def sum_sequence(sequence):<br>  sum = 0<br>  for num in sequence:<br>    sum += num<br>  return sum</td>\n",
       "      <td>Below is an instruction that describes a task. Write a response that appropriately completes the request.<br><br>### Instruction:<br>Create a function to calculate the sum of a sequence of integers.<br><br>### Input:<br>[1, 2, 3, 4, 5]<br><br>### Output:<br># Python code<br>def sum_sequence(sequence):<br>  sum = 0<br>  for num in sequence:<br>    sum += num<br>  return sum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Generate a Python code for crawling a website for a specific type of data.</td>\n",
       "      <td>website: www.example.com <br>data to crawl: phone numbers</td>\n",
       "      <td>import requests<br>import re<br><br>def crawl_website_for_phone_numbers(website):<br>    response = requests.get(website)<br>    phone_numbers = re.findall('\\d{3}-\\d{3}-\\d{4}', response.text)<br>    return phone_numbers<br>    <br>if __name__ == '__main__':<br>    print(crawl_website_for_phone_numbers('www.example.com'))</td>\n",
       "      <td>Below is an instruction that describes a task. Write a response that appropriately completes the request.<br><br>### Instruction:<br>Generate a Python code for crawling a website for a specific type of data.<br><br>### Input:<br>website: www.example.com <br>data to crawl: phone numbers<br><br>### Output:<br>import requests<br>import re<br><br>def crawl_website_for_phone_numbers(website):<br>    response = requests.get(website)<br>    phone_numbers = re.findall('\\d{3}-\\d{3}-\\d{4}', response.text)<br>    return phone_numbers<br>    <br>if __name__ == '__main__':<br>    print(crawl_website_for_phone_numbers('www.example.com'))</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Create a Python list comprehension to get the squared values of a list [1, 2, 3, 5, 8, 13].</td>\n",
       "      <td></td>\n",
       "      <td>[x*x for x in [1, 2, 3, 5, 8, 13]]</td>\n",
       "      <td>Below is an instruction that describes a task. Write a response that appropriately completes the request.<br><br>### Instruction:<br>Create a Python list comprehension to get the squared values of a list [1, 2, 3, 5, 8, 13].<br><br>### Input:<br><br><br>### Output:<br>[x*x for x in [1, 2, 3, 5, 8, 13]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "dataset_name = \"iamtarun/python_code_instructions_18k_alpaca\"\n",
    "dataset = load_dataset(dataset_name, split=\"train\")\n",
    "\n",
    "\n",
    "def display_table(dataset_or_sample):\n",
    "    # A helper fuction to display a Transformer dataset or single sample contains multi-line string nicely\n",
    "    pd.set_option(\"display.max_colwidth\", None)\n",
    "    pd.set_option(\"display.width\", None)\n",
    "    pd.set_option(\"display.max_rows\", None)\n",
    "\n",
    "    if isinstance(dataset_or_sample, dict):\n",
    "        df = pd.DataFrame(dataset_or_sample, index=[0])\n",
    "    else:\n",
    "        df = pd.DataFrame(dataset_or_sample)\n",
    "\n",
    "    html = df.to_html().replace(\"\\\\n\", \"<br>\")\n",
    "    styled_html = f\"\"\"<style> .dataframe th, .dataframe tbody td {{ text-align: left; padding-right: 30px; }} </style> {html}\"\"\"\n",
    "    display(HTML(styled_html))\n",
    "\n",
    "\n",
    "display_table(dataset.select(range(3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "37d3bbe1-9f94-4b5d-b766-a144d5674e81",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Split Train and Test Dataset\n",
    "The `iamtarun/python_code_instructions_18k_alpaca` dataset consists of a single split, \"train\". We will separate 20% of this as test samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c097d8e-2e87-40e0-ac60-a33c4e932ff1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset contains 14889 code generation prompts\n",
      "Test dataset contains 3723 code generation prompts\n"
     ]
    }
   ],
   "source": [
    "split_dataset = dataset.train_test_split(test_size=0.2, seed=42)\n",
    "train_dataset = split_dataset[\"train\"]\n",
    "test_dataset = split_dataset[\"test\"]\n",
    "\n",
    "print(f\"Training dataset contains {len(train_dataset)} code generation prompts\")\n",
    "print(f\"Test dataset contains {len(test_dataset)} code generation prompts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "45173de1-4776-4219-8897-0e152762e4ad",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 3. Load the Base Model\n",
    "\n",
    "Next, we'll load the `Qwen2.5-7B` model, which will serve as our base model for fine-tuning. This model can be loaded from the HuggingFace Hub repository [Qwen/Qwen2.5-7B](https://huggingface.co/Qwen/Qwen2.5-7B) using the Transformers' `from_pretrained()` API. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e3dd165-38b0-4d1d-ba72-07c912001155",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:02<00:00,  1.50it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "base_model_id = \"Qwen/Qwen2.5-7B\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_id, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token # Needed for training\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_id,\n",
    "    trust_remote_code=True,\n",
    "    cache_dir='',\n",
    "    use_cache = False,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    torch_dtype=getattr(torch, \"bfloat16\"),\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "\n",
    "model.device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b9d6ea49-a0b8-4593-ad05-11b9f0b7408d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### How Does the Base Model Perform?\n",
    "First, let's assess the performance of the vanilla `Qwen2.5-7B` model on the Python code generation task before any fine-tuning. The model produces some python code, however it may not be always correct and is also accompanied by natural language explatnations which will make using this model as a coding assistant difficult. This outcome indicates the necessity of fine-tuning the model for our specific task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73695a28-7778-48aa-98ec-86ccd748e4ac",
     "showTitle": true,
     "title": "Code Generation Using Transformers."
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style> .dataframe th, .dataframe tbody td { text-align: left; padding-right: 30px; } </style> <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>generated_output</th>\n",
       "      <th>ground_truth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Below is an instruction that describes a task. Write a response that appropriately completes the request.<br><br>### Instruction:<br>Construct a Python script to perform a linear regression model for the given data-points.<br><br>### Input:<br>X = [1, 2, 3, 4, 5]<br>Y = [1, 4, 9, 16, 25]<br><br>### Output:<br><br></td>\n",
       "      <td>To construct a Python script for performing a linear regression model on the provided dataset (X and Y), you can use libraries such as `numpy` for numerical operations and `scipy.stats` or other machine learning libraries like `sklearn`. Here's how you could do it using both approaches:<br><br>#### Using NumPy &amp; SciPy Stats<br>```python<br>import numpy as np<br>from scipy import stats<br><br># Given datasets<br>X = np.array([1, 2, 3, 4, 5])<br>Y = np.array([1, 4, 9, 16, 25])<br><br># Perform linear regression<br>slope, intercept, r_value, p_value, std_err = stats.linregress(X, Y)<br><br>print(f\"Slope: {slope}\")<br>print(f\"Intercept: {intercept}\")<br><br># Predicting values based on the fitted line equation y = mx + b<br>def predict(x):<br>    return slope * x + intercept<br><br># Example prediction at point X=6<br>predicted_y_at_6 = predict(6)<br>print(f\"Predicted value of Y when X=6: {predicted_y_at_6}\")<br>```<br><br>#### Using Scikit-Learn Library<br>If you prefer using scikit-learn which provides more flexibility with different types of models including Linear Regression, here‚Äôs how you would implement this:<br><br>```python<br>import numpy as np<br>from sklearn.linear_model import LinearRegression<br><br># Convert lists into arrays if they aren't already<br>if not isinstance(X, np.ndarray):<br>    X = np.array(X).reshape(-1, 1) # Reshape because SkLearn expects input in columns format even though we have only one feature.<br>if not isinstance(Y, np.ndarray):<br>    Y = np.array(Y)<br><br># Initialize and fit the model<br>model = LinearRegression()<br>model.fit(X, Y)<br><br># Get coefficients<br>coefficients = model.coef_<br>intercept = model.intercept_<br><br>print(\"Coefficients:\", coefficients[0])   # Slope<br>print(\"Intercept:\", intercept)<br><br># Making predictions<br>new_X = [[7]]     # New sample points where you want to make predictions<br>predictions = model.predict(new_X)<br>print(\"Prediction for new X=[7]:\", predictions[0][0])<br>```<br><br>Both scripts will output similar results showing the slope and intercept from your linear regression analysis along with predicted values outside the original range specified by your training set.</td>\n",
       "      <td>import matplotlib.pyplot as plt<br>import numpy as np<br>from sklearn.linear_model import LinearRegression<br><br>X = np.array([1, 2, 3, 4, 5]).reshape(-1, 1)<br>y = np.array([1, 4, 9, 16, 25])<br><br># Build the model<br>model = LinearRegression()<br>model.fit(X, y)<br><br># Plot the results<br>plt.scatter(X, y)<br>plt.plot(X, model.predict(X))<br>plt.title(\"Linear Regression Model\")<br>plt.xlabel(\"x\")<br>plt.ylabel(\"y\")<br>plt.show()</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "PROMPT_TEMPLATE = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Input:\n",
    "{input}\n",
    "\n",
    "### Output:\n",
    "{output}\n",
    "\"\"\"\n",
    "\n",
    "import transformers\n",
    "\n",
    "pipeline = transformers.pipeline(model=model, tokenizer=tokenizer, task=\"text-generation\")\n",
    "\n",
    "sample = test_dataset[42]\n",
    "prompt = PROMPT_TEMPLATE.format(\n",
    "    instruction=sample[\"instruction\"], input=sample[\"input\"], output=\"\"\n",
    ")  # Leave the answer part blank\n",
    "\n",
    "with torch.no_grad():\n",
    "    response = pipeline(prompt, max_new_tokens=2048, repetition_penalty=1.15, return_full_text=False)\n",
    "\n",
    "display_table({\"prompt\": prompt, \"generated_output\": response[0][\"generated_text\"], \"ground_truth\": sample[\"output\"]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c93e06e0-a711-46cb-a04c-fc7ba66b8f86",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 4. Define a PEFT Model\n",
    "\n",
    "[LoRA (Low Rank Adaptation)](https://github.com/microsoft/LoRA) is a preceding method for resource-efficient fine-tuning, by reducing the number of trainable parameters through matrix decomposition. Let `W'` represent the final weight matrix from fine-tuning. In LoRA, `W'` is approximated by the sum of the original weight and its update, i.e., `W + ŒîW`, then decomposing the delta part into two low-dimensional matrices, i.e., `ŒîW ‚âà AB`. Suppose `W` is `m`x`m`, and we select a smaller `r` for the rank of `A` and `B`, where `A` is `m`x`r` and `B` is `r`x`m`. Now, the original trainable parameters, which are quadratic in size of `W` (i.e., `m^2`), after decomposition, become `2mr`. Empirically, we can choose a much smaller number for `r`, e.g., 32, 64, compared to the full weight matrix size, therefore this significantly reduces the number of parameters to train.\n",
    "\n",
    "Although the mathematics behind LoRA is intricate, [PEFT](https://huggingface.co/docs/peft/en/index) helps us by simplifying the process of adapting LoRA to the pretrained Transformer model.\n",
    "\n",
    "In the next cell, we create a [LoraConfig](https://github.com/huggingface/peft/blob/main/src/peft/tuners/lora/config.py) with various settings for LoRA. These hyperparameters might need optimization to achieve the best model performance for your specific task. **MLflow** facilitates this process by tracking these hyperparameters, the associated model, and its outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76582f53-8427-4d31-9003-f751db690335",
     "showTitle": true,
     "title": "Configuring LORA-based Model Optimization"
    }
   },
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "# Enabling gradient checkpointing, to make the training further efficient\n",
    "model.gradient_checkpointing_enable()\n",
    "   \n",
    "# This is the rank of the decomposed matrices A and B to be learned during fine-tuning. A smaller number will save more GPU memory but might result in worse performance.\n",
    "lora_r = 64\n",
    "# This is the coefficient for the learned ŒîW factor, so the larger number will typically result in a larger behavior change after fine-tuning.\n",
    "lora_alpha = 32\n",
    "# Drop out ratio for the layers in LoRA adaptors A and B.\n",
    "lora_dropout = 0.1\n",
    "# Bias parameters to train. 'none' is recommended to keep the original model performing equally when turning off the adapter.\n",
    "bias=\"none\"\n",
    "\n",
    "# We fine-tune all linear layers in the model. It might sound a bit large, but the trainable adapter size is still only **1.16%** of the whole model.\n",
    "target_modules=[\n",
    "    \"k_proj\", \n",
    "    \"q_proj\", \n",
    "    \"v_proj\", \n",
    "    \"up_proj\", \n",
    "    \"down_proj\", \n",
    "    \"gate_proj\"\n",
    "]\n",
    "# These modules will be not affected by an adapter\n",
    "modules_to_save=[\n",
    "    \"embed_tokens\", \n",
    "    \"input_layernorm\", \n",
    "    \"post_attention_layernorm\", \n",
    "    \"norm\"\n",
    "]\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    r=lora_r,\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    target_modules=target_modules,\n",
    "    modules_to_save=modules_to_save,\n",
    "    bias=bias,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5023a2f8-98be-4017-bee1-033956b4c405",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**That's it!!!** PEFT has made the LoRA setup super easy.\n",
    "\n",
    "An additional bonus is that the PEFT model exposes the same interfaces as a Transformers model. This means that everything from here on is quite similar to the standard model training process using Transformers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e668fe2-d2c1-4946-8c21-020ce8e562e0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 5. Kick-off a Training Job\n",
    "\n",
    "Similar to conventional Transformers training, we'll first set up a Trainer object to organize the training iterations. There are numerous hyperparameters to configure, but MLflow will manage them on your behalf.\n",
    "\n",
    "To enable MLflow logging, you can specify `report_to=\"mlflow\"` and name your training trial with the `run_name` parameter. This action initiates an [MLflow run](https://mlflow.org/docs/latest/tracking.html#runs) that automatically logs training metrics, hyperparameters, configurations, and the trained model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f63eefa-2025-43f4-9cc1-c9705335faf8",
     "showTitle": true,
     "title": "Code Snippet Name: MLflow Transformer Trainer with SFT"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/12/18 09:34:03 INFO mlflow.system_metrics.system_metrics_monitor: Started monitoring system metrics.\n",
      "/home/cyril-k/.cache/pypoetry/virtualenvs/llms-with-mlflow-9f2i9-bN-py3.10/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field, max_seq_length. Will not be supported from version '0.13.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/home/cyril-k/.cache/pypoetry/virtualenvs/llms-with-mlflow-9f2i9-bN-py3.10/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:300: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/home/cyril-k/.cache/pypoetry/virtualenvs/llms-with-mlflow-9f2i9-bN-py3.10/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:328: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from transformers import TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "\n",
    "import mlflow\n",
    "mlflow.enable_system_metrics_logging()\n",
    "mlflow.set_experiment(\"Finetuning LLMs with MLFlow\")\n",
    "\n",
    "try:\n",
    "    mlflow.start_run(run_name=f\"{base_model_id}-demo-LoRA-{datetime.now().strftime('%Y-%m-%d-%H-%M-%s')}\")\n",
    "    run = mlflow.active_run()\n",
    "\n",
    "    max_seq_length = 2048\n",
    "    output_dir = \"./demo-results\"\n",
    "    max_grad_norm = 0.3\n",
    "    warmup_ratio = 0.1\n",
    "    lr_scheduler_type = \"cosine\"\n",
    "\n",
    "    training_arguments = TrainingArguments(\n",
    "        # Set this to mlflow for logging your training\n",
    "        report_to=\"mlflow\",\n",
    "        # Name the MLflow run\n",
    "        run_name=run.info.run_name,\n",
    "        # Replace with your output destination\n",
    "        output_dir=output_dir,\n",
    "        # For the following arguments, refer to https://huggingface.co/docs/transformers/main_classes/trainer\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        gradient_accumulation_steps=4,\n",
    "        gradient_checkpointing=True,\n",
    "        gradient_checkpointing_kwargs = {\"use_reentrant\": False},\n",
    "        optim=\"paged_adamw_32bit\",\n",
    "        bf16=True,\n",
    "        learning_rate=2e-5,\n",
    "        lr_scheduler_type=lr_scheduler_type,\n",
    "        max_grad_norm=max_grad_norm,\n",
    "        num_train_epochs=3,\n",
    "        logging_steps=10,\n",
    "        warmup_ratio=warmup_ratio,\n",
    "        # https://discuss.huggingface.co/t/training-llama-with-lora-on-multiple-gpus-may-exist-bug/47005/3\n",
    "        ddp_find_unused_parameters=False,\n",
    "        group_by_length=True,\n",
    "        eval_strategy=\"steps\",\n",
    "        eval_steps=20,\n",
    "    )\n",
    "\n",
    "    # log datasets to the same run\n",
    "    mlflow.log_input(\n",
    "        mlflow.data.huggingface_dataset.from_huggingface(\n",
    "            dataset,\n",
    "            path=dataset_name\n",
    "        ), \n",
    "        context=\"train\"\n",
    "    )\n",
    "\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        train_dataset=train_dataset.take(1000),\n",
    "        eval_dataset=test_dataset.take(1000),\n",
    "        peft_config=peft_config,\n",
    "        dataset_text_field=\"prompt\",\n",
    "        max_seq_length=max_seq_length,\n",
    "        tokenizer=tokenizer,\n",
    "        args=training_arguments,\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    mlflow.end_run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ad244e5-4abd-444a-8e1b-c509ee0711a8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The training duration may span several hours, contingent upon your hardware specifications. Nonetheless, the primary objective of this tutorial is to acquaint you with the process of fine-tuning using PEFT and MLflow, rather than to cultivate a highly code assistant. If you don't care much about the model performance, you may specify a smaller number of steps or interrupt the following cell to proceed with the rest of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "560c81c4-8e6e-48d4-95eb-4a2e53a62889",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='93' max='93' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [93/93 03:30, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.784800</td>\n",
       "      <td>0.680010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.598700</td>\n",
       "      <td>0.568557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.551600</td>\n",
       "      <td>0.552230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.536700</td>\n",
       "      <td>0.549905</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "2024/12/18 09:37:54 INFO mlflow.tracking._tracking_service.client: üèÉ View run Qwen/Qwen2.5-7B-demo-LoRA-2024-12-18-09-34-1734514443 at: https://tracking.mlflow-e00rhqs1bwevnqy5wj.backbone-e00ffdgj3ybad7mxrx.msp.eu-north1.nebius.cloud/#/experiments/25/runs/415a78c599c54e218f4212bc426028b1.\n",
      "2024/12/18 09:37:54 INFO mlflow.tracking._tracking_service.client: üß™ View experiment at: https://tracking.mlflow-e00rhqs1bwevnqy5wj.backbone-e00ffdgj3ybad7mxrx.msp.eu-north1.nebius.cloud/#/experiments/25.\n",
      "2024/12/18 09:37:55 INFO mlflow.system_metrics.system_metrics_monitor: Stopping system metrics monitoring...\n",
      "2024/12/18 09:37:55 INFO mlflow.system_metrics.system_metrics_monitor: Successfully terminated system metrics monitoring!\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    trainer.train()\n",
    "finally:\n",
    "    mlflow.end_run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve the information on training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset name: dataset\n",
      "Dataset digest: 2bfa6908\n",
      "Dataset profile: {\"num_rows\": 18612, \"dataset_size\": 25180782, \"size_in_bytes\": 36537858}\n",
      "Dataset schema: {\"mlflow_colspec\": [{\"type\": \"string\", \"name\": \"instruction\", \"required\": true}, {\"type\": \"string\", \"name\": \"input\", \"required\": true}, {\"type\": \"string\", \"name\": \"output\", \"required\": true}, {\"type\": \"string\", \"name\": \"prompt\", \"required\": true}]}\n"
     ]
    }
   ],
   "source": [
    "run = mlflow.get_run(mlflow.last_active_run().info.run_id)\n",
    "dataset_info = run.inputs.dataset_inputs[0].dataset\n",
    "print(f\"Dataset name: {dataset_info.name}\")\n",
    "print(f\"Dataset digest: {dataset_info.digest}\")\n",
    "print(f\"Dataset profile: {dataset_info.profile}\")\n",
    "print(f\"Dataset schema: {dataset_info.schema}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'path': 'iamtarun/python_code_instructions_18k_alpaca',\n",
       " 'config_name': 'default',\n",
       " 'data_dir': None,\n",
       " 'data_files': None,\n",
       " 'split': 'train',\n",
       " 'revision': None}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_source = mlflow.data.get_source(dataset_info)\n",
    "dataset_source.to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ba41b0a1-4976-4f52-9a31-6aed528994f5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 6. Save the PEFT Model to MLflow\n",
    "\n",
    "Hooray! We have successfully fine-tuned the `Qwen2.5-7B` model into a Python coding assistant. Before concluding the training, one final step is to save the trained PEFT model to MLflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Prompt Template and Default Inference Parameters (optional)\n",
    "\n",
    "LLMs prediction behavior is not only defined by the model weights, but also largely controlled by the prompt and inference paramters such as `max_token_length`, `repetition_penalty`. Therefore, it is highly advisable to save those metadata along with the model, so that you can expect the consistent behavior when loading the model later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prompt Template\n",
    "The user prompt itself is free text, but you can harness the input by applying a 'template'. MLflow Transformer flavor supports saving a prompt template with the model, and apply it automatically before the prediction. This also allows you to hide the system prompt from model clients. To save the prompt template, we have to define a single string that contains `{prompt}` variable, and pass it to the `prompt_template` argument of [mlflow.transformers.log_model](https://mlflow.org/docs/latest/python_api/mlflow.transformers.html#mlflow.transformers.log_model) API. Refer to [Saving Prompt Templates with Transformer Pipelines](https://mlflow.org/docs/latest/llms/transformers/guide/index.html#saving-prompt-templates-with-transformer-pipelines) for more detailed usage of this feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basically the same format as we applied to the dataset. However, the template only accepts {prompt} variable so both instruction and input need to be fed in there.\n",
    "prompt_template = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "\n",
    "{prompt}\n",
    "\n",
    "### Output:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inference Parameters\n",
    "\n",
    "Inference parameters can be saved with MLflow model as a part of [Model Signature](https://mlflow.org/docs/latest/model/signatures.html). The signature defines model input and output format with additional parameters passed to the model prediction, and you can let MLflow to infer it from some sample input using [mlflow.models.infer_signature](https://mlflow.org/docs/latest/python_api/mlflow.models.html#mlflow.models.infer_signature) API. If you pass the concrete value for parameters, MLflow treats them as default values and apply them at the inference if they are not provided by users. For more details about the Model Signature, please refer to the [MLflow documentation](https://mlflow.org/docs/latest/model/signatures.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "inputs: \n",
       "  [string (required)]\n",
       "outputs: \n",
       "  [string (required)]\n",
       "params: \n",
       "  ['max_new_tokens': long (default: 2048), 'repetition_penalty': double (default: 1.15), 'return_full_text': boolean (default: False)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mlflow.models import infer_signature\n",
    "\n",
    "sample = train_dataset[42]\n",
    "_prompt_prompt = \"\"\"### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Input:\n",
    "{input}\"\"\"\n",
    "\n",
    "\n",
    "# MLflow infers schema from the provided sample input/output/params\n",
    "signature = infer_signature(\n",
    "    model_input=_prompt_prompt.format(\n",
    "        instruction=sample[\"instruction\"],\n",
    "        input=sample[\"input\"]\n",
    "    ),\n",
    "    model_output=sample[\"output\"],\n",
    "    # Parameters are saved with default values if specified\n",
    "    params={\"max_new_tokens\": 2048, \"repetition_penalty\": 1.15, \"return_full_text\": False},\n",
    ")\n",
    "signature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the PEFT Model to MLflow\n",
    "Finally, we will call [mlflow.transformers.log_model](https://mlflow.org/docs/latest/python_api/mlflow.transformers.html#mlflow.transformers.log_model) API to log the model to MLflow. A few critical points to remember when logging a PEFT model to MLflow are:\n",
    "\n",
    "1. **MLflow logs the Transformer model as a [Pipeline](https://huggingface.co/docs/transformers/en/main_classes/pipelines).** A pipeline bundles a model with its tokenizer (or other components, depending on the task type) and simplifies the prediction steps into an easy-to-use interface, making it an excellent tool for ensuring reproducibility. In the code below, we pass the model and tokenizer as a dictionary, then MLflow automatically deduces the correct pipeline type and saves it.\n",
    "2. **MLflow does not save the base model weight for the PEFT model**. When executing `mlflow.transformers.log_model`, MLflow only saves the small number of trained parameters, i.e., the PEFT adapter. For the base model, MLflow instead records a reference to the HuggingFace hub (repository name and commit hash), and downloads the base model weights on the fly when loading the PEFT model. This approach significantly reduces storage usage and logging latency; for instance, the logged artifacts size in this tutorial is about 1GB, while the full `Qwen2.5-7B` model is about 20GB.\n",
    "3. **Save a tokenizer without padding**. During fine-tuning, we applied padding to the dataset to standardize the sequence length in a batch. However, padding is no longer necessary at inference, so we save a different tokenizer without padding. This ensures the loaded model can be used for inference immediately.\n",
    "\n",
    "**Note**: Currently, manual logging is required for the PEFT adapter and config, while other information, such as dataset, metrics, Trainer parameters, etc., are automatically logged. However, this process may be automated in future versions of MLflow and Transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45ac775d-3b26-43a0-8157-80079264b967",
     "showTitle": true,
     "title": "MLflow PEFT Model Logging Integration"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/12/18 09:47:46 INFO mlflow.system_metrics.system_metrics_monitor: Started monitoring system metrics.\n",
      "/home/cyril-k/.cache/pypoetry/virtualenvs/llms-with-mlflow-9f2i9-bN-py3.10/lib/python3.10/site-packages/mlflow/utils/docstring_utils.py:422: UserWarning: The 'transformers' MLflow Models integration is known to be compatible with the following package version ranges: ``4.25.1`` -  ``4.42.4``. MLflow Models integrations with transformers may not succeed when used with package versions outside of this range.\n",
      "  _do_version_compatibility_warning(notice)\n",
      "2024/12/18 09:47:47 INFO mlflow.transformers: Overriding save_pretrained to False for PEFT models, following the Transformers behavior. The PEFT adaptor and config will be saved, but the base model weights will not and reference to the HuggingFace Hub repository will be logged instead.\n",
      "2024/12/18 09:47:50 INFO mlflow.transformers: Skipping saving pretrained model weights to disk as the save_pretrained is set to False. The reference to HuggingFace Hub repository Qwen/Qwen2.5-7B will be logged instead.\n",
      "2024/12/18 09:47:50 INFO mlflow.transformers: text-generation pipelines saved with prompt templates have the `return_full_text` pipeline kwarg set to False by default. To override this behavior, provide a `model_config` dict with `return_full_text` set to `True` when saving the model.\n",
      "/home/cyril-k/.cache/pypoetry/virtualenvs/llms-with-mlflow-9f2i9-bN-py3.10/lib/python3.10/site-packages/_distutils_hack/__init__.py:30: UserWarning: Setuptools is replacing distutils. Support for replacing an already imported distutils is deprecated. In the future, this condition will fail. Register concerns at https://github.com/pypa/setuptools/issues/new?template=distutils-deprecation.yml\n",
      "  warnings.warn(\n",
      "2024/12/18 09:48:48 INFO mlflow.tracking._tracking_service.client: üèÉ View run Qwen/Qwen2.5-7B-demo-LoRA-2024-12-18-09-34-1734514443 at: https://tracking.mlflow-e00rhqs1bwevnqy5wj.backbone-e00ffdgj3ybad7mxrx.msp.eu-north1.nebius.cloud/#/experiments/25/runs/415a78c599c54e218f4212bc426028b1.\n",
      "2024/12/18 09:48:48 INFO mlflow.tracking._tracking_service.client: üß™ View experiment at: https://tracking.mlflow-e00rhqs1bwevnqy5wj.backbone-e00ffdgj3ybad7mxrx.msp.eu-north1.nebius.cloud/#/experiments/25.\n",
      "2024/12/18 09:48:49 INFO mlflow.system_metrics.system_metrics_monitor: Stopping system metrics monitoring...\n",
      "2024/12/18 09:48:49 INFO mlflow.system_metrics.system_metrics_monitor: Successfully terminated system metrics monitoring!\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "\n",
    "# Get the ID of the MLflow Run that was automatically created above\n",
    "last_run_id = mlflow.last_active_run().info.run_id\n",
    "\n",
    "# Save a tokenizer without padding because it is only needed for training\n",
    "tokenizer_no_pad = AutoTokenizer.from_pretrained(base_model_id, add_bos_token=True)\n",
    "\n",
    "# If you interrupt the training, uncomment the following line to stop the MLflow run\n",
    "mlflow.end_run()\n",
    "\n",
    "with mlflow.start_run(run_id=last_run_id):\n",
    "    mlflow.log_params(peft_config.to_dict())\n",
    "    mlflow.transformers.log_model(\n",
    "        transformers_model={\"model\": trainer.model, \"tokenizer\": tokenizer_no_pad},\n",
    "        prompt_template=prompt_template,\n",
    "        signature=signature,\n",
    "        artifact_path=\"model\",  # This is a relative path to save model files within MLflow run\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bbc10f9d-4db7-43dd-872b-65b1b23ec16f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### What's Logged to MLflow?\n",
    "\n",
    "Let's briefly review what is logged/saved to MLflow as a result of your training. Select the experiment \"Finetuning LLMs with MLFlow\" in the MLFlow UI on the left side. Then click on the latest MLflow Run named `Qwen/Qwen2.5-7B-demo-LoRA-...` to view the Run details.\n",
    "\n",
    "#### Parameters\n",
    "\n",
    "The `Parameters` section displays hundreds of parameters specified for the Trainer and LoraConfig such as `learning_rate`, `r`, `lora_alpha`. It also includes default parameters that were not explicitly specified, which is crucial for ensuring reproducibility, especially if the library's default values change.\n",
    "\n",
    "#### Metrics\n",
    "The `Metrics` section presents the model metrics collected during the run, such as `train_loss`. You can visualize these metrics with various types of graphs in the \"Chart\" tab. \n",
    "\n",
    "#### Artifacts\n",
    "The `Artifacts` section displays the files/directories saved in MLflow as a result of training. For Transformers PEFT training, you should see the following files/directories:\n",
    "\n",
    "```\n",
    "\n",
    "    model/\n",
    "      ‚îú‚îÄ peft/\n",
    "      ‚îÇ  ‚îú‚îÄ adapter_config.json       # JSON file of the LoraConfig\n",
    "      ‚îÇ  ‚îú‚îÄ adapter_module.safetensor # The weight file of the LoRA adapter\n",
    "      ‚îÇ  ‚îî‚îÄ README.md                 # Empty README file generated by Transformers\n",
    "      ‚îÇ\n",
    "      ‚îú‚îÄ LICENSE.txt                  # License information about the base model (Qwen2.5-7B)\n",
    "      ‚îú‚îÄ MLModel                      # Contains various metadata about your model\n",
    "      ‚îú‚îÄ conda.yaml                   # Dependencies to create conda environment\n",
    "      ‚îú‚îÄ model_card.md                # Model card text for the base model\n",
    "      ‚îú‚îÄ model_card_data.yaml         # Model card data for the base model\n",
    "      ‚îú‚îÄ python_env.yaml              # Dependencies to create Python virtual environment\n",
    "      ‚îî‚îÄ requirements.txt             # Pip requirements for model inference\n",
    "\n",
    "```\n",
    "\n",
    "#### Model Metadata\n",
    "\n",
    "In the MLModel file, you can see the many detailed metadata are saved about the PEFT and base model.\n",
    "Here is an excerpt of the MLModel file (some fields are omitted for simplicity)\n",
    "\n",
    "```\n",
    "flavors:\n",
    "  transformers:\n",
    "    peft_adaptor: peft                                 # Points the location of the saved PEFT model\n",
    "    pipeline_model_type: Qwen2ForCausalLM              # The base model implementation\n",
    "    source_model_name: Qwen/Qwen2.5-7B.                # Repository name of the base model\n",
    "    source_model_revision: xxxxxxx                     # Commit hash in the repository for the base model\n",
    "    task: text-generation                              # Pipeline type\n",
    "    torch_dtype: torch.bfloat16                        # Dtype for loading the model\n",
    "    tokenizer_type: Qwen2TokenizerFast                 # Tokenizer implementation\n",
    "\n",
    "# Prompt template saved with the model above\n",
    "metadata:\n",
    "  prompt_template: 'Below is an instruction that describes a task. Write a response\n",
    "    that appropriately completes the request.\n",
    "\n",
    "\n",
    "    {prompt}\n",
    "\n",
    "\n",
    "    ### Output:\n",
    "\n",
    "    '\n",
    "# Defines the input and output format of the model, with additional inference parameters with default values\n",
    "signature:\n",
    "  inputs: '[{\"type\": \"string\", \"required\": true}]'\n",
    "  outputs: '[{\"type\": \"string\", \"required\": true}]'\n",
    "  params: '[{\"name\": \"max_new_tokens\", \"type\": \"long\", \"default\": 2048, \"shape\": null},\n",
    "    {\"name\": \"repetition_penalty\", \"type\": \"double\", \"default\": 1.15, \"shape\": null},\n",
    "    {\"name\": \"return_full_text\", \"type\": \"boolean\", \"default\": false, \"shape\": null}]'\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2ce65d14-6da7-44a1-8eee-9040bb3ca06d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 7. Load the Saved PEFT Model from MLflow\n",
    "\n",
    "Finally, let's load the model logged in MLflow and evaluate its performance as a Python code assistant. There are two ways to load a Transformer model in MLflow:\n",
    "\n",
    "1. Use [mlflow.transformers.load_model()](https://mlflow.org/docs/latest/python_api/mlflow.transformers.html#mlflow.transformers.load_model). This method returns a native Transformers pipeline instance.\n",
    "2. Use [mlflow.pyfunc.load_model()](https://mlflow.org/docs/latest/python_api/mlflow.pyfunc.html#mlflow.pyfunc.load_model). This method returns an MLflow's PythonModel instance that wraps the Transformers pipeline, offering additional features over the native pipeline, such as (1) a unified `predict()` API for inference, (2) model signature enforcement, and (3) automatically applying a prompt template and default parameters if saved. Please note that not all the Transformer pipelines are supported for pyfunc loading, refer to the [MLflow documentation](https://mlflow.org/docs/latest/llms/transformers/guide/index.html#supported-transformers-pipeline-types-for-pyfunc) for the full list of supported pipeline types.\n",
    "\n",
    "The first option is preferable if you wish to use the model via the native Transformers interface. The second option offers a simplified and unified interface across different model types and is particularly useful for model testing before production deployment. In the following code, we will use the [mlflow.pyfunc.load_model()](https://mlflow.org/docs/latest/python_api/mlflow.pyfunc.html#mlflow.pyfunc.load_model) to show how it applies the prompt template and the default inference parameters defined above.\n",
    "\n",
    "\n",
    "**NOTE**: Invoking `load_model()` loads a new model instance onto your GPU, which may exceed GPU memory limits and trigger an Out Of Memory (OOM) error, or cause the Transformers library to attempt to offload parts of the model to other devices or disk. This offloading can lead to issues, such as a \"ValueError: We need an `offload_dir` to dispatch this model according to this `decide_map`.\" If you encounter this error, consider restarting the Python Kernel and loading the model again.\n",
    "\n",
    "**CAUTION**: Restarting the Python Kernel will erase all intermediate states and variables from the above cells. Ensure that the trained PEFT model is properly logged in MLflow before restarting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59df3b6d-e5f6-4274-a624-19dc767769bc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:56<00:00,  5.60s/it]  \n",
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:03<00:00,  1.28it/s]\n",
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    }
   ],
   "source": [
    "# You can find the ID of run in the Run detail page on MLflow UI\n",
    "mlflow_model = mlflow.pyfunc.load_model(\"runs:/415a78c599c54e218f4212bc426028b1/model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/12/18 09:54:15 WARNING mlflow.transformers: params provided to the `predict` method will override the inference configuration saved with the model. If the params provided are not valid for the pipeline, MlflowException will be raised.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style> .dataframe th, .dataframe tbody td { text-align: left; padding-right: 30px; } </style> <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>generated_output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td><br>### Instruction:<br>Develop a Python program to convert the following Fahrenheit value to Celsius.<br><br>### Input:<br>212<br></td>\n",
       "      <td>def fahrenheit_to_celsius(f):<br>    c = (f - 32) * 5/9<br>    return round(c, 2)<br><br>print(fahrenheit_to_celsius(212))</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # We only input table and question, since system prompt is adeed in the prompt template.\n",
    "test_prompt = \"\"\"\n",
    "### Instruction:\n",
    "Develop a Python program to convert the following Fahrenheit value to Celsius.\n",
    "\n",
    "### Input:\n",
    "212\n",
    "\"\"\"\n",
    "\n",
    "# Inference parameters like max_tokens_length are set to default values specified in the Model Signature\n",
    "generated_query = mlflow_model.predict(test_prompt)[0]\n",
    "display_table({\"prompt\": test_prompt, \"generated_output\": generated_query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.0\n"
     ]
    }
   ],
   "source": [
    "def fahrenheit_to_celsius(f):\n",
    "    c = (f - 32) * 5/9\n",
    "    return round(c, 2)\n",
    "\n",
    "print(fahrenheit_to_celsius(212))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect!! The fine-tuned model now generates Python code correctly. As you can see in the code and result above, the system prompt and default inference parameters are applied automatically, so we don't have to pass it to the loaded model. This is super powerful when you want to deploy multiple models (or update an existing model) with different the system prompt or parameters, because you don't have to edit client's implementation as they are abstracted behind the MLflow model :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this tutorial, you learned how to fine-tune a large language model with LoRA for Python coding assistant task using PEFT. You also learned the key role of MLflow in the LLM fine-tuning process, which tracks parameters and metrics during the fine-tuning, and manage models and other assets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "PEFT Llama Finetuning",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "llms-with-mlflow-9f2i9-bN-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
